Many important algorithms, especially those for sorting and searching, work by
           comparing items of their inputs. We can study the performance of such algorithms
           with a device called a decision tree. As an example, Figure 11.1 presents a decision
           tree of an algorithm for finding a minimum of three numbers. Each internal node
           of a binary decision tree represents a key comparison indicated in the node,
           e.g., k < k . The node's left subtree contains the information about subsequent
           comparisons made if k < k , and its right subtree does the same for the case of
           k > k . (For the sake of simplicity, we assume throughout this section that all input
           items are distinct.) Each leaf represents a possible outcome of the algorithm's
           run on some input of size n. Note that the number of leaves can be greater than
           the number of outcomes because, for some algorithms, the same outcome can
           be arrived at through a different chain of comparisons. (This happens to be the
           case for the decision tree in Figure 11.1.) An important point is that the number of
           leaves must be at least as large as the number of possible outcomes. The algorithm's
           work on a particular input of size n can be traced by a path from the root to a leaf
           in its decision tree, and the number of comparisons made by the algorithm on such
                   yes         a <b                       no
yes          a <c  no                                 yes     b <c         no
a                  c                                  b                        c
FIGURE 11.1  Decision tree for finding a minimum of three numbers.
a run is equal to the length of this path. Hence, the number of comparisons in the
worst case is equal to the height of the algorithm's decision tree.
The central idea behind this model lies in the observation that a tree with a
given number of leaves, which is dictated by the number of possible outcomes, has
to be tall enough to have that many leaves. Specifically, it is not difficult to prove
that for any binary tree with l leaves and height h,
                            h  log2 l  .                                          (11.1)
Indeed, a binary tree of height h with the largest number of leaves has all its leaves
on the last level (why?). Hence, the largest number of leaves in such a tree is 2h.
In other words, 2h  l, which immediately implies (11.1).
Inequality (11.1) puts a lower bound on the heights of binary decision trees
and hence the worst-case number of comparisons made by any comparison-based
algorithm for the problem in question. Such a bound is called the information-
theoretic lower bound (see Section 11.1). We illustrate this technique below on
two important problems: sorting and searching in a sorted array.
