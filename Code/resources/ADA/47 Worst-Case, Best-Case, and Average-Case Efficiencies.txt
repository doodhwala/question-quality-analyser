In the beginning of this section, we established that it is reasonable to measure
an algorithm's efficiency as a function of a parameter indicating the size of the
algorithm's input. But there are many algorithms for which running time depends
not only on an input size but also on the specifics of a particular input. Consider,
as an example, sequential search. This is a straightforward algorithm that searches
for a given item (some search key K) in a list of n elements by checking successive
elements of the list until either a match with the search key is found or the list
is exhausted. Here is the algorithm's pseudocode, in which, for simplicity, a list is
implemented as an array. It also assumes that the second condition A[i] = K will
not be checked if the first one, which checks that the array's index does not exceed
its upper bound, fails.
ALGORITHM           SequentialSearch(A[0..n - 1], K)
//Searches for a given value in a given array by sequential search
//Input: An array A[0..n - 1] and a search key K
//Output: The index of the first element in A that matches K
//         or -1 if there are no matching elements
i0
while i < n and A[i] = K do
    i i +1
if i < n return i
else return -1
Clearly, the running time of this algorithm can be quite different for the
same list size n. In the worst case, when there are no matching elements or
the first matching element happens to be the last one on the list, the algorithm
makes the largest number of key comparisons among all possible inputs of size
n: Cworst (n) = n.
The worst-case efficiency of an algorithm is its efficiency for the worst-case
input of size n, which is an input (or inputs) of size n for which the algorithm
runs the longest among all possible inputs of that size. The way to determine
the worst-case efficiency of an algorithm is, in principle, quite straightforward:
analyze the algorithm to see what kind of inputs yield the largest value of the basic
operation's count C(n) among all possible inputs of size n and then compute this
worst-case value Cworst(n). (For sequential search, the answer was obvious. The
methods for handling less trivial situations are explained in subsequent sections of
this chapter.) Clearly, the worst-case analysis provides very important information
about an algorithm's efficiency by bounding its running time from above. In other
    words, it guarantees that for any instance of size n, the running time will not exceed
    Cworst(n), its running time on the worst-case inputs.
    The best-case efficiency of an algorithm is its efficiency for the best-case input
    of size n, which is an input (or inputs) of size n for which the algorithm runs the
    fastest among all possible inputs of that size. Accordingly, we can analyze the best-
    case efficiency as follows. First, we determine the kind of inputs for which the count
    C(n) will be the smallest among all possible inputs of size n. (Note that the best
    case does not mean the smallest input; it means the input of size n for which the
    algorithm runs the fastest.) Then we ascertain the value of C(n) on these most
    convenient inputs. For example, the best-case inputs for sequential search are lists
    of size n with their first element equal to a search key; accordingly, Cbest(n) = 1
    for this algorithm.
    The analysis of the best-case efficiency is not nearly as important as that
    of the worst-case efficiency. But it is not completely useless, either. Though we
    should not expect to get best-case inputs, we might be able to take advantage of
    the fact that for some algorithms a good best-case performance extends to some
    useful types of inputs close to being the best-case ones. For example, there is a
    sorting algorithm (insertion sort) for which the best-case inputs are already sorted
    arrays on which the algorithm works very fast. Moreover, the best-case efficiency
    deteriorates only slightly for almost-sorted arrays. Therefore, such an algorithm
    might well be the method of choice for applications dealing with almost-sorted
    arrays. And, of course, if the best-case efficiency of an algorithm is unsatisfactory,
    we can immediately discard it without further analysis.
    It should be clear from our discussion, however, that neither the worst-case
    analysis nor its best-case counterpart yields the necessary information about an
    algorithm's behavior on a "typical" or "random" input. This is the information that
    the average-case efficiency seeks to provide. To analyze the algorithm's average-
    case efficiency, we must make some assumptions about possible inputs of size n.
    Let's consider again sequential search. The standard assumptions are that
    (a) the probability of a successful search is equal to p (0  p  1) and (b) the
    probability of the first match occurring in the ith position of the list is the same
    for every i. Under these assumptions--the validity of which is usually difficult to
    verify, their reasonableness notwithstanding--we can find the average number
    of key comparisons Cavg(n) as follows. In the case of a successful search, the
    probability of the first match occurring in the ith position of the list is p/n for
    every i, and the number of comparisons made by the algorithm in such a situation
    is obviously i. In the case of an unsuccessful search, the number of comparisons
    will be n with the probability of such a search being (1 - p). Therefore,
    Cavg(n)  =           [1  .  p  +  2  .  p  +  .  .  .  +  i  .  p  +  .  .  .  +  n  .  p]+  n  .  (1  -  p)
                                n           n                       n                       n
             = p [1 + 2 + . . . + i + . . . + n] + n(1 - p)
                         n
             = p n(n + 1) + n(1 - p) = p(n + 1) + n(1 - p).
                         n         2                                   2
This general formula yields some quite reasonable answers. For example, if p = 1
(the search must be successful), the average number of key comparisons made
by sequential search is (n + 1)/2; that is, the algorithm will inspect, on average,
about half of the list's elements. If p = 0 (the search must be unsuccessful), the
average number of key comparisons will be n because the algorithm will inspect
all n elements on all such inputs.
As you can see from this very elementary example, investigation of the
average-case efficiency is considerably more difficult than investigation of the
worst-case and best-case efficiencies. The direct approach for doing this involves
dividing all instances of size n into several classes so that for each instance of the
class the number of times the algorithm's basic operation is executed is the same.
(What were these classes for sequential search?) Then a probability distribution
of inputs is obtained or assumed so that the expected value of the basic operation's
count can be found.
The technical implementation of this plan is rarely easy, however, and prob-
abilistic assumptions underlying it in each particular case are usually difficult to
verify. Given our quest for simplicity, we will mostly quote known results about
the average-case efficiency of algorithms under discussion. If you are interested
in derivations of these results, consult such books as [Baa00], [Sed96], [KnuI],
[KnuII], and [KnuIII].
It should be clear from the preceding discussion that the average-case ef-
ficiency cannot be obtained by taking the average of the worst-case and the
best-case efficiencies. Even though this average does occasionally coincide with
the average-case cost, it is not a legitimate way of performing the average-case
analysis.
Does one really need the average-case efficiency information? The answer is
unequivocally yes: there are many important algorithms for which the average-
case efficiency is much better than the overly pessimistic worst-case efficiency
would lead us to believe. So, without the average-case analysis, computer scientists
could have missed many important algorithms.
Yet another type of efficiency is called amortized efficiency. It applies not to
a single run of an algorithm but rather to a sequence of operations performed
on the same data structure. It turns out that in some situations a single operation
can be expensive, but the total time for an entire sequence of n such operations is
always significantly better than the worst-case efficiency of that single operation
multiplied by n. So we can "amortize" the high cost of such a worst-case occur-
rence over the entire sequence in a manner similar to the way a business would
amortize the cost of an expensive item over the years of the item's productive life.
This sophisticated approach was discovered by the American computer scientist
Robert Tarjan, who used it, among other applications, in developing an interest-
ing variation of the classic binary search tree (see [Tar87] for a quite readable
nontechnical discussion and [Tar85] for a technical account). We will see an ex-
ample of the usefulness of amortized efficiency in Section 9.2, when we consider
algorithms for finding unions of disjoint sets.
