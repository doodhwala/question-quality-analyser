This page intentionally left blank
Vice  President and Editorial Director, ECS  Marcia Horton

                         Editor-in-Chief     Michael Hirsch

                    Acquisitions Editor      Matt Goldstein

                    Editorial Assistant      Chelsea Bell

                 Vice President, Marketing   Patrice Jones

                    Marketing Manager        Yezan Alayan

               Senior Marketing Coordinator  Kathryn Ferranti

                    Marketing Assistant      Emma Snider

                 Vice President, Production  Vince O'Brien

                         Managing Editor     Jeff Holcomb

                 Production Project Manager  Kayla Smith-Tarbox

               Senior Operations Supervisor  Alan Fischer

                 Manufacturing Buyer         Lisa McDowell

                            Art Director     Anthony Gemmellaro

                            Text Designer    Sandra Rigney

                         Cover Designer      Anthony Gemmellaro

                         Cover Illustration  Jennifer Kohnke

                            Media Editor     Daniel Sandin

          Full-Service Project Management    Windfall Software

                            Composition      Windfall Software, using  ZzTEX

                         Printer/Binder      Courier Westford

                            Cover Printer    Courier Westford

                            Text Font        Times Ten

Copyright © 2012, 2007, 2003 Pearson Education, Inc., publishing as Addison-Wesley. All rights

reserved. Printed in the United States of America. This publication is protected by Copyright,

and permission should be obtained from the publisher prior to any prohibited reproduction,

storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical,

photocopying, recording, or likewise. To obtain permission(s) to use material from this work,

please submit a written request to Pearson Education, Inc., Permissions Department, One Lake

Street, Upper Saddle River, New Jersey 07458, or you may fax your request to 201-236-3290.

This is the eBook of the printed book and may not include any media, Website access codes or

print supplements that may come packaged with the bound book.

Many of the designations by manufacturers and sellers to distinguish their products are claimed

as trademarks. Where those designations appear in this book, and the publisher was aware of a

trademark claim, the designations have been printed in initial caps or all caps.

Library of Congress Cataloging-in-Publication Data

Levitin, Anany.

Introduction to the design & analysis of algorithms / Anany Levitin. -- 3rd ed.

      p.  cm.

Includes bibliographical references and index.

ISBN-13: 978-0-13-231681-1

ISBN-10: 0-13-231681-1

1. Computer algorithms.     I. Title.  II. Title: Introduction to the design and analysis of

algorithms.

QA76.9.A43L48 2012

005.1--dc23                                                                       2011027089

15 14 13 12 11--CRW--10 9 8 7 6 5 4 3 2 1

                    ISBN 10: 0-13-231681-1

                    ISBN 13: 978-0-13-231681-1
Boston     Columbus   Indianapolis  New York     San Francisco  Upper Saddle River

Amsterdam  Cape Town  Dubai  London      Madrid  Milan  Munich  Paris     Montreal  Toronto

Delhi  Mexico City    Sao Paulo  Sydney  Hong Kong      Seoul  Singapore  Taipei    Tokyo
This page intentionally left blank
    Brief Contents

    New to the Third Edition                              xvii

    Preface                                               xix

1   Introduction                                          1

2   Fundamentals of the Analysis of Algorithm Efficiency  41

3   Brute Force and Exhaustive Search                     97

4   Decrease-and-Conquer                                  131

5   Divide-and-Conquer                                    169

6   Transform-and-Conquer                                 201

7   Space and Time Trade-Offs                             253

8   Dynamic Programming                                   283

9   Greedy Technique                                      315

10  Iterative Improvement                                 345

11  Limitations of Algorithm Power                        387

12  Coping with the Limitations of Algorithm Power        423

    Epilogue                                              471

APPENDIX A

    Useful Formulas for the Analysis of Algorithms        475

APPENDIX B

    Short Tutorial on Recurrence Relations                479

    References                                            493

    Hints to Exercises                                    503

    Index                                                 547

                                                                v
This page intentionally left blank
     Contents

     New to the Third Edition                                   xvii

     Preface                                                    xix

1    Introduction                                               1

1.1  What Is an Algorithm?                                      3

     Exercises 1.1                                              7

1.2  Fundamentals of Algorithmic Problem Solving                9

     Understanding the Problem                                  9

     Ascertaining the Capabilities of the Computational Device  9

     Choosing between Exact and Approximate Problem Solving     11

     Algorithm Design Techniques                                11

     Designing an Algorithm and Data Structures                 12

     Methods of Specifying an Algorithm                         12

     Proving an Algorithm's Correctness                         13

     Analyzing an Algorithm                                     14

     Coding an Algorithm                                        15

     Exercises 1.2                                              17

1.3  Important Problem Types                                    18

     Sorting                                                    19

     Searching                                                  20

     String Processing                                          20

     Graph Problems                                             21

     Combinatorial Problems                                     21

     Geometric Problems                                         22

     Numerical Problems                                         22

     Exercises 1.3                                              23

                                                                      vii
viii  Contents

      1.4  Fundamental Data Structures                           25

           Linear Data Structures                                25

           Graphs                                                28

           Trees                                                 31

           Sets and Dictionaries                                 35

           Exercises 1.4                                         37

           Summary                                               38

      2    Fundamentals of the Analysis of Algorithm

           Efficiency                                            41

      2.1  The Analysis Framework                                42

           Measuring an Input's Size                             43

           Units for Measuring Running Time                      44

           Orders of Growth                                      45

           Worst-Case, Best-Case, and Average-Case Efficiencies  47

           Recapitulation of the Analysis Framework              50

           Exercises 2.1                                         50

      2.2  Asymptotic Notations and Basic Efficiency Classes     52

           Informal Introduction                                 52

           O -notation                                           53

                -notation                                        54

                -notation                                        55

           Useful Property Involving the Asymptotic Notations    55

           Using Limits for Comparing Orders of Growth           56

           Basic Efficiency Classes                              58

           Exercises 2.2                                         58

      2.3  Mathematical Analysis of Nonrecursive Algorithms      61

           Exercises 2.3                                         67

      2.4  Mathematical Analysis of Recursive Algorithms         70

           Exercises 2.4                                         76

      2.5  Example: Computing the nth Fibonacci Number           80

           Exercises 2.5                                         83

      2.6  Empirical Analysis of Algorithms                      84

           Exercises 2.6                                         89

      2.7  Algorithm Visualization                               91

           Summary                                               94
                                        Contents           ix

3    Brute Force and Exhaustive Search                     97

3.1  Selection Sort and Bubble Sort                        98

     Selection Sort                                        98

     Bubble Sort                                           100

     Exercises 3.1                                         102

3.2  Sequential Search and Brute-Force String Matching     104

     Sequential Search                                     104

     Brute-Force String Matching                           105

     Exercises 3.2                                         106

3.3  Closest-Pair and Convex-Hull Problems by Brute Force  108

     Closest-Pair Problem                                  108

     Convex-Hull Problem                                   109

     Exercises 3.3                                         113

3.4  Exhaustive Search                                     115

     Traveling Salesman Problem                            116

     Knapsack Problem                                      116

     Assignment Problem                                    119

     Exercises 3.4                                         120

3.5  Depth-First Search and Breadth-First Search           122

     Depth-First Search                                    122

     Breadth-First Search                                  125

     Exercises 3.5                                         128

     Summary                                               130

4    Decrease-and-Conquer                                  131

4.1  Insertion Sort                                        134

     Exercises 4.1                                         136

4.2  Topological Sorting                                   138

     Exercises 4.2                                         142

4.3  Algorithms for Generating    Combinatorial  Objects   144

     Generating Permutations                               144

     Generating Subsets                                    146

     Exercises 4.3                                         148
x  Contents

   4.4  Decrease-by-a-Constant-Factor Algorithms         150

        Binary Search                                    150

        Fake-Coin Problem                                152

        Russian Peasant Multiplication                   153

        Josephus Problem                                 154

        Exercises 4.4                                    156

   4.5  Variable-Size-Decrease Algorithms                157

        Computing a Median and the Selection Problem     158

        Interpolation Search                             161

        Searching and Insertion in a Binary Search Tree  163

        The Game of Nim                                  164

        Exercises 4.5                                    166

        Summary                                          167

   5    Divide-and-Conquer                               169

   5.1  Mergesort                                        172

        Exercises 5.1                                    174

   5.2  Quicksort                                        176

        Exercises 5.2                                    181

   5.3  Binary Tree Traversals and Related Properties    182

        Exercises 5.3                                    185

   5.4  Multiplication of Large Integers and

        Strassen's Matrix Multiplication                 186

        Multiplication of Large Integers                 187

        Strassen's Matrix Multiplication                 189

        Exercises 5.4                                    191

   5.5  The Closest-Pair and Convex-Hull Problems

        by Divide-and-Conquer                            192

        The Closest-Pair Problem                         192

        Convex-Hull Problem                              195

        Exercises 5.5                                    197

        Summary                                          198
                                           Contents  xi

6    Transform-and-Conquer                           201

6.1  Presorting                                      202

     Exercises 6.1                                   205

6.2  Gaussian Elimination                            208

     LU Decomposition                                212

     Computing a Matrix Inverse                      214

     Computing a Determinant                         215

     Exercises 6.2                                   216

6.3  Balanced Search Trees                           218

     AVL Trees                                       218

     2-3 Trees                                       223

     Exercises 6.3                                   225

6.4  Heaps and Heapsort                              226

     Notion of the Heap                              227

     Heapsort                                        231

     Exercises 6.4                                   233

6.5  Horner's Rule and Binary Exponentiation         234

     Horner's Rule                                   234

     Binary Exponentiation                           236

     Exercises 6.5                                   239

6.6  Problem Reduction                               240

     Computing the Least Common Multiple             241

     Counting Paths in a Graph                       242

     Reduction of Optimization Problems              243

     Linear Programming                              244

     Reduction to Graph Problems                     246

     Exercises 6.6                                   248

     Summary                                         250

7    Space and Time Trade-Offs                       253

7.1  Sorting by Counting                             254

     Exercises 7.1                                   257

7.2  Input Enhancement in String Matching            258

     Horspool's Algorithm                            259
xii  Contents

          Boyer-Moore Algorithm                                       263

          Exercises 7.2                                               267

     7.3  Hashing                                                     269

          Open Hashing (Separate Chaining)                            270

          Closed Hashing (Open Addressing)                            272

          Exercises 7.3                                               274

     7.4  B-Trees                                                     276

          Exercises 7.4                                               279

          Summary                                                     280

     8    Dynamic Programming                                         283

     8.1  Three Basic Examples                                        285

          Exercises 8.1                                               290

     8.2  The Knapsack Problem and Memory Functions                   292

          Memory Functions                                            294

          Exercises 8.2                                               296

     8.3  Optimal Binary Search Trees                                 297

          Exercises 8.3                                               303

     8.4  Warshall's and Floyd's Algorithms                           304

          Warshall's Algorithm                                        304

          Floyd's Algorithm for the All-Pairs Shortest-Paths Problem  308

          Exercises 8.4                                               311

          Summary                                                     312

     9    Greedy Technique                                            315

     9.1  Prim's Algorithm                                            318

          Exercises 9.1                                               322

     9.2  Kruskal's Algorithm                                         325

          Disjoint Subsets and Union-Find Algorithms                  327

          Exercises 9.2                                               331

     9.3  Dijkstra's Algorithm                                        333

          Exercises 9.3                                               337
                                                      Contents  xiii

9.4   Huffman Trees and Codes                                   338

      Exercises 9.4                                             342

      Summary                                                   344

10    Iterative Improvement                                     345

10.1  The Simplex Method                                        346

      Geometric Interpretation of Linear Programming            347

      An Outline of the Simplex Method                          351

      Further Notes on the Simplex Method                       357

      Exercises 10.1                                            359

10.2  The Maximum-Flow Problem                                  361

      Exercises 10.2                                            371

10.3  Maximum Matching in Bipartite Graphs                      372

      Exercises 10.3                                            378

10.4  The Stable Marriage Problem                               380

      Exercises 10.4                                            383

      Summary                                                   384

11    Limitations of Algorithm Power                            387

11.1  Lower-Bound Arguments                                     388

      Trivial Lower Bounds                                      389

      Information-Theoretic Arguments                           390

      Adversary Arguments                                       390

      Problem Reduction                                         391

      Exercises 11.1                                            393

11.2  Decision Trees                                            394

      Decision Trees for Sorting                                395

      Decision Trees for Searching a Sorted Array               397

      Exercises 11.2                                            399

11.3  P , NP , and NP-Complete Problems                         401

      P and NP Problems                                         402

      NP -Complete Problems                                     406

      Exercises 11.3                                            409
xiv  Contents

     11.4  Challenges of Numerical Algorithms                           412

           Exercises 11.4                                               419

           Summary                                                      420

     12    Coping with the Limitations of Algorithm Power               423

     12.1  Backtracking                                                 424

           n-Queens Problem                                             425

           Hamiltonian Circuit Problem                                  426

           Subset-Sum Problem                                           427

           General Remarks                                              428

           Exercises 12.1                                               430

     12.2  Branch-and-Bound                                             432

           Assignment Problem                                           433

           Knapsack Problem                                             436

           Traveling Salesman Problem                                   438

           Exercises 12.2                                               440

     12.3  Approximation Algorithms for NP -Hard Problems               441

           Approximation Algorithms for the Traveling Salesman Problem  443

           Approximation Algorithms for the Knapsack Problem            453

           Exercises 12.3                                               457

     12.4  Algorithms for Solving Nonlinear Equations                   459

           Bisection Method                                             460

           Method of False Position                                     464

           Newton's Method                                              464

           Exercises 12.4                                               467

           Summary                                                      468

           Epilogue                                                     471

     APPENDIX A

           Useful Formulas for the Analysis of Algorithms               475

           Properties of Logarithms                                     475

           Combinatorics                                                475

           Important Summation Formulas                                 476

           Sum Manipulation Rules                                       476
                                               Contents  xv

Approximation of a Sum by a Definite Integral            477

Floor and Ceiling Formulas                               477

Miscellaneous                                            477

APPENDIX B

Short Tutorial on Recurrence Relations                   479

Sequences and Recurrence Relations                       479

Methods for Solving Recurrence Relations                 480

Common Recurrence Types in Algorithm Analysis            485

References                                               493

Hints to Exercises                                       503

Index                                                    547
This page intentionally left blank
New  to the Third Edition

     Reordering of chapters to introduce decrease-and-conquer before divide-

     and-conquer

     Restructuring of chapter 8 on dynamic programming, including all new intro-

     ductory material and new exercises focusing on well-known applications

     More coverage of the applications of the algorithms discussed

     Reordering of select sections throughout the book to achieve a better align-

     ment of specific algorithms and general algorithm design techniques

     Addition of the Lomuto partition and Gray code algorithms

     Seventy new problems added to the end-of-chapter exercises, including algo-

     rithmic puzzles and questions asked during job interviews

                                                                             xvii
This page intentionally left blank
Preface

The most valuable acquisitions in a scientific or technical education are the

general-purpose mental tools which remain serviceable for a life-time.

--George Forsythe, "What to do till the computer scientist comes." (1968)

Algorithms play the central role both in the science and practice of computing.
Recognition of this fact has led to the appearance of a considerable number

of textbooks on the subject. By and large, they follow one of two alternatives

in presenting algorithms. One classifies algorithms according to a problem type.

Such a book would have separate chapters on algorithms for sorting, searching,

graphs, and so on. The advantage of this approach is that it allows an immediate

comparison of, say, the efficiency of different algorithms for the same problem.

The drawback of this approach is that it emphasizes problem types at the expense

of algorithm design techniques.

The second alternative organizes the presentation around algorithm design

techniques. In this organization, algorithms from different areas of computing are

grouped together if they have the same design approach. I share the belief of many

(e.g., [BaY95]) that this organization is more appropriate for a basic course on the

design and analysis of algorithms. There are three principal reasons for emphasis

on algorithm design techniques. First, these techniques provide a student with

tools for designing algorithms for new problems. This makes learning algorithm

design techniques a very valuable endeavor from a practical standpoint. Second,

they seek to classify multitudes of known algorithms according to an underlying

design idea. Learning to see such commonality among algorithms from different

application areas should be a major goal of computer science education. After all,

every science considers classification of its principal subject as a major if not the

central point of its discipline. Third, in my opinion, algorithm design techniques

have utility as general problem solving strategies, applicable to problems beyond

computing.

                                                                                       xix
xx  Preface

    Unfortunately, the traditional classification of algorithm design techniques

    has several serious shortcomings, from both theoretical and educational points

    of view. The most significant of these shortcomings is the failure to classify many

    important algorithms. This limitation has forced the authors of other textbooks

    to depart from the design technique organization and to include chapters dealing

    with specific problem types. Such a switch leads to a loss of course coherence and

    almost unavoidably creates a confusion in students' minds.

    New taxonomy of algorithm design techniques

    My frustration with the shortcomings of the traditional classification of algorithm

    design techniques has motivated me to develop a new taxonomy of them [Lev99],

    which is the basis of this book. Here are the principal advantages of the new

    taxonomy:

    The new taxonomy is more comprehensive than the traditional one. It includes

    several strategies--brute-force, decrease-and-conquer, transform-and-con-

    quer, space and time trade-offs, and iterative improvement--that are rarely

    if ever recognized as important design paradigms.

    The new taxonomy covers naturally many classic algorithms (Euclid's algo-

    rithm, heapsort, search trees, hashing, topological sorting, Gaussian elimi-

    nation, Horner's rule--to name a few) that the traditional taxonomy cannot

    classify. As a result, the new taxonomy makes it possible to present the stan-

    dard body of classic algorithms in a unified and coherent fashion.

    It naturally accommodates the existence of important varieties of several

    design techniques. For example, it recognizes three variations of decrease-

    and-conquer and three variations of transform-and-conquer.

    It is better aligned with analytical methods for the efficiency analysis (see

    Appendix B).

    Design techniques as general problem solving strategies

    Most applications of the design techniques in the book are to classic problems of

    computer science. (The only innovation here is an inclusion of some material on

    numerical algorithms, which are covered within the same general framework.)

    But these design techniques can be considered general problem solving tools,

    whose applications are not limited to traditional computing and mathematical

    problems. Two factors make this point particularly important. First, more and

    more computing applications go beyond the traditional domain, and there are

    reasons to believe that this trend will strengthen in the future. Second, developing

    students' problem solving skills has come to be recognized as a major goal of

    college education. Among all the courses in a computer science curriculum, a

    course on the design and analysis of algorithms is uniquely suitable for this task

    because it can offer a student specific strategies for solving problems.

    I am not proposing that a course on the design and analysis of algorithms

    should become a course on general problem solving. But I do believe that the
                                                 Preface                              xxi

unique opportunity provided by studying the design and analysis of algorithms

should not be missed. Toward this goal, the book includes applications to puzzles

and puzzle-like games. Although using puzzles in teaching algorithms is certainly

not a new idea, the book tries to do this systematically by going well beyond a few

standard examples.

Textbook pedagogy

My goal was to write a text that would not trivialize the subject but would still be

readable by most students on their own. Here are some of the things done toward

this objective.

Sharing the opinion of George Forsythe expressed in the epigraph, I have

sought to stress major ideas underlying the design and analysis of algorithms.

In choosing specific algorithms to illustrate these ideas, I limited the number of

covered algorithms to those that demonstrate an underlying design technique

or an analysis method most clearly. Fortunately, most classic algorithms satisfy

this criterion.

In Chapter 2, which is devoted to efficiency analysis, the methods used for

analyzing nonrecursive algorithms are separated from those typically used for

analyzing recursive algorithms. The chapter also includes sections devoted to

empirical analysis and algorithm visualization.

The narrative is systematically interrupted by questions to the reader. Some

of them are asked rhetorically, in anticipation of a concern or doubt, and are

answered immediately. The goal of the others is to prevent the reader from

drifting through the text without a satisfactory level of comprehension.

Each chapter ends with a summary recapping the most important concepts

and results discussed in the chapter.

The book contains over 600 exercises. Some of them are drills; others make

important points about the material covered in the body of the text or intro-

duce algorithms not covered there at all. A few exercises take advantage of

Internet resources. More difficult problems--there are not many of them--

are marked by special symbols in the Instructor's Manual. (Because marking

problems as difficult may discourage some students from trying to tackle them,

problems are not marked in the book itself.) Puzzles, games, and puzzle-like

questions are marked in the exercises with a special icon.

The book provides hints to all the exercises. Detailed solutions, except for

programming projects, are provided in the Instructor's Manual, available

to qualified adopters through Pearson's Instructor Resource Center. (Please

contact your local Pearson sales representative or go to www.pearsonhighered

.com/irc to access this material.) Slides in PowerPoint are available to all

readers of this book via anonymous ftp at the CS Support site: http://cssupport

.pearsoncmg.com/.
xxii  Preface

      Changes for the third edition

      There are a few changes in the third edition. The most important is the new order of

      the chapters on decrease-and-conquer and divide-and-conquer. There are several

      advantages in introducing decrease-and-conquer before divide-and-conquer:

      Decrease-and-conquer is a simpler strategy than divide-and-conquer.

      Decrease-and-conquer is applicable to more problems than divide-and-con-

      quer.

      The new order makes it possible to discuss insertion sort before mergesort

      and quicksort.

      The idea of array partitioning is now introduced in conjunction with the

      selection problem. I took advantage of an opportunity to do this via the one-

      directional scan employed by Lomuto's algorithm, leaving the two-directional

      scan used by Hoare's partitioning to a later discussion in conjunction with

      quicksort.

      Binary search is now considered in the section devoted to decrease-by-a-

      constant-factor algorithms, where it belongs.

      The second important change is restructuring of Chapter 8 on dynamic pro-

      gramming. Specifically:

      The introductory section is completely new. It contains three basic examples

      that provide a much better introduction to this important technique than

      computing a binomial coefficient, the example used in the first two editions.

      All the exercises for Section 8.1 are new as well; they include well-known

      applications not available in the previous editions.

      I also changed the order of the other sections in this chapter to get a smoother

      progression from the simpler applications to the more advanced ones.

      The other changes include the following. More applications of the algorithms

      discussed are included. The section on the graph-traversal algorithms is moved

      from the decrease-and-conquer chapter to the brute-force and exhaustive-search

      chapter, where it fits better, in my opinion. The Gray code algorithm is added to the

      section dealing with algorithms for generating combinatorial objects. The divide-

      and-conquer algorithm for the closest-pair problem is discussed in more detail.

      Updates include the section on algorithm visualization, approximation algorithms

      for the traveling salesman problem, and, of course, the bibliography.

      I also added about 70 new problems to the exercises. Some of them are algo-

      rithmic puzzles and questions asked during job interviews.

      Prerequisites

      The book assumes that a reader has gone through an introductory programming

      course and a standard course on discrete structures. With such a background,

      he or she should be able to handle the book's material without undue difficulty.
                                                        Preface                    xxiii

Still, fundamental data structures, necessary summation formulas, and recurrence

relations are reviewed in Section 1.4, Appendix A, and Appendix B, respectively.

Calculus is used in only three sections (Section 2.2, 11.4, and 12.4), and to a very

limited degree; if students lack calculus as an assured part of their background, the

relevant portions of these three sections can be omitted without hindering their

understanding of the rest of the material.

Use in the curriculum

The book can serve as a textbook for a basic course on design and analysis of

algorithms organized around algorithm design techniques. It might contain slightly

more material than can be covered in a typical one-semester course. By and large,

portions of Chapters 3 through 12 can be skipped without the danger of making

later parts of the book incomprehensible to the reader. Any portion of the book

can be assigned for self-study. In particular, Sections 2.6 and 2.7 on empirical

analysis and algorithm visualization, respectively, can be assigned in conjunction

with projects.

    Here is a possible plan for a one-semester course; it assumes a 40-class meeting

format.

Lecture  Topic                                                   Sections

1        Introduction                                            1.1­1.3

2, 3     Analysis framework; O,  ,          notations            2.1, 2.2

4        Mathematical analysis of nonrecursive algorithms        2.3

5, 6     Mathematical analysis of recursive algorithms           2.4, 2.5 (+ App. B)

7        Brute-force algorithms                                  3.1, 3.2 (+ 3.3)

8        Exhaustive search                                       3.4

9        Depth-first search and breadth-first search             3.5

10, 11   Decrease-by-one: insertion sort, topological sorting    4.1, 4.2

12       Binary search and other decrease-by-a-constant-         4.4

         factor algorithms

13       Variable-size-decrease algorithms                       4.5

14, 15   Divide-and-conquer: mergesort, quicksort                5.1­5.2

16       Other divide-and-conquer examples                       5.3 or 5.4 or 5.5

17­19    Instance simplification: presorting, Gaussian elimi-    6.1­6.3

         nation, balanced search trees

20       Representation change: heaps and heapsort or            6.4 or 6.5

         Horner's rule and binary exponentiation

21       Problem reduction                                       6.6

22­24    Space-time trade-offs: string matching, hashing, B-     7.2­7.4

         trees

25­27    Dynamic programming algorithms                          3 from 8.1­8.4
xxiv  Preface

      28­30       Greedy algorithms: Prim's, Kruskal's, Dijkstra's,  9.1­9.4

                  Huffman's

      31­33       Iterative improvement algorithms                   3 from 10.1­10.4

      34          Lower-bound arguments                              11.1

      35          Decision trees                                     11.2

      36          P, NP, and NP-complete problems                    11.3

      37          Numerical algorithms                               11.4 (+ 12.4)

      38          Backtracking                                       12.1

      39          Branch-and-bound                                   12.2

      40          Approximation algorithms for NP-hard problems      12.3

      Acknowledgments

      I would like to express my gratitude to the reviewers and many readers who

      have shared with me their opinions about the first two editions of the book and

      suggested improvements and corrections. The third edition has certainly ben-

      efited from the reviews by Andrew Harrington (Loyola University Chicago),

      David Levine (Saint Bonaventure University), Stefano Lombardi (UC Riverside),

      Daniel McKee (Mansfield University), Susan Brilliant (Virginia Commonwealth

      University), David Akers (University of Puget Sound), and two anonymous re-

      viewers.

          My thanks go to all the people at Pearson and their associates who worked

      on my book. I am especially grateful to my editor, Matt Goldstein; the editorial

      assistant, Chelsea Bell; the marketing manager, Yez Alayan; and the production

      supervisor, Kayla Smith-Tarbox. I am also grateful to Richard Camp for copyedit-

      ing the book, Paul Anagnostopoulos of Windfall Software and Jacqui Scarlott for

      its project management and typesetting, and MaryEllen Oliver for proofreading

      the book.

          Finally, I am indebted to two members of my family. Living with a spouse

      writing a book is probably more trying than doing the actual writing. My wife,

      Maria, lived through several years of this, helping me any way she could. And help

      she did: over 400 figures in the book and the Instructor's Manual were created

      by her. My daughter Miriam has been my English prose guru over many years.

      She read large portions of the book and was instrumental in finding the chapter

      epigraphs.

      Anany Levitin

      anany.levitin@villanova.edu

      June 2011
This page intentionally left blank
1

Introduction

   Two ideas lie gleaming on the jeweler's velvet. The first is the calculus, the

   second, the algorithm. The calculus and the rich body of mathematical

   analysis to which it gave rise made modern science possible; but it has been

   the algorithm that has made possible the modern world.

   --David Berlinski, The Advent of the Algorithm, 2000

   Why do you need to study algorithms? If you are going to be a computer
   professional, there are both practical and theoretical reasons to study algo-

   rithms. From a practical standpoint, you have to know a standard set of important

   algorithms from different areas of computing; in addition, you should be able to

   design new algorithms and analyze their efficiency. From the theoretical stand-

   point, the study of algorithms, sometimes called algorithmics, has come to be

   recognized as the cornerstone of computer science. David Harel, in his delightful

   book pointedly titled Algorithmics: the Spirit of Computing, put it as follows:

   Algorithmics is more than a branch of computer science. It is the core of

   computer science, and, in all fairness, can be said to be relevant to most of

   science, business, and technology. [Har92, p. 6]

   But even if you are not a student in a computing-related program, there are

   compelling reasons to study algorithms. To put it bluntly, computer programs

   would not exist without algorithms. And with computer applications becoming

   indispensable in almost all aspects of our professional and personal lives, studying

   algorithms becomes a necessity for more and more people.

   Another reason for studying algorithms is their usefulness in developing an-

   alytical skills. After all, algorithms can be seen as special kinds of solutions to

   problems--not just answers but precisely defined procedures for getting answers.

   Consequently, specific algorithm design techniques can be interpreted as problem-

   solving strategies that can be useful regardless of whether a computer is involved.

   Of course, the precision inherently imposed by algorithmic thinking limits the

   kinds of problems that can be solved with an algorithm. You will not find, for

   example, an algorithm for living a happy life or becoming rich and famous. On

                                                                                         1
2  Introduction

   the other hand, this required precision has an important educational advantage.

   Donald Knuth, one of the most prominent computer scientists in the history of

   algorithmics, put it as follows:

   A person well-trained in computer science knows how to deal with algorithms:

   how to construct them, manipulate them, understand them, analyze them.

   This knowledge is preparation for much more than writing good computer

   programs; it is a general-purpose mental tool that will be a definite aid to

   the understanding of other subjects, whether they be chemistry, linguistics,

   or music, etc. The reason for this may be understood in the following way:

   It has often been said that a person does not really understand something

   until after teaching it to someone else. Actually, a person does not really

   understand something until after teaching it to a computer, i.e., expressing

   it as an algorithm . . . An attempt to formalize things as algorithms leads to

   a much deeper understanding than if we simply try to comprehend things in

   the traditional way. [Knu96, p. 9]

   We take up the notion of algorithm in Section 1.1. As examples, we use three

   algorithms for the same problem: computing the greatest common divisor. There

   are several reasons for this choice. First, it deals with a problem familiar to ev-

   erybody from their middle-school days. Second, it makes the important point that

   the same problem can often be solved by several algorithms. Quite typically, these

   algorithms differ in their idea, level of sophistication, and efficiency. Third, one of

   these algorithms deserves to be introduced first, both because of its age--it ap-

   peared in Euclid's famous treatise more than two thousand years ago--and its

   enduring power and importance. Finally, investigation of these three algorithms

   leads to some general observations about several important properties of algo-

   rithms in general.

   Section 1.2 deals with algorithmic problem solving. There we discuss several

   important issues related to the design and analysis of algorithms. The different

   aspects of algorithmic problem solving range from analysis of the problem and the

   means of expressing an algorithm to establishing its correctness and analyzing its

   efficiency. The section does not contain a magic recipe for designing an algorithm

   for an arbitrary problem. It is a well-established fact that such a recipe does not

   exist. Still, the material of Section 1.2 should be useful for organizing your work

   on designing and analyzing algorithms.

   Section 1.3 is devoted to a few problem types that have proven to be partic-

   ularly important to the study of algorithms and their application. In fact, there

   are textbooks (e.g., [Sed11]) organized around such problem types. I hold the

   view--shared by many others--that an organization based on algorithm design

   techniques is superior. In any case, it is very important to be aware of the princi-

   pal problem types. Not only are they the most commonly encountered problem

   types in real-life applications, they are used throughout the book to demonstrate

   particular algorithm design techniques.

   Section 1.4 contains a review of fundamental data structures. It is meant to

   serve as a reference rather than a deliberate discussion of this topic. If you need
                                       1.1         What Is an Algorithm?                3

     a more detailed exposition, there is a wealth of good books on the subject, most

     of them tailored to a particular programming language.

1.1  What Is an Algorithm?

     Although there is no universally agreed-on wording to describe this notion, there

     is general agreement about what the concept means:

     An algorithm is a sequence of unambiguous instructions for solving a

     problem, i.e., for obtaining a required output for any legitimate input in

     a finite amount of time.

     This definition can be illustrated by a simple diagram (Figure 1.1).

     The reference to "instructions" in the definition implies that there is some-

     thing or someone capable of understanding and following the instructions given.

     We call this a "computer," keeping in mind that before the electronic computer

     was invented, the word "computer" meant a human being involved in perform-

     ing numeric calculations. Nowadays, of course, "computers" are those ubiquitous

     electronic devices that have become indispensable in almost everything we do.

     Note, however, that although the majority of algorithms are indeed intended for

     eventual computer implementation, the notion of algorithm does not depend on

     such an assumption.

     As examples illustrating the notion of the algorithm, we consider in this

     section three methods for solving the same problem: computing the greatest

     common divisor of two integers. These examples will help us to illustrate several

     important points:

     The nonambiguity requirement for each step of an algorithm cannot be com-

     promised.

     The range of inputs for which an algorithm works has to be specified carefully.

     The same algorithm can be represented in several different ways.

     There may exist several algorithms for solving the same problem.

                                       problem

                                       algorithm

                        input          "computer"            output

     FIGURE  1.1  The  notion of  the  algorithm.
4  Introduction

   Algorithms for the same problem can be based on very different ideas and

   can solve the problem with dramatically different speeds.

   Recall that the greatest common divisor of two nonnegative, not-both-zero

   integers m and n, denoted gcd(m, n), is defined as the largest integer that divides

   both m and n evenly, i.e., with a remainder of zero. Euclid of Alexandria (third

   century b.c.) outlined an algorithm for solving this problem in one of the volumes

   of his Elements most famous for its systematic exposition of geometry. In modern

   terms, Euclid's algorithm is based on applying repeatedly the equality

                   gcd(m, n) = gcd(n, m mod n),

   where m mod n is the remainder of the division of m by n, until m mod n is equal

   to 0. Since gcd(m, 0) = m (why?), the last value of m is also the greatest common

   divisor of the initial m and n.

   For example, gcd(60, 24) can be computed as follows:

                   gcd(60, 24) = gcd(24, 12) = gcd(12, 0) = 12.

   (If you are not impressed by this algorithm, try finding the greatest common divisor

   of larger numbers, such as those in Problem 6 in this section's exercises.)

   Here is a more structured description of this algorithm:

   Euclid's algorithm for computing gcd(m, n)

   Step 1 If n = 0, return the value of m as the answer and stop; otherwise,

                 proceed to Step 2.

   Step 2 Divide m by n and assign the value of the remainder to r.

   Step 3 Assign the value of n to m and the value of r to n. Go to Step 1.

   Alternatively, we can express the same algorithm in pseudocode:

   ALGORITHM     Euclid(m, n)

   //Computes gcd(m, n) by Euclid's algorithm

   //Input: Two nonnegative, not-both-zero integers m and n

   //Output: Greatest common divisor of m and n

   while n = 0 do

   r  m mod n

   mn

   nr

   return m

   How do we know that Euclid's algorithm eventually comes to a stop? This

   follows from the observation that the second integer of the pair gets smaller with

   each iteration and it cannot become negative. Indeed, the new value of n on the

   next iteration is m mod n, which is always smaller than n (why?). Hence, the value

   of the second integer eventually becomes 0, and the algorithm stops.
                                  1.1  What Is an Algorithm?                               5

Just as with many other problems, there are several algorithms for computing

the greatest common divisor. Let us look at the other two methods for this prob-

lem. The first is simply based on the definition of the greatest common divisor of

m and n as the largest integer that divides both numbers evenly. Obviously, such

a common divisor cannot be greater than the smaller of these numbers, which we

will denote by t = min{m, n}. So we can start by checking whether t divides both

m and n: if it does, t is the answer; if it does not, we simply decrease t by 1 and

try again. (How do we know that the process will eventually stop?) For example,

for numbers 60 and 24, the algorithm will try first 24, then 23, and so on, until it

reaches 12, where it stops.

Consecutive integer checking algorithm for computing gcd(m, n)

Step 1 Assign the value of min{m, n} to t.

Step 2 Divide m by t. If the remainder of this division is 0, go to Step 3;

        otherwise, go to Step 4.

Step 3 Divide n by t. If the remainder of this division is 0, return the value of

        t as the answer and stop; otherwise, proceed to Step 4.

Step 4 Decrease the value of t by 1. Go to Step 2.

Note that unlike Euclid's algorithm, this algorithm, in the form presented,

does not work correctly when one of its input numbers is zero. This example

illustrates why it is so important to specify the set of an algorithm's inputs explicitly

and carefully.

The third procedure for finding the greatest common divisor should be famil-

iar to you from middle school.

Middle-school procedure for computing gcd(m, n)

Step 1 Find the prime factors of m.

Step 2 Find the prime factors of n.

Step 3  Identify all the common factors in the two prime expansions found in

        Step 1 and Step 2. (If p is a common factor occurring pm and pn times

        in m and n, respectively, it should be repeated min{pm, pn} times.)

Step 4  Compute the product of all the common factors and return it as the

        greatest common divisor of the numbers given.

Thus, for the numbers 60 and 24, we get

                                  60 = 2 . 2 . 3 . 5

                                  24 = 2 . 2 . 2 . 3

                             gcd(60, 24) = 2 . 2 . 3 = 12.

Nostalgia for the days when we learned this method should not prevent us

from noting that the last procedure is much more complex and slower than Euclid's

algorithm. (We will discuss methods for finding and comparing running times

of algorithms in the next chapter.) In addition to inferior efficiency, the middle-

school procedure does not qualify, in the form presented, as a legitimate algorithm.

Why? Because the prime factorization steps are not defined unambiguously: they
6  Introduction

   require a list of prime numbers, and I strongly suspect that your middle-school

   math teacher did not explain how to obtain such a list. This is not a matter

   of unnecessary nitpicking. Unless this issue is resolved, we cannot, say, write a

   program implementing this procedure. Incidentally, Step 3 is also not defined

   clearly enough. Its ambiguity is much easier to rectify than that of the factorization

   steps, however. How would you find common elements in two sorted lists?

   So, let us introduce a simple algorithm for generating consecutive primes not

   exceeding any given integer n > 1. It was probably invented in ancient Greece

   and is known as the sieve of Eratosthenes (ca. 200 b.c.). The algorithm starts by

   initializing a list of prime candidates with consecutive integers from 2 to n. Then,

   on its first iteration, the algorithm eliminates from the list all multiples of 2, i.e., 4,

   6, and so on. Then it moves to the next item on the list, which is 3, and eliminates

   its multiples. (In this straightforward version, there is an overhead because some

   numbers, such as 6, are eliminated more than once.) No pass for number 4 is

   needed: since 4 itself and all its multiples are also multiples of 2, they were already

   eliminated on a previous pass. The next remaining number on the list, which is

   used on the third pass, is 5. The algorithm continues in this fashion until no more

   numbers can be eliminated from the list. The remaining integers of the list are the

   primes needed.

   As an example, consider the application of the algorithm to finding the list of

   primes not exceeding n = 25:

   2  3  4       5   6  7  8  9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24    25

   2  3          5      7     9       11      13      15      17      19      21      23        25

   2  3          5      7             11      13              17      19              23        25

   2  3          5      7             11      13              17      19              23

   For this example, no more passes are needed because they would eliminate num-

   bers already eliminated on previous iterations of the algorithm. The remaining

   numbers on the list are the consecutive primes less than or equal to 25.

   What is the largest number p whose multiples can still remain on the list

   to make further iterations of the algorithm necessary? Before we answer this

   question, let us first note that if p is a number whose multiples are being eliminated

   on the current pass, then the first multiple we should consider is p . p because all its

   smaller multiples 2p, . . . , (p - 1)p have been eliminated on earlier passes through

   the list. This observation helps to avoid eliminating the same number more than

   once. Obviously, p . p should not be greater than n, and therefore p cannot exceed
   n rounded down (denoted                n   using the so-called floor function). We assume

   in the following pseudocode that there is a function available for computing                 n;

   alternatively, we could check the inequality p . p  n as the loop continuation

   condition there.

   ALGORITHM            Sieve(n)

   //Implements the sieve of Eratosthenes

   //Input: A positive integer n > 1

   //Output: Array L of all prime numbers less than or equal to n
                                   1.1  What Is an Algorithm?                         7

    for p  2 to ndo A[p]  p
    for p  2 to      n   do   //see note before pseudocode

        if A[p] = 0           //p hasn't been eliminated on previous  passes

              j  pp

              while j  n do

                 A[j ]  0     //mark element as eliminated

                 j j +p

    //copy the remaining elements of A to array L of the primes

    i0

    for p  2 to n do

        if A[p] = 0

              L[i]  A[p]

              ii+1

    return L

    So now we can incorporate the sieve of Eratosthenes into the middle-school

procedure to get a legitimate algorithm for computing the greatest common divi-

sor of two positive integers. Note that special care needs to be exercised if one or

both input numbers are equal to 1: because mathematicians do not consider 1 to

be a prime number, strictly speaking, the method does not work for such inputs.

    Before we leave this section, one more comment is in order. The exam-

ples considered in this section notwithstanding, the majority of algorithms in use

today--even those that are implemented as computer programs--do not deal with

mathematical problems. Look around for algorithms helping us through our daily

routines, both professional and personal. May this ubiquity of algorithms in to-

day's world strengthen your resolve to learn more about these fascinating engines

of the information age.

Exercises 1.1

1.  Do some research on al-Khorezmi (also al-Khwarizmi), the man from whose

    name the word "algorithm" is derived. In particular, you should learn what

    the origins of the words "algorithm" and "algebra" have in common.

2.  Given that the official purpose of the U.S. patent system is the promotion

    of the "useful arts," do you think algorithms are patentable in this country?

    Should they be?

3.  a.  Write down driving directions for going from your school to your home

        with the precision required from an algorithm's description.

    b. Write down a recipe for cooking your favorite dish with the precision

        required by an algorithm.       

4.  Design an algorithm for computing     n  for any positive integer n. Besides

    assignment and comparison, your algorithm may only use the four basic

    arithmetical operations.
8  Introduction

   5.   Design an algorithm to find all the common elements in two sorted lists of

        numbers. For example, for the lists 2, 5, 5, 5 and 2, 2, 3, 5, 5, 7, the output

        should be 2, 5, 5. What is the maximum number of comparisons your algorithm

        makes if the lengths of the two given lists are m and n, respectively?

   6.   a.  Find gcd(31415, 14142) by applying Euclid's algorithm.

        b.  Estimate how many times faster it will be to find gcd(31415, 14142) by

            Euclid's algorithm compared with the algorithm based on checking con-

            secutive integers from min{m, n} down to gcd(m, n).

   7.   Prove the equality gcd(m, n) = gcd(n, m mod n) for every pair of positive

        integers m and n.

   8.   What does Euclid's algorithm do for a pair of integers in which the first is

        smaller than the second? What is the maximum number of times this can

        happen during the algorithm's execution on such an input?

   9.   a.  What is the minimum number     of  divisions  made   by   Euclid's  algorithm

            among all inputs 1  m, n  10?

        b.  What is the maximum number     of  divisions  made   by   Euclid's  algorithm

            among all inputs 1  m, n  10?

   10.  a.  Euclid's algorithm, as presented in Euclid's treatise, uses subtractions

            rather than integer divisions. Write pseudocode for this version of Euclid's

            algorithm.

        b.  Euclid's game (see [Bog]) starts with two unequal positive integers on the

            board. Two players move in turn. On each move, a player has to write on

            the board a positive number equal to the difference of two numbers already

            on the board; this number must be new, i.e., different from all the numbers

            already on the board. The player who cannot move loses the game. Should

            you choose to move first or second in this game?

   11.  The extended Euclid's algorithm determines not only the greatest common

        divisor d of two positive integers m and n but also integers (not necessarily

        positive) x and y, such that mx + ny = d.

        a.  Look up a description of the extended Euclid's algorithm (see, e.g., [KnuI,

            p. 13]) and implement it in the language of your choice.

        b. Modify your program to find integer solutions to the Diophantine equation

            ax + by = c with any set of integer coefficients a, b, and c.

   12.  Locker doors    There are n lockers in a hallway, numbered sequentially from

        1 to n. Initially, all the locker doors are closed. You make n passes by the

        lockers, each time starting with locker #1. On the ith pass, i = 1, 2, . . . , n, you

        toggle the door of every ith locker: if the door is closed, you open it; if it is

        open, you close it. After the last pass, which locker doors are open and which

        are closed? How many of them are open?
                  1.2  Fundamentals of Algorithmic Problem Solving                            9

1.2  Fundamentals of Algorithmic Problem Solving

     Let us start by reiterating an important point made in the introduction to this

     chapter:

              We can consider algorithms to be procedural solutions to problems.

     These solutions are not answers but specific instructions for getting answers. It is

     this emphasis on precisely defined constructive procedures that makes computer

     science distinct from other disciplines. In particular, this distinguishes it from the-

     oretical mathematics, whose practitioners are typically satisfied with just proving

     the existence of a solution to a problem and, possibly, investigating the solution's

     properties.

     We now list and briefly discuss a sequence of steps one typically goes through

     in designing and analyzing an algorithm (Figure 1.2).

     Understanding the Problem

     From a practical perspective, the first thing you need to do before designing an

     algorithm is to understand completely the problem given. Read the problem's

     description carefully and ask questions if you have any doubts about the problem,

     do a few small examples by hand, think about special cases, and ask questions

     again if needed.

     There are a few types of problems that arise in computing applications quite

     often. We review them in the next section. If the problem in question is one of

     them, you might be able to use a known algorithm for solving it. Of course, it

     helps to understand how such an algorithm works and to know its strengths and

     weaknesses, especially if you have to choose among several available algorithms.

     But often you will not find a readily available algorithm and will have to design

     your own. The sequence of steps outlined in this section should help you in this

     exciting but not always easy task.

     An input to an algorithm specifies an instance of the problem the algorithm

     solves. It is very important to specify exactly the set of instances the algorithm

     needs to handle. (As an example, recall the variations in the set of instances for

     the three greatest common divisor algorithms discussed in the previous section.)

     If you fail to do this, your algorithm may work correctly for a majority of inputs

     but crash on some "boundary" value. Remember that a correct algorithm is not

     one that works most of the time, but one that works correctly for all legitimate

     inputs.

     Do not skimp on this first step of the algorithmic problem-solving process;

     otherwise, you will run the risk of unnecessary rework.

     Ascertaining the Capabilities of the Computational Device

     Once you completely understand a problem, you need to ascertain the capabilities

     of the computational device the algorithm is intended for. The vast majority of
10  Introduction

                  Understand the problem

                  Decide on:

                  computational means,

                  exact vs. approximate solving,

                  algorithm design technique

                  Design an algorithm

                  Prove correctness

                  Analyze the algorithm

                  Code the algorithm

    FIGURE  1.2   Algorithm design and analysis process.

    algorithms in use today are still destined to be programmed for a computer closely

    resembling the von Neumann machine--a computer architecture outlined by

    the prominent Hungarian-American mathematician John von Neumann (1903­

    1957), in collaboration with A. Burks and H. Goldstine, in 1946. The essence of

    this architecture is captured by the so-called random-access machine (RAM).

    Its central assumption is that instructions are executed one after another, one

    operation at a time. Accordingly, algorithms designed to be executed on such

    machines are called sequential algorithms.

    The central assumption of the RAM model does not hold for some newer

    computers that can execute operations concurrently, i.e., in parallel. Algorithms

    that take advantage of this capability are called parallel algorithms. Still, studying

    the classic techniques for design and analysis of algorithms under the RAM model

    remains the cornerstone of algorithmics for the foreseeable future.
1.2                 Fundamentals of Algorithmic Problem Solving                        11

Should you worry about the speed and amount of memory of a computer at

your disposal? If you are designing an algorithm as a scientific exercise, the answer

is a qualified no. As you will see in Section 2.1, most computer scientists prefer to

study algorithms in terms independent of specification parameters for a particular

computer. If you are designing an algorithm as a practical tool, the answer may

depend on a problem you need to solve. Even the "slow" computers of today are

almost unimaginably fast. Consequently, in many situations you need not worry

about a computer being too slow for the task. There are important problems,

however, that are very complex by their nature, or have to process huge volumes

of data, or deal with applications where the time is critical. In such situations,

it is imperative to be aware of the speed and memory available on a particular

computer system.

Choosing between Exact and Approximate Problem Solving

The next principal decision is to choose between solving the problem exactly or

solving it approximately. In the former case, an algorithm is called an exact algo-

rithm; in the latter case, an algorithm is called an approximation algorithm. Why

would one opt for an approximation algorithm? First, there are important prob-

lems that simply cannot be solved exactly for most of their instances; examples

include extracting square roots, solving nonlinear equations, and evaluating def-

inite integrals. Second, available algorithms for solving a problem exactly can be

unacceptably slow because of the problem's intrinsic complexity. This happens, in

particular, for many problems involving a very large number of choices; you will

see examples of such difficult problems in Chapters 3, 11, and 12. Third, an ap-

proximation algorithm can be a part of a more sophisticated algorithm that solves

a problem exactly.

Algorithm Design Techniques

Now, with all the components of the algorithmic problem solving in place, how do

you design an algorithm to solve a given problem? This is the main question this

book seeks to answer by teaching you several general design techniques.

What is an algorithm design technique?

An algorithm design technique (or "strategy" or "paradigm") is a general

approach to solving problems algorithmically that is applicable to a variety

of problems from different areas of computing.

Check this book's table of contents and you will see that a majority of its

chapters are devoted to individual design techniques. They distill a few key ideas

that have proven to be useful in designing algorithms. Learning these techniques

is of utmost importance for the following reasons.

First, they provide guidance for designing algorithms for new problems, i.e.,

problems for which there is no known satisfactory algorithm. Therefore--to use

the language of a famous proverb--learning such techniques is akin to learning
12  Introduction

    to fish as opposed to being given a fish caught by somebody else. It is not true, of

    course, that each of these general techniques will be necessarily applicable to every

    problem you may encounter. But taken together, they do constitute a powerful

    collection of tools that you will find quite handy in your studies and work.

    Second, algorithms are the cornerstone of computer science. Every science is

    interested in classifying its principal subject, and computer science is no exception.

    Algorithm design techniques make it possible to classify algorithms according

    to an underlying design idea; therefore, they can serve as a natural way to both

    categorize and study algorithms.

    Designing an Algorithm and Data Structures

    While the algorithm design techniques do provide a powerful set of general ap-

    proaches to algorithmic problem solving, designing an algorithm for a particular

    problem may still be a challenging task. Some design techniques can be simply

    inapplicable to the problem in question. Sometimes, several techniques need to

    be combined, and there are algorithms that are hard to pinpoint as applications

    of the known design techniques. Even when a particular design technique is ap-

    plicable, getting an algorithm often requires a nontrivial ingenuity on the part of

    the algorithm designer. With practice, both tasks--choosing among the general

    techniques and applying them--get easier, but they are rarely easy.

    Of course, one should pay close attention to choosing data structures appro-

    priate for the operations performed by the algorithm. For example, the sieve of

    Eratosthenes introduced in Section 1.1 would run longer if we used a linked list

    instead of an array in its implementation (why?). Also note that some of the al-

    gorithm design techniques discussed in Chapters 6 and 7 depend intimately on

    structuring or restructuring data specifying a problem's instance. Many years ago,

    an influential textbook proclaimed the fundamental importance of both algo-

    rithms and data structures for computer programming by its very title: Algorithms

    + Data Structures = Programs [Wir76]. In the new world of object-oriented pro-

    gramming, data structures remain crucially important for both design and analysis

    of algorithms. We review basic data structures in Section 1.4.

    Methods of Specifying an Algorithm

    Once you have designed an algorithm, you need to specify it in some fashion. In

    Section 1.1, to give you an example, Euclid's algorithm is described in words (in a

    free and also a step-by-step form) and in pseudocode. These are the two options

    that are most widely used nowadays for specifying algorithms.

    Using a natural language has an obvious appeal; however, the inherent ambi-

    guity of any natural language makes a succinct and clear description of algorithms

    surprisingly difficult. Nevertheless, being able to do this is an important skill that

    you should strive to develop in the process of learning algorithms.

    Pseudocode is a mixture of a natural language and programming language-

    like constructs. Pseudocode is usually more precise than natural language, and its
               1.2  Fundamentals of Algorithmic Problem Solving                        13

usage often yields more succinct algorithm descriptions. Surprisingly, computer

scientists have never agreed on a single form of pseudocode, leaving textbook

authors with a need to design their own "dialects." Fortunately, these dialects are

so close to each other that anyone familiar with a modern programming language

should be able to understand them all.

This book's dialect was selected to cause minimal difficulty for a reader. For

the sake of simplicity, we omit declarations of variables and use indentation to

show the scope of such statements as for, if, and while. As you saw in the previous

section, we use an arrow "" for the assignment operation and two slashes "//"

for comments.

In the earlier days of computing, the dominant vehicle for specifying algo-

rithms was a flowchart, a method of expressing an algorithm by a collection of

connected geometric shapes containing descriptions of the algorithm's steps. This

representation technique has proved to be inconvenient for all but very simple

algorithms; nowadays, it can be found only in old algorithm books.

The state of the art of computing has not yet reached a point where an

algorithm's description--be it in a natural language or pseudocode--can be fed

into an electronic computer directly. Instead, it needs to be converted into a

computer program written in a particular computer language. We can look at such

a program as yet another way of specifying the algorithm, although it is preferable

to consider it as the algorithm's implementation.

Proving an Algorithm's Correctness

Once an algorithm has been specified, you have to prove its correctness. That is,

you have to prove that the algorithm yields a required result for every legitimate

input in a finite amount of time. For example, the correctness of Euclid's algorithm

for computing the greatest common divisor stems from the correctness of the

equality gcd(m, n) = gcd(n, m mod n) (which, in turn, needs a proof; see Problem

7 in Exercises 1.1), the simple observation that the second integer gets smaller on

every iteration of the algorithm, and the fact that the algorithm stops when the

second integer becomes 0.

For some algorithms, a proof of correctness is quite easy; for others, it can be

quite complex. A common technique for proving correctness is to use mathemati-

cal induction because an algorithm's iterations provide a natural sequence of steps

needed for such proofs. It might be worth mentioning that although tracing the

algorithm's performance for a few specific inputs can be a very worthwhile activ-

ity, it cannot prove the algorithm's correctness conclusively. But in order to show

that an algorithm is incorrect, you need just one instance of its input for which the

algorithm fails.

The notion of correctness for approximation algorithms is less straightforward

than it is for exact algorithms. For an approximation algorithm, we usually would

like to be able to show that the error produced by the algorithm does not exceed

a predefined limit. You can find examples of such investigations in Chapter 12.
14  Introduction

    Analyzing an Algorithm

    We usually want our algorithms to possess several qualities. After correctness,

    by far the most important is efficiency. In fact, there are two kinds of algorithm

    efficiency: time efficiency, indicating how fast the algorithm runs, and space ef-

    ficiency, indicating how much extra memory it uses. A general framework and

    specific techniques for analyzing an algorithm's efficiency appear in Chapter 2.

    Another desirable characteristic of an algorithm is simplicity. Unlike effi-

    ciency, which can be precisely defined and investigated with mathematical rigor,

    simplicity, like beauty, is to a considerable degree in the eye of the beholder. For

    example, most people would agree that Euclid's algorithm is simpler than the

    middle-school procedure for computing gcd(m, n), but it is not clear whether Eu-

    clid's algorithm is simpler than the consecutive integer checking algorithm. Still,

    simplicity is an important algorithm characteristic to strive for. Why? Because sim-

    pler algorithms are easier to understand and easier to program; consequently, the

    resulting programs usually contain fewer bugs. There is also the undeniable aes-

    thetic appeal of simplicity. Sometimes simpler algorithms are also more efficient

    than more complicated alternatives. Unfortunately, it is not always true, in which

    case a judicious compromise needs to be made.

    Yet another desirable characteristic of an algorithm is generality. There are,

    in fact, two issues here: generality of the problem the algorithm solves and the

    set of inputs it accepts. On the first issue, note that it is sometimes easier to

    design an algorithm for a problem posed in more general terms. Consider, for

    example, the problem of determining whether two integers are relatively prime,

    i.e., whether their only common divisor is equal to 1. It is easier to design an

    algorithm for a more general problem of computing the greatest common divisor

    of two integers and, to solve the former problem, check whether the gcd is 1 or

    not. There are situations, however, where designing a more general algorithm is

    unnecessary or difficult or even impossible. For example, it is unnecessary to sort

    a list of n numbers to find its median, which is its  n/2 th smallest element. To give

    another example, the standard formula for roots of a quadratic equation cannot

    be generalized to handle polynomials of arbitrary degrees.

    As to the set of inputs, your main concern should be designing an algorithm

    that can handle a set of inputs that is natural for the problem at hand. For example,

    excluding integers equal to 1 as possible inputs for a greatest common divisor

    algorithm would be quite unnatural. On the other hand, although the standard

    formula for the roots of a quadratic equation holds for complex coefficients, we

    would normally not implement it on this level of generality unless this capability

    is explicitly required.

    If you are not satisfied with the algorithm's efficiency, simplicity, or generality,

    you must return to the drawing board and redesign the algorithm. In fact, even if

    your evaluation is positive, it is still worth searching for other algorithmic solutions.

    Recall the three different algorithms in the previous section for computing the

    greatest common divisor: generally, you should not expect to get the best algorithm

    on the first try. At the very least, you should try to fine-tune the algorithm you
                    1.2  Fundamentals of Algorithmic Problem Solving                                        15

    already have. For example, we made several improvements in our implementation

    of the sieve of Eratosthenes compared with its initial outline in Section 1.1. (Can

    you identify them?) You will do well if you keep in mind the following observation

    of Antoine de Saint-Exupe´ ry, the French writer, pilot, and aircraft designer: "A

    designer knows he has arrived at perfection not when there is no longer anything

    to add, but when there is no longer anything to take away."1

    Coding an Algorithm

    Most algorithms are destined to be ultimately implemented as computer pro-

    grams. Programming an algorithm presents both a peril and an opportunity. The

    peril lies in the possibility of making the transition from an algorithm to a pro-

    gram either incorrectly or very inefficiently. Some influential computer scientists

    strongly believe that unless the correctness of a computer program is proven

    with full mathematical rigor, the program cannot be considered correct. They

    have developed special techniques for doing such proofs (see [Gri81]), but the

    power of these techniques of formal verification is limited so far to very small

    programs.

    As a practical matter, the validity of programs is still established by testing.

    Testing of computer programs is an art rather than a science, but that does not

    mean that there is nothing in it to learn. Look up books devoted to testing

    and debugging; even more important, test and debug your program thoroughly

    whenever you implement an algorithm.

    Also note that throughout the book, we assume that inputs to algorithms

    belong to the specified sets and hence require no verification. When implementing

    algorithms as programs to be used in actual applications, you should provide such

    verifications.

    Of course, implementing an algorithm correctly is necessary but not sufficient:

    you would not like to diminish your algorithm's power by an inefficient implemen-

    tation. Modern compilers do provide a certain safety net in this regard, especially

    when they are used in their code optimization mode. Still, you need to be aware

    of such standard tricks as computing a loop's invariant (an expression that does

    not change its value) outside the loop, collecting common subexpressions, replac-

    ing expensive operations by cheap ones, and so on. (See [Ker99] and [Ben00] for

    a good discussion of code tuning and other issues related to algorithm program-

    ming.) Typically, such improvements can speed up a program only by a constant

    factor, whereas a better algorithm can make a difference in running time by orders

    of magnitude. But once an algorithm is selected, a 10­50% speedup may be worth

    an effort.

1.  I found this call for design simplicity in an essay collection by Jon Bentley [Ben00]; the essays deal

    with a variety of issues in algorithm design and implementation and are justifiably titled Programming

    Pearls. I wholeheartedly recommend the writings of both Jon Bentley and Antoine de Saint-Exupe´ ry.
16  Introduction

    A working program provides an additional opportunity in allowing an em-

    pirical analysis of the underlying algorithm. Such an analysis is based on timing

    the program on several inputs and then analyzing the results obtained. We dis-

    cuss the advantages and disadvantages of this approach to analyzing algorithms

    in Section 2.6.

    In conclusion, let us emphasize again the main lesson of the process depicted

    in Figure 1.2:

    As a rule, a good algorithm is a result of repeated effort and rework.

    Even if you have been fortunate enough to get an algorithmic idea that seems

    perfect, you should still try to see whether it can be improved.

    Actually, this is good news since it makes the ultimate result so much more

    enjoyable. (Yes, I did think of naming this book The Joy of Algorithms.) On the

    other hand, how does one know when to stop? In the real world, more often than

    not a project's schedule or the impatience of your boss will stop you. And so it

    should be: perfection is expensive and in fact not always called for. Designing

    an algorithm is an engineering-like activity that calls for compromises among

    competing goals under the constraints of available resources, with the designer's

    time being one of the resources.

    In the academic world, the question leads to an interesting but usually difficult

    investigation of an algorithm's optimality. Actually, this question is not about the

    efficiency of an algorithm but about the complexity of the problem it solves: What

    is the minimum amount of effort any algorithm will need to exert to solve the

    problem? For some problems, the answer to this question is known. For example,

    any algorithm that sorts an array by comparing values of its elements needs about

    n log2 n comparisons for some arrays of size n (see Section 11.2). But for many

    seemingly easy problems such as integer multiplication, computer scientists do

    not yet have a final answer.

    Another important issue of algorithmic problem solving is the question of

    whether or not every problem can be solved by an algorithm. We are not talking

    here about problems that do not have a solution, such as finding real roots of

    a quadratic equation with a negative discriminant. For such cases, an output

    indicating that the problem does not have a solution is all we can and should

    expect from an algorithm. Nor are we talking about ambiguously stated problems.

    Even some unambiguous problems that must have a simple yes or no answer are

    "undecidable," i.e., unsolvable by any algorithm. An important example of such

    a problem appears in Section 11.3. Fortunately, a vast majority of problems in

    practical computing can be solved by an algorithm.

    Before          leaving  this  section,  let  us   be  sure  that  you  do  not  have   the

    misconception--possibly        caused    by   the  somewhat  mechanical     nature  of  the

    diagram of Figure 1.2--that designing an algorithm is a dull activity. There is

    nothing further from the truth: inventing (or discovering?) algorithms is a very

    creative and rewarding process. This book is designed to convince you that this is

    the case.
                 1.2      Fundamentals of Algorithmic Problem Solving                        17

Exercises 1.2

1.  Old World puzzle            A peasant finds himself on a riverbank with a wolf, a goat,

    and a head of cabbage. He needs to transport all three to the other side of the

    river in his boat. However, the boat has room for only the peasant himself

    and one other item (either the wolf, the goat, or the cabbage). In his absence,

    the wolf would eat the goat, and the goat would eat the cabbage. Solve this

    problem for the peasant or prove it has no solution. (Note: The peasant is a

    vegetarian but does not like cabbage and hence can eat neither the goat nor

    the cabbage to help him solve the problem. And it goes without saying that

    the wolf is a protected species.)

2.  New World puzzle            There are four people who want to cross a rickety bridge;

    they all begin on the same side. You have 17 minutes to get them all across

    to the other side. It is night, and they have one flashlight. A maximum of two

    people can cross the bridge at one time. Any party that crosses, either one or

    two people, must have the flashlight with them. The flashlight must be walked

    back and forth; it cannot be thrown, for example. Person 1 takes 1 minute

    to cross the bridge, person 2 takes 2 minutes, person 3 takes 5 minutes, and

    person 4 takes 10 minutes. A pair must walk together at the rate of the slower

    person's pace. (Note: According to a rumor on the Internet, interviewers at a

    well-known software company located near Seattle have given this problem

    to interviewees.)

3.  Which of the following formulas can be considered an algorithm for comput-

    ing the area of a triangle whose side lengths are given positive numbers a, b,

    and c?

    a.  S=       p(p - a)(p - b)(p - c), where p = (a + b + c)/2

    b.  S=    1  bc  sin  A,  where     A  is  the  angle  between  sides  b  and  c

              2

    c.  S  =  1  aha  ,  where  ha  is  the    height  to  base  a

              2

4.  Write pseudocode for an algorithm for finding real roots of equation ax2 +

    bx + c = 0 for arbitrary real coefficients a, b, and c. (You may assume the

    availability of the square root function sqrt (x).)

5.  Describe the standard algorithm for finding the binary representation of a

    positive decimal integer

    a.  in English.

    b. in pseudocode.

6.  Describe the algorithm used by your favorite ATM machine in dispensing

    cash. (You may give your description in either English or pseudocode, which-

    ever you find more convenient.)

7.  a.  Can the problem of computing the number                     be solved exactly?

    b. How many instances does this problem have?

    c.  Look up an algorithm for this problem on the Internet.
18       Introduction

         8.   Give an example of a problem other than computing the greatest common

              divisor for which you know more than one algorithm. Which of them is

              simpler? Which is more efficient?

         9.   Consider the following algorithm for finding the distance between the two

              closest elements in an array of numbers.

              ALGORITHM          MinDistance(A[0..n - 1])

              //Input: Array A[0..n - 1] of numbers

              //Output: Minimum distance between two of its elements

              dmin  

                       for i  0 to n - 1 do

                          for j  0 to n - 1 do

                                 if i = j and |A[i] - A[j ]| < dmin

                                 dmin  |A[i] - A[j ]|

              return dmin

              Make as many improvements as you can in this algorithmic solution to the

              problem. If you need to, you may change the algorithm altogether; if not,

              improve the implementation given.

         10.  One of the most influential books on problem solving, titled How To Solve

              It [Pol57], was written by the Hungarian-American mathematician George

              Po´ lya (1887­1985). Po´ lya summarized his ideas in a four-point summary. Find

              this summary on the Internet or, better yet, in his book, and compare it with

              the plan outlined in Section 1.2. What do they have in common? How are they

              different?

    1.3  Important Problem Types

         In the limitless sea of problems one encounters in computing, there are a few

         areas that have attracted particular attention from researchers. By and large,

         their interest has been driven either by the problem's practical importance or by

         some specific characteristics making the problem an interesting research subject;

         fortunately, these two motivating forces reinforce each other in most cases.

              In this section, we are going to introduce the most important problem types:

              Sorting

              Searching

              String processing

              Graph problems

              Combinatorial problems

              Geometric problems

              Numerical problems
         1.3  Important Problem Types                                                    19

These problems are used in subsequent chapters of the book to illustrate

different algorithm design techniques and methods of algorithm analysis.

Sorting

The sorting problem is to rearrange the items of a given list in nondecreasing

order. Of course, for this problem to be meaningful, the nature of the list items

must allow such an ordering. (Mathematicians would say that there must exist

a relation of total ordering.) As a practical matter, we usually need to sort lists

of numbers, characters from an alphabet, character strings, and, most important,

records similar to those maintained by schools about their students, libraries about

their holdings, and companies about their employees. In the case of records, we

need to choose a piece of information to guide sorting. For example, we can choose

to sort student records in alphabetical order of names or by student number or by

student grade-point average. Such a specially chosen piece of information is called

a key. Computer scientists often talk about sorting a list of keys even when the list's

items are not records but, say, just integers.

Why would we want a sorted list? To begin with, a sorted list can be a required

output of a task such as ranking Internet search results or ranking students by their

GPA scores. Further, sorting makes many questions about the list easier to answer.

The most important of them is searching: it is why dictionaries, telephone books,

class lists, and so on are sorted. You will see other examples of the usefulness of

list presorting in Section 6.1. In a similar vein, sorting is used as an auxiliary step

in several important algorithms in other areas, e.g., geometric algorithms and data

compression. The greedy approach--an important algorithm design technique

discussed later in the book--requires a sorted input.

By now, computer scientists have discovered dozens of different sorting algo-

rithms. In fact, inventing a new sorting algorithm has been likened to designing

the proverbial mousetrap. And I am happy to report that the hunt for a better

sorting mousetrap continues. This perseverance is admirable in view of the fol-

lowing facts. On the one hand, there are a few good sorting algorithms that sort

an arbitrary array of size n using about n log2 n comparisons. On the other hand,

no algorithm that sorts by key comparisons (as opposed to, say, comparing small

pieces of keys) can do substantially better than that.

There is a reason for this embarrassment of algorithmic riches in the land

of sorting. Although some algorithms are indeed better than others, there is no

algorithm that would be the best solution in all situations. Some of the algorithms

are simple but relatively slow, while others are faster but more complex; some

work better on randomly ordered inputs, while others do better on almost-sorted

lists; some are suitable only for lists residing in the fast memory, while others can

be adapted for sorting large files stored on a disk; and so on.

Two properties of sorting algorithms deserve special mention. A sorting algo-

rithm is called stable if it preserves the relative order of any two equal elements in

its input. In other words, if an input list contains two equal elements in positions

i and j  where i < j, then in the sorted list they have to be in positions i  and j ,
20  Introduction

    respectively, such that i  < j . This property can be desirable if, for example, we

    have a list of students sorted alphabetically and we want to sort it according to

    student GPA: a stable algorithm will yield a list in which students with the same

    GPA will still be sorted alphabetically. Generally speaking, algorithms that can

    exchange keys located far apart are not stable, but they usually work faster; you

    will see how this general comment applies to important sorting algorithms later

    in the book.

    The second notable feature of a sorting algorithm is the amount of extra

    memory the algorithm requires. An algorithm is said to be in-place if it does

    not require extra memory, except, possibly, for a few memory units. There are

    important sorting algorithms that are in-place and those that are not.

    Searching

    The searching problem deals with finding a given value, called a search key, in a

    given set (or a multiset, which permits several elements to have the same value).

    There are plenty of searching algorithms to choose from. They range from the

    straightforward sequential search to a spectacularly efficient but limited binary

    search and algorithms based on representing the underlying set in a different form

    more conducive to searching. The latter algorithms are of particular importance

    for real-world applications because they are indispensable for storing and retriev-

    ing information from large databases.

    For searching, too, there is no single algorithm that fits all situations best.

    Some algorithms work faster than others but require more memory; some are

    very fast but applicable only to sorted arrays; and so on. Unlike with sorting

    algorithms, there is no stability problem, but different issues arise. Specifically,

    in applications where the underlying data may change frequently relative to the

    number of searches, searching has to be considered in conjunction with two other

    operations: an addition to and deletion from the data set of an item. In such

    situations, data structures and algorithms should be chosen to strike a balance

    among the requirements of each operation. Also, organizing very large data sets

    for efficient searching poses special challenges with important implications for

    real-world applications.

    String Processing

    In recent decades, the rapid proliferation of applications dealing with nonnumer-

    ical data has intensified the interest of researchers and computing practitioners in

    string-handling algorithms. A string is a sequence of characters from an alphabet.

    Strings of particular interest are text strings, which comprise letters, numbers, and

    special characters; bit strings, which comprise zeros and ones; and gene sequences,

    which can be modeled by strings of characters from the four-character alphabet {A,

    C, G, T}. It should be pointed out, however, that string-processing algorithms have

    been important for computer science for a long time in conjunction with computer

    languages and compiling issues.
                        1.3          Important Problem Types                            21

One particular problem--that of searching for a given word in a text--has

attracted special attention from researchers. They call it string matching. Several

algorithms that exploit the special nature of this type of searching have been

invented. We introduce one very simple algorithm in Chapter 3 and discuss two

algorithms based on a remarkable idea by R. Boyer and J. Moore in Chapter 7.

Graph Problems

One of the oldest and most interesting areas in algorithmics is graph algorithms.

Informally, a graph can be thought of as a collection of points called vertices, some

of which are connected by line segments called edges. (A more formal definition

is given in the next section.) Graphs are an interesting subject to study, for both

theoretical and practical reasons. Graphs can be used for modeling a wide variety

of applications, including transportation, communication, social and economic

networks, project scheduling, and games. Studying different technical and social

aspects of the Internet in particular is one of the active areas of current research

involving computer scientists, economists, and social scientists (see, e.g., [Eas10]).

Basic graph algorithms include graph-traversal algorithms (how can one reach

all the points in a network?), shortest-path algorithms (what is the best route be-

tween two cities?), and topological sorting for graphs with directed edges (is a set

of courses with their prerequisites consistent or self-contradictory?). Fortunately,

these algorithms can be considered illustrations of general design techniques; ac-

cordingly, you will find them in corresponding chapters of the book.

Some graph problems are computationally very hard; the most well-known

examples are the traveling salesman problem and the graph-coloring problem.

The traveling salesman problem (TSP) is the problem of finding the shortest tour

through n cities that visits every city exactly once. In addition to obvious appli-

cations involving route planning, it arises in such modern applications as circuit

board and VLSI chip fabrication, X-ray crystallography, and genetic engineer-

ing. The graph-coloring problem seeks to assign the smallest number of colors to

the vertices of a graph so that no two adjacent vertices are the same color. This

problem arises in several applications, such as event scheduling: if the events are

represented by vertices that are connected by an edge if and only if the correspond-

ing events cannot be scheduled at the same time, a solution to the graph-coloring

problem yields an optimal schedule.

Combinatorial Problems

From a more abstract perspective, the traveling salesman problem and the graph-

coloring problem are examples of combinatorial problems. These are problems

that ask, explicitly or implicitly, to find a combinatorial object--such as a permu-

tation, a combination, or a subset--that satisfies certain constraints. A desired

combinatorial object may also be required to have some additional property such

as a maximum value or a minimum cost.
22  Introduction

    Generally speaking, combinatorial problems are the most difficult problems in

    computing, from both a theoretical and practical standpoint. Their difficulty stems

    from the following facts. First, the number of combinatorial objects typically grows

    extremely fast with a problem's size, reaching unimaginable magnitudes even

    for moderate-sized instances. Second, there are no known algorithms for solving

    most such problems exactly in an acceptable amount of time. Moreover, most

    computer scientists believe that such algorithms do not exist. This conjecture has

    been neither proved nor disproved, and it remains the most important unresolved

    issue in theoretical computer science. We discuss this topic in more detail in

    Section 11.3.

    Some combinatorial problems can be solved by efficient algorithms, but they

    should be considered fortunate exceptions to the rule. The shortest-path problem

    mentioned earlier is among such exceptions.

    Geometric Problems

    Geometric algorithms deal with geometric objects such as points, lines, and poly-

    gons. The ancient Greeks were very much interested in developing procedures

    (they did not call them algorithms, of course) for solving a variety of geometric

    problems, including problems of constructing simple geometric shapes--triangles,

    circles, and so on--with an unmarked ruler and a compass. Then, for about 2000

    years, intense interest in geometric algorithms disappeared, to be resurrected in

    the age of computers--no more rulers and compasses, just bits, bytes, and good old

    human ingenuity. Of course, today people are interested in geometric algorithms

    with quite different applications in mind, such as computer graphics, robotics, and

    tomography.

    We will discuss algorithms for only two classic problems of computational

    geometry: the closest-pair problem and the convex-hull problem. The closest-pair

    problem is self-explanatory: given n points in the plane, find the closest pair among

    them. The convex-hull problem asks to find the smallest convex polygon that

    would include all the points of a given set. If you are interested in other geometric

    algorithms, you will find a wealth of material in such specialized monographs as

    [deB10], [ORo98], and [Pre85].

    Numerical Problems

    Numerical problems, another large special area of applications, are problems

    that involve mathematical objects of continuous nature: solving equations and

    systems of equations, computing definite integrals, evaluating functions, and so on.

    The majority of such mathematical problems can be solved only approximately.

    Another principal difficulty stems from the fact that such problems typically

    require manipulating real numbers, which can be represented in a computer only

    approximately. Moreover, a large number of arithmetic operations performed on

    approximately represented numbers can lead to an accumulation of the round-off
                                   1.3  Important Problem Types                       23

error to a point where it can drastically distort an output produced by a seemingly

sound algorithm.

    Many sophisticated algorithms have been developed over the years in this

area, and they continue to play a critical role in many scientific and engineering

applications. But in the last 30 years or so, the computing industry has shifted

its focus to business applications. These new applications require primarily algo-

rithms for information storage, retrieval, transportation through networks, and

presentation to users. As a result of this revolutionary change, numerical analysis

has lost its formerly dominating position in both industry and computer science

programs. Still, it is important for any computer-literate person to have at least a

rudimentary idea about numerical algorithms. We discuss several classical numer-

ical algorithms in Sections 6.2, 11.4, and 12.4.

Exercises 1.3

1.  Consider the algorithm for the sorting problem that sorts an array by counting,

    for each of its elements, the number of smaller elements and then uses this

    information to put the element in its appropriate position in the sorted array:

    ALGORITHM            ComparisonCountingSort(A[0..n - 1])

        //Sorts an array by comparison counting

        //Input: Array A[0..n - 1] of orderable values

        //Output: Array S[0..n - 1] of A's elements sorted

        //  in nondecreasing order

        for i  0 to n - 1 do

            Count[i]  0

        for i  0 to n - 2 do

            for j  i + 1 to n - 1 do

                  if A[i] < A[j ]

                         Count[j ]  Count[j ] + 1

                  else Count[i]  Count[i] + 1

        for i  0 to n - 1 do

            S[Count[i]]  A[i]

        return S

    a.  Apply this algorithm to sorting the list 60, 35, 81, 98, 14, 47.

    b. Is this algorithm stable?

    c.  Is it in-place?

2.  Name the algorithms for the searching problem that you already know. Give

    a good succinct description of each algorithm in English. If you know no such

    algorithms, use this opportunity to design one.

3.  Design a simple algorithm for the string-matching problem.
24  Introduction

    4.  Ko¨ nigsberg bridges  The Ko¨ nigsberg bridge puzzle is universally accepted

        as the problem that gave birth to graph theory. It was solved by the great

        Swiss-born mathematician Leonhard Euler (1707­1783). The problem asked

        whether one could, in a single stroll, cross all seven bridges of the city of

        Ko¨ nigsberg exactly once and return to a starting point. Following is a sketch

        of the river with its two islands and seven bridges:

        a.  State the problem as a graph problem.

        b.  Does this problem have a solution? If you believe it does, draw such a stroll;

            if you believe it does not, explain why and indicate the smallest number of

            new bridges that would be required to make such a stroll possible.

    5.  Icosian Game  A century after Euler's discovery (see Problem 4), another

        famous puzzle--this one invented by the renowned Irish mathematician Sir

        William Hamilton (1805­1865)--was presented to the world under the name

        of the Icosian Game. The game's board was a circular wooden board on which

        the following graph was carved:

        Find a Hamiltonian circuit--a path that visits all the graph's vertices exactly

        once before returning to the starting vertex--for this graph.

    6.  Consider the following problem: Design an algorithm to determine the best

        route for a subway passenger to take from one designated station to another in

        a well-developed subway system similar to those in such cities as Washington,

        D.C., and London, UK.
                                 1.4   Fundamental Data Structures                          25

          a.  The problem's statement is somewhat vague, which is typical of real-life

              problems. In particular, what reasonable criterion can be used for defining

              the "best" route?

          b. How would you model this problem by a graph?

     7.   a.  Rephrase the traveling-salesman problem in combinatorial object terms.

          b. Rephrase the graph-coloring problem in combinatorial object terms.

     8.   Consider the following map:

                                             b

                                       a

                                                   d

                                          c

                                       e

                                                f

          a.  Explain how we can use the graph-coloring problem to color the map so

              that no two neighboring regions are colored the same.

          b. Use your answer to part (a) to color the map with the smallest number of

              colors.

     9.   Design an algorithm for the following problem: Given a set of n points in the

          Cartesian plane, determine whether all of them lie on the same circumference.

     10.  Write a program that reads as its inputs the (x, y) coordinates of the endpoints

          of two line segments P1Q1 and P2Q2 and determines whether the segments

          have a common point.

1.4  Fundamental Data Structures

     Since the vast majority of algorithms of interest operate on data, particular ways of

     organizing data play a critical role in the design and analysis of algorithms. A data

     structure can be defined as a particular scheme of organizing related data items.

     The nature of the data items is dictated by the problem at hand; they can range

     from elementary data types (e.g., integers or characters) to data structures (e.g., a

     one-dimensional array of one-dimensional arrays is often used for implementing

     matrices). There are a few data structures that have proved to be particularly

     important for computer algorithms. Since you are undoubtedly familiar with most

     if not all of them, just a quick review is provided here.

     Linear Data Structures

     The two most important elementary data structures are the array and the linked

     list. A (one-dimensional) array is a sequence of n items of the same data type that
26  Introduction

    are stored contiguously in computer memory and made accessible by specifying a

    value of the array's index (Figure 1.3).

    In the majority of cases, the index is an integer either between 0 and n - 1

    (as shown in Figure 1.3) or between 1 and n. Some computer languages allow an

    array index to range between any two integer bounds low and high, and some even

    permit nonnumerical indices to specify, for example, data items corresponding to

    the 12 months of the year by the month names.

    Each and every element of an array can be accessed in the same constant

    amount of time regardless of where in the array the element in question is located.

    This feature positively distinguishes arrays from linked lists, discussed below.

    Arrays are used for implementing a variety of other data structures. Promi-

    nent among them is the string, a sequence of characters from an alphabet termi-

    nated by a special character indicating the string's end. Strings composed of zeros

    and ones are called binary strings or bit strings. Strings are indispensable for pro-

    cessing textual data, defining computer languages and compiling programs written

    in them, and studying abstract computational models. Operations we usually per-

    form on strings differ from those we typically perform on other arrays (say, arrays

    of numbers). They include computing the string length, comparing two strings to

    determine which one precedes the other in lexicographic (i.e., alphabetical) or-

    der, and concatenating two strings (forming one string from two given strings by

    appending the second to the end of the first).

    A linked list is a sequence of zero or more elements called nodes, each

    containing two kinds of information: some data and one or more links called

    pointers to other nodes of the linked list. (A special pointer called "null" is used

    to indicate the absence of a node's successor.) In a singly linked list, each node

    except the last one contains a single pointer to the next element (Figure 1.4).

    To access a particular node of a linked list, one starts with the list's first node

    and traverses the pointer chain until the particular node is reached. Thus, the time

    needed to access an element of a singly linked list, unlike that of an array, depends

    on where in the list the element is located. On the positive side, linked lists do

                  Item [0]       Item [1]                Item [n­1]

    FIGURE  1.3   Array of n  elements.

                  Item 0                      Item 1     Item n ­1 null

    FIGURE  1.4   Singly linked  list  of  n  elements.
                           1.4      Fundamental Data Structures                             27

null       Item 0          Item 1                                Item n­1 null

FIGURE 1.5 Doubly linked list of n elements.

not require any preliminary reservation of the computer memory, and insertions

and deletions can be made quite efficiently in a linked list by reconnecting a few

appropriate pointers.

We can exploit flexibility of the linked list structure in a variety of ways. For

example, it is often convenient to start a linked list with a special node called the

header. This node may contain information about the linked list itself, such as its

current length; it may also contain, in addition to a pointer to the first element, a

pointer to the linked list's last element.

Another extension is the structure called the doubly linked list, in which every

node, except the first and the last, contains pointers to both its successor and its

predecessor (Figure 1.5).

The array and linked list are two principal choices in representing a more

abstract data structure called a linear list or simply a list. A list is a finite sequence

of data items, i.e., a collection of data items arranged in a certain linear order. The

basic operations performed on this data structure are searching for, inserting, and

deleting an element.

Two special types of lists, stacks and queues, are particularly important. A

stack is a list in which insertions and deletions can be done only at the end. This

end is called the top because a stack is usually visualized not horizontally but

vertically--akin to a stack of plates whose "operations" it mimics very closely.

As a result, when elements are added to (pushed onto) a stack and deleted from

(popped off) it, the structure operates in a "last-in­first-out" (LIFO) fashion--

exactly like a stack of plates if we can add or remove a plate only from the top.

Stacks have a multitude of applications; in particular, they are indispensable for

implementing recursive algorithms.

A queue, on the other hand, is a list from which elements are deleted from

one end of the structure, called the front (this operation is called dequeue),

and new elements are added to the other end, called the rear (this operation is

called enqueue). Consequently, a queue operates in a "first-in­first-out" (FIFO)

fashion--akin to a queue of customers served by a single teller in a bank. Queues

also have many important applications, including several algorithms for graph

problems.

Many important applications require selection of an item of the highest pri-

ority among a dynamically changing set of candidates. A data structure that seeks

to satisfy the needs of such applications is called a priority queue. A priority

queue is a collection of data items from a totally ordered universe (most often,
28  Introduction

    integer or real numbers). The principal operations on a priority queue are find-

    ing its largest element, deleting its largest element, and adding a new element.

    Of course, a priority queue must be implemented so that the last two operations

    yield another priority queue. Straightforward implementations of this data struc-

    ture can be based on either an array or a sorted array, but neither of these options

    yields the most efficient solution possible. A better implementation of a priority

    queue is based on an ingenious data structure called the heap. We discuss heaps

    and an important sorting algorithm based on them in Section 6.4.

    Graphs

    As we mentioned in the previous section, a graph is informally thought of as

    a collection of points in the plane called "vertices" or "nodes," some of them

    connected by line segments called "edges" or "arcs." Formally, a graph G =                V, E

    is defined by a pair of two sets: a finite nonempty set V of items called vertices

    and a set E of pairs of these items called edges. If these pairs of vertices are

    unordered, i.e., a pair of vertices (u, v) is the same as the pair (v, u), we say that

    the vertices u and v are adjacent to each other and that they are connected by the

    undirected edge (u, v). We call the vertices u and v endpoints of the edge (u, v)

    and say that u and v are incident to this edge; we also say that the edge (u, v) is

    incident to its endpoints u and v. A graph G is called undirected if every edge in

    it is undirected.

    If a pair of vertices (u, v) is not the same as the pair (v, u), we say that the

    edge (u, v) is directed from the vertex u, called the edge's tail, to the vertex v,

    called the edge's head. We also say that the edge (u, v) leaves u and enters v. A

    graph whose every edge is directed is called directed. Directed graphs are also

    called digraphs.

    It is normally convenient to label vertices of a graph or a digraph with letters,

    integer numbers, or, if an application calls for it, character strings (Figure 1.6). The

    graph depicted in Figure 1.6a has six vertices and seven undirected edges:

    V = {a, b, c, d, e, f }, E = {(a, c), (a, d), (b, c), (b, f ), (c, e), (d, e), (e, f )}.

    The digraph depicted in Figure 1.6b has six vertices and eight directed edges:

    V = {a, b, c, d, e, f }, E = {(a, c), (b, c), (b, f ), (c, e), (d, a), (d, e), (e, c), (e, f )}.

                  a    c    b                           a  c          b

                  d    e    f                           d  e          f

                       (a)                                 (b)

    FIGURE 1.6    (a)  Undirected graph. (b)  Digraph.
                              1.4      Fundamental Data Structures                          29

Our definition of a graph does not forbid loops, or edges connecting vertices

to themselves. Unless explicitly stated otherwise, we will consider graphs without

loops. Since our definition disallows multiple edges between the same vertices of

an undirected graph, we have the following inequality for the number of edges |E|

possible in an undirected graph with |V | vertices and no loops:

                              0  |E|  |V |(|V | - 1)/2.

(We get the largest number of edges in a graph if there is an edge connecting

each of its |V | vertices with all |V | - 1 other vertices. We have to divide product

|V |(|V | - 1) by 2, however, because it includes every edge twice.)

A graph with every pair of its vertices connected by an edge is called complete.

A standard notation for the complete graph with |V | vertices is K|V |. A graph

with relatively few possible edges missing is called dense; a graph with few edges

relative to the number of its vertices is called sparse. Whether we are dealing with

a dense or sparse graph may influence how we choose to represent the graph and,

consequently, the running time of an algorithm being designed or used.

Graph Representations Graphs for computer algorithms are usually repre-

sented in one of two ways: the adjacency matrix and adjacency lists. The adjacency

matrix of a graph with n vertices is an n × n boolean matrix with one row and one

column for each of the graph's vertices, in which the element in the ith row and

the j th column is equal to 1 if there is an edge from the ith vertex to the j th vertex,

and equal to 0 if there is no such edge. For example, the adjacency matrix for the

graph of Figure 1.6a is given in Figure 1.7a.

Note that the adjacency matrix of an undirected graph is always symmetric,

i.e., A[i, j ] = A[j, i] for every 0  i, j  n - 1 (why?).

The adjacency lists of a graph or a digraph is a collection of linked lists,

one for each vertex, that contain all the vertices adjacent to the list's vertex

(i.e., all the vertices connected to it by an edge). Usually, such lists start with a

header identifying a vertex for which the list is compiled. For example, Figure 1.7b

represents the graph in Figure 1.6a via its adjacency lists. To put it another way,

             a  b  c    d  e  f

        a    0  0  1    1  0  0                a             c       d

        b    0  0  1    0  0  1                b             c       f

        c    1  1  0    0  1  0                c             a       b    e

        d    1  0  0    0  1  0                d             a       e

        e    0  0  1    1  0  1                e             c       d    f

        f    0  1  0    0  1  0                f             b       e

                   (a)                                          (b)

FIGURE  1.7  (a) Adjacency matrix and  (b)  adjacency lists  of the graph in Figure  1.6a.
30  Introduction

    adjacency lists indicate columns of the adjacency matrix that, for a given vertex,

    contain 1's.

    If a graph is sparse, the adjacency list representation may use less space

    than the corresponding adjacency matrix despite the extra storage consumed by

    pointers of the linked lists; the situation is exactly opposite for dense graphs. In

    general, which of the two representations is more convenient depends on the

    nature of the problem, on the algorithm used for solving it, and, possibly, on the

    type of input graph (sparse or dense).

    Weighted      Graphs  A  weighted  graph      (or  weighted  digraph)        is  a  graph  (or    di-

    graph) with numbers assigned to its edges. These numbers are called weights or

    costs. An interest in such graphs is motivated by numerous real-world applica-

    tions, such as finding the shortest path between two points in a transportation or

    communication network or the traveling salesman problem mentioned earlier.

    Both principal representations of a graph can be easily adopted to accommo-

    date weighted graphs. If a weighted graph is represented by its adjacency matrix,

    then its element A[i, j ] will simply contain the weight of the edge from the ith to

    the j th vertex if there is such an edge and a special symbol, e.g., , if there is no

    such edge. Such a matrix is called the weight matrix or cost matrix. This approach

    is illustrated in Figure 1.8b for the weighted graph in Figure 1.8a. (For some ap-

    plications, it is more convenient to put 0's on the main diagonal of the adjacency

    matrix.) Adjacency lists for a weighted graph have to include in their nodes not

    only the name of an adjacent vertex but also the weight of the corresponding edge

    (Figure 1.8c).

    Paths and Cycles Among the many properties of graphs, two are important for a

    great number of applications: connectivity and acyclicity. Both are based on the

    notion of a path. A path from vertex u to vertex v of a graph G can be defined as a

    sequence of adjacent (connected by an edge) vertices that starts with u and ends

    with v. If all vertices of a path are distinct, the path is said to be simple. The length

    of a path is the total number of vertices in the vertex sequence defining the path

    minus 1, which is the same as the number of edges in the path. For example, a, c,

    b, f is a simple path of length 3 from a to f in the graph in Figure 1.6a, whereas

    a, c, e, c, b, f is a path (not simple) of length 5 from a to f.

              5                 a      b       c  d

       a            b        a         5       1                 a         b, 5      c, 1

    1         7        4     b  5              7  4              b         a, 5      c, 7      d,  4

                             c  1      7          2              c         a, 1      b, 7      d,  2

       c      2     d        d         4       2                 d         b, 4      c, 2

            (a)                           (b)                              (c)

    FIGURE  1.8 (a) Weighted graph.  (b) Its weight    matrix.  (c) Its  adjacency lists.
                                        1.4   Fundamental Data Structures                          31

                                        a                              f

                          b             c           e         g              h

                                        d                                 i

    FIGURE 1.9     Graph  that is  not  connected.

    In the case of a directed graph, we are usually interested in directed paths.

    A directed path is a sequence of vertices in which every consecutive pair of the

    vertices is connected by an edge directed from the vertex listed first to the vertex

    listed next. For example, a, c, e, f is a directed path from a to f in the graph in

    Figure 1.6b.

    A graph is said to be connected if for every pair of its vertices u and v there

    is a path from u to v. If we make a model of a connected graph by connecting

    some balls representing the graph's vertices with strings representing the edges,

    it will be a single piece. If a graph is not connected, such a model will consist

    of several connected pieces that are called connected components of the graph.

    Formally, a connected component is a maximal (not expandable by including

    another vertex and an edge) connected subgraph2 of a given graph. For example,

    the graphs in Figures 1.6a and 1.8a are connected, whereas the graph in Figure 1.9

    is not, because there is no path, for example, from a to f. The graph in Figure

    1.9 has two connected components with vertices {a, b, c, d, e} and {f, g, h, i},

    respectively.

    Graphs with several connected components do happen in real-world appli-

    cations. A graph representing the Interstate highway system of the United States

    would be an example (why?).

    It is important to know for many applications whether or not a graph under

    consideration has cycles. A cycle is a path of a positive length that starts and ends at

    the same vertex and does not traverse the same edge more than once. For example,

    f , h, i, g, f is a cycle in the graph in Figure 1.9. A graph with no cycles is said to

    be acyclic. We discuss acyclic graphs in the next subsection.

    Trees

    A tree (more accurately, a free tree) is a connected acyclic graph (Figure 1.10a).

    A graph that has no cycles but is not necessarily connected is called a forest: each

    of its connected components is a tree (Figure 1.10b).

2.  A subgraph of a given graph G =     V, E  is a graph G =     V ,E  such that V   V and E   E.
32  Introduction

                  a           b                 a                  b                    h

                  c           d                 c                  d          e         i

                  f           g                 f                  g                    j

                       (a)                                            (b)

    FIGURE 1.10   (a)  Tree.  (b) Forest.

               i                   d

                                                                                     a

               c                   a         e                             b         d     e

                       b

                                                                      c       g            f

               h       g                     f                h            i

                              (a)                                             (b)

    FIGURE 1.11   (a)  Free   tree. (b) Its  transformation  into  a  rooted  tree.

    Trees have several important properties other graphs do not have. In par-

    ticular, the number of edges in a tree is always one less than the number of its

    vertices:

                                             |E| = |V | - 1.

    As the graph in Figure 1.9 demonstrates, this property is necessary but not suffi-

    cient for a graph to be a tree. However, for connected graphs it is sufficient and

    hence provides a convenient way of checking whether a connected graph has a

    cycle.

    Rooted Trees       Another very important property of trees is the fact that for every

    two vertices in a tree, there always exists exactly one simple path from one of these

    vertices to the other. This property makes it possible to select an arbitrary vertex

    in a free tree and consider it as the root of the so-called rooted tree. A rooted tree

    is usually depicted by placing its root on the top (level 0 of the tree), the vertices

    adjacent to the root below it (level 1), the vertices two edges apart from the root

    still below (level 2), and so on. Figure 1.11 presents such a transformation from a

    free tree to a rooted tree.
                           1.4       Fundamental Data Structures                            33

Rooted trees play a very important role in computer science, a much more

important one than free trees do; in fact, for the sake of brevity, they are often

referred to as simply "trees." An obvious application of trees is for describing

hierarchies, from file directories to organizational charts of enterprises. There are

many less obvious applications, such as implementing dictionaries (see below),

efficient access to very large data sets (Section 7.4), and data encoding (Section

9.4). As we discuss in Chapter 2, trees also are helpful in analysis of recursive

algorithms. To finish this far-from-complete list of tree applications, we should

mention the so-called state-space trees that underline two important algorithm

design techniques: backtracking and branch-and-bound (Sections 12.1 and 12.2).

For any vertex v in a tree T , all the vertices on the simple path from the root

to that vertex are called ancestors of v. The vertex itself is usually considered its

own ancestor; the set of ancestors that excludes the vertex itself is referred to as

the set of proper ancestors. If (u, v) is the last edge of the simple path from the

root to vertex v (and u = v), u is said to be the parent of v and v is called a child

of u; vertices that have the same parent are said to be siblings. A vertex with no

children is called a leaf ; a vertex with at least one child is called parental. All the

vertices for which a vertex v is an ancestor are said to be descendants of v; the

proper descendants exclude the vertex v itself. All the descendants of a vertex v

with all the edges connecting them form the subtree of T rooted at that vertex.

Thus, for the tree in Figure 1.11b, the root of the tree is a; vertices d, g, f, h, and i

are leaves, and vertices a, b, e, and c are parental; the parent of b is a; the children

of b are c and g; the siblings of b are d and e; and the vertices of the subtree rooted

at b are {b, c, g, h, i}.

The depth of a vertex v is the length of the simple path from the root to v. The

height of a tree is the length of the longest simple path from the root to a leaf. For

example, the depth of vertex c of the tree in Figure 1.11b is 2, and the height of

the tree is 3. Thus, if we count tree levels top down starting with 0 for the root's

level, the depth of a vertex is simply its level in the tree, and the tree's height is the

maximum level of its vertices. (You should be alert to the fact that some authors

define the height of a tree as the number of levels in it; this makes the height of

a tree larger by 1 than the height defined as the length of the longest simple path

from the root to a leaf.)

Ordered Trees  An ordered tree is a rooted tree in which all the children of each

vertex are ordered. It is convenient to assume that in a tree's diagram, all the

children are ordered left to right.

A binary tree can be defined as an ordered tree in which every vertex has

no more than two children and each child is designated as either a left child or a

right child of its parent; a binary tree may also be empty. An example of a binary

tree is given in Figure 1.12a. The binary tree with its root at the left (right) child

of a vertex in a binary tree is called the left (right) subtree of that vertex. Since

left and right subtrees are binary trees as well, a binary tree can also be defined

recursively. This makes it possible to solve many problems involving binary trees

by recursive algorithms.
34  Introduction

                                                                           9

                                                                  5                  12

                                              1                         7        10

                                                               4

                              (a)                                          (b)

    FIGURE 1.12   (a) Binary tree. (b) Binary search tree.

                                                            9

                           5                                                                 12      null

    null    1                       null  7   null                null     10        null

                  null  4     null

    FIGURE  1.13 Standard  implementation of  the binary       search      tree  in  Figure  1.12b.

          In Figure 1.12b, some numbers are assigned to vertices of the binary tree in

    Figure 1.12a. Note that a number assigned to each parental vertex is larger than all

    the numbers in its left subtree and smaller than all the numbers in its right subtree.

    Such trees are called binary search trees. Binary trees and binary search trees have

    a wide variety of applications in computer science; you will encounter some of

    them throughout the book. In particular, binary search trees can be generalized

    to more general types of search trees called multiway search trees, which are

    indispensable for efficient access to very large data sets.

          As you will see later in the book, the efficiency of most important algorithms

    for binary search trees and their extensions depends on the tree's height. There-

    fore, the following inequalities for the height h of a binary tree with n nodes are

    especially important for analysis of such algorithms:

                                    log2 n     h  n - 1.
                                                 1.4  Fundamental Data Structures                               35

               A binary tree is usually implemented for computing purposes by a collection

               of nodes corresponding to vertices of the tree. Each node contains some informa-

               tion associated with the vertex (its name or some value assigned to it) and two

               pointers to the nodes representing the left child and right child of the vertex, re-

               spectively. Figure 1.13 illustrates such an implementation for the binary search

               tree in Figure 1.12b.

               A computer representation of an arbitrary ordered tree can be done by simply

               providing a parental vertex with the number of pointers equal to the number of

               its children. This representation may prove to be inconvenient if the number of

               children varies widely among the nodes. We can avoid this inconvenience by using

               nodes with just two pointers, as we did for binary trees. Here, however, the left

               pointer will point to the first child of the vertex, and the right pointer will point

               to its next sibling. Accordingly, this representation is called the first child­next

               sibling representation. Thus, all the siblings of a vertex are linked via the nodes'

               right pointers in a singly linked list, with the first element of the list pointed to

               by the left pointer of their parent. Figure 1.14a illustrates this representation for

               the tree in Figure 1.11b. It is not difficult to see that this representation effectively

               transforms an ordered tree into a binary tree said to be associated with the ordered

               tree. We get this representation by "rotating" the pointers about 45 degrees

               clockwise (see Figure 1.14b).

               Sets and Dictionaries

               The notion of a set plays a central role in mathematics. A set can be described as

               an unordered collection (possibly empty) of distinct items called elements of the

      a  null                                                                              a

      b        null  d                        e       null                           b

      c        null  g      null      null    f       null                   c             d

null  h        null  i      null                            h                     g                e

                                                                         i                    f

                     (a)                                                          (b)

               FIGURE 1.14  (a) First child­next sibling representation  of  the  tree in  Figure  1.11b.  (b)  Its

                            binary tree representation.
36  Introduction

    set. A specific set is defined either by an explicit listing of its elements (e.g., S = {2,

    3, 5, 7}) or by specifying a property that all the set's elements and only they must

    satisfy (e.g., S = {n: n is a prime number smaller than 10}). The most important set

    operations are: checking membership of a given item in a given set; finding the

    union of two sets, which comprises all the elements in either or both of them; and

    finding the intersection of two sets, which comprises all the common elements in

    the sets.

    Sets can be implemented in computer applications in two ways. The first

    considers only sets that are subsets of some large set U, called the universal

    set. If set U has n elements, then any subset S of U can be represented by a bit

    string of size n, called a bit vector, in which the ith element is 1 if and only if

    the ith element of U is included in set S. Thus, to continue with our example, if

    U = {1, 2, 3, 4, 5, 6, 7, 8, 9}, then S = {2, 3, 5, 7} is represented by the bit string

    011010100. This way of representing sets makes it possible to implement the

    standard set operations very fast, but at the expense of potentially using a large

    amount of storage.

    The second and more common way to represent a set for computing purposes

    is to use the list structure to indicate the set's elements. Of course, this option, too,

    is feasible only for finite sets; fortunately, unlike mathematics, this is the kind of

    sets most computer applications need. Note, however, the two principal points of

    distinction between sets and lists. First, a set cannot contain identical elements;

    a list can. This requirement for uniqueness is sometimes circumvented by the

    introduction of a multiset, or bag, an unordered collection of items that are not

    necessarily distinct. Second, a set is an unordered collection of items; therefore,

    changing the order of its elements does not change the set. A list, defined as an

    ordered collection of items, is exactly the opposite. This is an important theoretical

    distinction, but fortunately it is not important for many applications. It is also

    worth mentioning that if a set is represented by a list, depending on the application

    at hand, it might be worth maintaining the list in a sorted order.

    In computing, the operations we need to perform for a set or a multiset most

    often are searching for a given item, adding a new item, and deleting an item

    from the collection. A data structure that implements these three operations is

    called the dictionary. Note the relationship between this data structure and the

    problem of searching mentioned in Section 1.3; obviously, we are dealing here

    with searching in a dynamic context. Consequently, an efficient implementation

    of a dictionary has to strike a compromise between the efficiency of searching and

    the efficiencies of the other two operations. There are quite a few ways a dictionary

    can be implemented. They range from an unsophisticated use of arrays (sorted or

    not) to much more sophisticated techniques such as hashing and balanced search

    trees, which we discuss later in the book.

    A number of applications in computing require a dynamic partition of some

    n-element set into a collection of disjoint subsets. After being initialized as a

    collection of n one-element subsets, the collection is subjected to a sequence

    of intermixed union and search operations. This problem is called the set union

    problem. We discuss efficient algorithmic solutions to this problem in Section 9.2,

    in conjunction with one of its important applications.
                               1.4    Fundamental Data Structures                     37

    You may have noticed that in our review of basic data structures we almost al-

ways mentioned specific operations that are typically performed for the structure

in question. This intimate relationship between the data and operations has been

recognized by computer scientists for a long time. It has led them in particular

to the idea of an abstract data type (ADT): a set of abstract objects represent-

ing data items with a collection of operations that can be performed on them. As

illustrations of this notion, reread, say, our definitions of the priority queue and

dictionary. Although abstract data types could be implemented in older procedu-

ral languages such as Pascal (see, e.g., [Aho83]), it is much more convenient to

do this in object-oriented languages such as C++ and Java, which support abstract

data types by means of classes.

Exercises 1.4

1.  Describe how one can implement each of the following operations on an array

    so that the time it takes does not depend on the array's size n.

    a.  Delete the ith element of an array (1  i  n).

    b. Delete the ith element of a sorted array (the remaining array has to stay

        sorted, of course).

2.  If you have to solve the searching problem for a list of n numbers, how can you

    take advantage of the fact that the list is known to be sorted? Give separate

    answers for

    a.  lists represented as arrays.

    b. lists represented as linked lists.

3.  a.  Show the stack after each operation of the following sequence that starts

        with the empty stack:

                 push(a), push(b), pop, push(c), push(d), pop

    b.  Show the queue after each operation of the following sequence that starts

        with the empty queue:

        enqueue(a), enqueue(b), dequeue, enqueue(c), enqueue(d), dequeue

4.  a.  Let A be the adjacency matrix of an undirected graph. Explain what prop-

        erty of the matrix indicates that

        i. the graph is complete.

        ii. the graph has a loop, i.e., an edge connecting a vertex to itself.

        iii. the graph has an isolated vertex, i.e., a vertex with no edges incident

        to it.

    b. Answer the same questions for the adjacency list representation.

5.  Give a detailed description of an algorithm for transforming a free tree into

    a tree rooted at a given vertex of the free tree.
38  Introduction

    6.   Prove the inequalities that bracket the height of a binary tree with n vertices:

                                    log2 n   h  n - 1.

    7.   Indicate how the ADT priority queue can be implemented as

         a.  an (unsorted) array.

         b. a sorted array.

         c.  a binary search tree.

    8.   How would you implement a dictionary of a reasonably small size n if you

         knew that all its elements are distinct (e.g., names of the 50 states of the United

         States)? Specify an implementation of each dictionary operation.

    9.   For each of the following applications, indicate the most appropriate data

         structure:

         a.  answering telephone calls in the order of their known priorities

         b. sending backlog orders to customers in the order they have been received

         c.  implementing a calculator for computing simple arithmetical expressions

    10.  Anagram checking    Design an algorithm for checking whether two given

         words are anagrams, i.e., whether one word can be obtained by permuting

         the letters of the other. For example, the words tea and eat are anagrams.

    SUMMARY

         An algorithm is a sequence of nonambiguous instructions for solving a

         problem in a finite amount of time. An input to an algorithm specifies an

         instance of the problem the algorithm solves.

         Algorithms can be specified in a natural language or pseudocode; they can

         also be implemented as computer programs.

         Among several ways to classify algorithms, the two principal alternatives are:

         .   to group algorithms according to types of problems they solve

         .   to group algorithms according to underlying design techniques they are

             based upon

         The important problem types are sorting, searching, string processing, graph

         problems, combinatorial problems, geometric problems, and numerical

         problems.

         Algorithm design techniques (or "strategies" or "paradigms") are general

         approaches to solving problems algorithmically, applicable to a variety of

         problems from different areas of computing.
                               Summary                                           39

Although designing an algorithm is undoubtedly a creative activity, one can

identify a sequence of interrelated actions involved in such a process. They

are summarized in Figure 1.2.

A good algorithm is usually the result of repeated efforts and rework.

The same problem can often be solved by several algorithms. For example,

three algorithms were given for computing the greatest common divisor of

two integers: Euclid's algorithm, the consecutive integer checking algorithm,

and the middle-school method enhanced by the sieve of Eratosthenes for

generating a list of primes.

Algorithms operate on data. This makes the issue of data structuring critical

for efficient algorithmic problem solving. The most important elementary data

structures are the array and the linked list. They are used for representing

more abstract data structures such as the list, the stack, the queue, the graph

(via its adjacency matrix or adjacency lists), the binary tree, and the set.

An abstract collection of objects with several operations that can be per-

formed on them is called an abstract data type (ADT). The list, the stack, the

queue, the priority queue, and the dictionary are important examples of ab-

stract data types. Modern object-oriented languages support implementation

of ADTs by means of classes.
This page intentionally left blank
2

Fundamentals of the Analysis

of Algorithm Efficiency

   I often say that when you can measure what you are speaking about and

   express it in numbers you know something about it; but when you cannot

   express it in numbers your knowledge is a meagre and unsatisfactory

   kind: it may be the beginning of knowledge but you have scarcely, in your

   thoughts, advanced to the stage of science, whatever the matter may be.

                               --Lord Kelvin (1824­1907)

   Not everything that can be counted counts, and not everything that counts

   can be counted.

                               --Albert Einstein (1879­1955)

   This chapter is devoted to analysis of algorithms. The American Heritage Dic-
   tionary defines "analysis" as "the separation of an intellectual or substantial

   whole into its constituent parts for individual study." Accordingly, each of the prin-

   cipal dimensions of an algorithm pointed out in Section 1.2 is both a legitimate and

   desirable subject of study. But the term "analysis of algorithms" is usually used in

   a narrower, technical sense to mean an investigation of an algorithm's efficiency

   with respect to two resources: running time and memory space. This emphasis on

   efficiency is easy to explain. First, unlike such dimensions as simplicity and gen-

   erality, efficiency can be studied in precise quantitative terms. Second, one can

   argue--although this is hardly always the case, given the speed and memory of

   today's computers--that the efficiency considerations are of primary importance

   from a practical point of view. In this chapter, we too will limit the discussion to

   an algorithm's efficiency.

                                                                                           41
42       Fundamentals of the Analysis of Algorithm Efficiency

              We start with a general framework for analyzing algorithm efficiency in Sec-

         tion 2.1. This section is arguably the most important in the chapter; the funda-

         mental nature of the topic makes it also one of the most important sections in the

         entire book.

              In Section 2.2, we introduce three notations: O ("big oh"),  ("big omega"),

         and  ("big theta"). Borrowed from mathematics, these notations have become

         the language for discussing the efficiency of algorithms.

              In Section 2.3, we show how the general framework outlined in Section 2.1 can

         be systematically applied to analyzing the efficiency of nonrecursive algorithms.

         The main tool of such an analysis is setting up a sum representing the algorithm's

         running time and then simplifying the sum by using standard sum manipulation

         techniques.

              In Section 2.4, we show how the general framework outlined in Section 2.1

         can be systematically applied to analyzing the efficiency of recursive algorithms.

         Here, the main tool is not a summation but a special kind of equation called a

         recurrence relation. We explain how such recurrence relations can be set up and

         then introduce a method for solving them.

              Although we illustrate the analysis framework and the methods of its appli-

         cations by a variety of examples in the first four sections of this chapter, Section

         2.5 is devoted to yet another example--that of the Fibonacci numbers. Discov-

         ered 800 years ago, this remarkable sequence appears in a variety of applications

         both within and outside computer science. A discussion of the Fibonacci sequence

         serves as a natural vehicle for introducing an important class of recurrence rela-

         tions not solvable by the method of Section 2.4. We also discuss several algorithms

         for computing the Fibonacci numbers, mostly for the sake of a few general obser-

         vations about the efficiency of algorithms and methods of analyzing them.

              The methods of Sections 2.3 and 2.4 provide a powerful technique for analyz-

         ing the efficiency of many algorithms with mathematical clarity and precision, but

         these methods are far from being foolproof. The last two sections of the chapter

         deal with two approaches--empirical analysis and algorithm visualization--that

         complement the pure mathematical techniques of Sections 2.3 and 2.4. Much

         newer and, hence, less developed than their mathematical counterparts, these ap-

         proaches promise to play an important role among the tools available for analysis

         of algorithm efficiency.

    2.1  The Analysis Framework

         In this section, we outline a general framework for analyzing the efficiency of algo-

         rithms. We already mentioned in Section 1.2 that there are two kinds of efficiency:

         time efficiency and space efficiency. Time efficiency, also called time complexity,

         indicates how fast an algorithm in question runs. Space efficiency, also called space

         complexity, refers to the amount of memory units required by the algorithm in ad-

         dition to the space needed for its input and output. In the early days of electronic

         computing, both resources--time and space--were at a premium. Half a century
                               2.1           The Analysis Framework                                         43

    of relentless technological innovations have improved the computer's speed and

    memory size by many orders of magnitude. Now the amount of extra space re-

    quired by an algorithm is typically not of as much concern, with the caveat that

    there is still, of course, a difference between the fast main memory, the slower

    secondary memory, and the cache. The time issue has not diminished quite to the

    same extent, however. In addition, the research experience has shown that for

    most problems, we can achieve much more spectacular progress in speed than in

    space. Therefore, following a well-established tradition of algorithm textbooks, we

    primarily concentrate on time efficiency, but the analytical framework introduced

    here is applicable to analyzing space efficiency as well.

    Measuring an Input's Size

    Let's start with the obvious observation that almost all algorithms run longer on

    larger inputs. For example, it takes longer to sort larger arrays, multiply larger

    matrices, and so on. Therefore, it is logical to investigate an algorithm's efficiency

    as a function of some parameter n indicating the algorithm's input size.1 In most

    cases, selecting such a parameter is quite straightforward. For example, it will be

    the size of the list for problems of sorting, searching, finding the list's smallest

    element, and most other problems dealing with lists. For the problem of evaluating

    a polynomial p(x) = anxn + . . . + a0 of degree n, it will be the polynomial's degree

    or the number of its coefficients, which is larger by 1 than its degree. You'll see from

    the discussion that such a minor difference is inconsequential for the efficiency

    analysis.

    There are situations, of course, where the choice of a parameter indicating

    an input size does matter. One such example is computing the product of two

    n × n matrices. There are two natural measures of size for this problem. The first

    and more frequently used is the matrix order n. But the other natural contender

    is the total number of elements N in the matrices being multiplied. (The latter

    is also more general since it is applicable to matrices that are not necessarily

    square.) Since there is a simple formula relating these two measures, we can easily

    switch from one to the other, but the answer about an algorithm's efficiency will

    be qualitatively different depending on which of these two measures we use (see

    Problem 2 in this section's exercises).

    The choice of an appropriate size metric can be influenced by operations of

    the algorithm in question. For example, how should we measure an input's size

    for a spell-checking algorithm? If the algorithm examines individual characters of

    its input, we should measure the size by the number of characters; if it works by

    processing words, we should count their number in the input.

    We should make a special note about measuring input size for algorithms

    solving problems such as checking primality of a positive integer n. Here, the input

    is just one number, and it is this number's magnitude that determines the input

1.  Some algorithms require more than one parameter to indicate the size of their inputs (e.g., the number

    of vertices and the number of edges for algorithms on graphs represented by their adjacency lists).
44      Fundamentals of the Analysis of Algorithm Efficiency

        size. In such situations, it is preferable to measure size by the number b of bits in

        the n's binary representation:

                                        b=      log2 n  + 1.                      (2.1)

        This metric usually gives a better idea about the efficiency of algorithms in ques-

        tion.

        Units for Measuring Running Time

        The next issue concerns units for measuring an algorithm's running time. Of

        course, we can simply use some standard unit of time measurement--a second,

        or millisecond, and so on--to measure the running time of a program implement-

        ing the algorithm. There are obvious drawbacks to such an approach, however:

        dependence on the speed of a particular computer, dependence on the quality of

        a program implementing the algorithm and of the compiler used in generating the

        machine code, and the difficulty of clocking the actual running time of the pro-

        gram. Since we are after a measure of an algorithm's efficiency, we would like to

        have a metric that does not depend on these extraneous factors.

        One possible approach is to count the number of times each of the algorithm's

        operations is executed. This approach is both excessively difficult and, as we

        shall see, usually unnecessary. The thing to do is to identify the most important

        operation of the algorithm, called the basic operation, the operation contributing

        the most to the total running time, and compute the number of times the basic

        operation is executed.

        As a rule, it is not difficult to identify the basic operation of an algorithm: it

        is usually the most time-consuming operation in the algorithm's innermost loop.

        For example, most sorting algorithms work by comparing elements (keys) of a

        list being sorted with each other; for such algorithms, the basic operation is a key

        comparison. As another example, algorithms for mathematical problems typically

        involve some or all of the four arithmetical operations: addition, subtraction,

        multiplication, and division. Of the four, the most time-consuming operation is

        division, followed by multiplication and then addition and subtraction, with the

        last two usually considered together.2

        Thus, the established framework for the analysis of an algorithm's time ef-

        ficiency suggests measuring it by counting the number of times the algorithm's

        basic operation is executed on inputs of size n. We will find out how to compute

        such a count for nonrecursive and recursive algorithms in Sections 2.3 and 2.4,

        respectively.

        Here is an important application. Let cop be the execution time of an algo-

        rithm's basic operation on a particular computer, and let C(n) be the number of

        times this operation needs to be executed for this algorithm. Then we can estimate

    2.  On some computers, multiplication does not take longer than addition/subtraction (see, for example,

        the timing data provided by Kernighan and Pike in [Ker99, pp. 185­186]).
                                            2.1  The Analysis Framework                                   45

the running time T (n) of a program implementing this algorithm on that computer

by the formula

                                            T (n)  copC(n).

Of course, this formula should be used with caution. The count C(n) does not

contain any information about operations that are not basic, and, in fact, the

count itself is often computed only approximately. Further, the constant cop is

also an approximation whose reliability is not always easy to assess. Still, unless

n is extremely large or very small, the formula can give a reasonable estimate of

the algorithm's running time. It also makes it possible to answer such questions as

"How much faster would this algorithm run on a machine that is 10 times faster

than the one we have?" The answer is, obviously, 10 times. Or, assuming that

C(n)   =  1  n(n  -  1),  how  much     longer   will  the  algorithm  run     if  we  double  its  input

          2
size? The answer is about four times longer. Indeed, for all but very small values

of n,

                            C(n) = 1 n(n - 1) = 1 n2 - 1 n  1 n2
                                         2             2          2    2

and therefore

                                 T (2n)     copC(2n)        1  (2n)2

                                                            2          = 4.

                                 T (n)      copC(n)            1  n2

                                                               2

     Note that we were able to answer the last question without actually knowing

the  value   of   cop:  it  was  neatly     cancelled  out  in    the  ratio.  Also    note  that   1  ,  the

                                                                                                    2
multiplicative constant in the formula for the count C(n), was also cancelled out.

It is for these reasons that the efficiency analysis framework ignores multiplicative

constants and concentrates on the count's order of growth to within a constant

multiple for large-size inputs.

Orders of Growth

Why this emphasis on the count's order of growth for large input sizes? A differ-

ence in running times on small inputs is not what really distinguishes efficient

algorithms from inefficient ones. When we have to compute, for example, the

greatest common divisor of two small numbers, it is not immediately clear how

much more efficient Euclid's algorithm is compared to the other two algorithms

discussed in Section 1.1 or even why we should care which of them is faster and

by how much. It is only when we have to find the greatest common divisor of two

large numbers that the difference in algorithm efficiencies becomes both clear and

important. For large values of n, it is the function's order of growth that counts: just

look at Table 2.1, which contains values of a few functions particularly important

for analysis of algorithms.

     The magnitude of the numbers in Table 2.1 has a profound significance for

the analysis of algorithms. The function growing the slowest among these is the

logarithmic function. It grows so slowly, in fact, that we should expect a program
46  Fundamentals of the Analysis of Algorithm Efficiency

    TABLE  2.1 Values (some approximate)      of several functions important for

           analysis of algorithms

    n      log2 n  n    n log2 n              n2          n3    2n        n!

    10     3.3     101  3.3.101               102         103   103       3.6.106

    102    6.6     102  6.6.102               104         106   1.3.1030  9.3.10157

    103    10      103  1.0.104               106         109

    104    13      104  1.3.105               108         1012

    105    17      105  1.7.106               1010        1015

    106    20      106  2.0.107               1012        1018

    implementing an algorithm with a logarithmic basic-operation count to run practi-

    cally instantaneously on inputs of all realistic sizes. Also note that although specific

    values of such a count depend, of course, on the logarithm's base, the formula

                        loga n = loga b logb n

    makes it possible to switch from one base to another, leaving the count logarithmic

    but with a new multiplicative constant. This is why we omit a logarithm's base and

    write simply log n in situations where we are interested just in a function's order

    of growth to within a multiplicative constant.

    On the other end of the spectrum are the exponential function 2n and the

    factorial function n! Both these functions grow so fast that their values become

    astronomically large even for rather small values of n. (This is the reason why we

    did not include their values for n > 102 in Table 2.1.) For example, it would take

    about 4 . 1010 years for a computer making a trillion (1012) operations per second

    to execute 2100 operations. Though this is incomparably faster than it would have

    taken to execute 100! operations, it is still longer than 4.5 billion (4.5 . 109) years--

    the estimated age of the planet Earth. There is a tremendous difference between

    the orders of growth of the functions 2n and n!, yet both are often referred to as

    "exponential-growth functions" (or simply "exponential") despite the fact that,

    strictly speaking, only the former should be referred to as such. The bottom line,

    which is important to remember, is this:

    Algorithms that require an exponential number of operations are practical

    for solving only problems of very small sizes.

    Another way to appreciate the qualitative difference among the orders of

    growth of the functions in Table 2.1 is to consider how they react to, say, a

    twofold increase in the value of their argument n. The function log2 n increases in

    value by just 1 (because log2 2n = log2 2 + log2 n = 1 + log2 n); the linear function

    increases twofold, the linearithmic function n log2 n increases slightly more than
    twofold; the quadratic function n2 and cubic function n3 increase fourfold and
                             2.1  The Analysis Framework                               47

eightfold, respectively (because (2n)2 = 4n2 and (2n)3 = 8n3); the value of 2n gets

squared (because 22n = (2n)2); and n! increases much more than that (yes, even

mathematics refuses to cooperate to give a neat answer for n!).

Worst-Case, Best-Case, and Average-Case Efficiencies

In the beginning of this section, we established that it is reasonable to measure

an algorithm's efficiency as a function of a parameter indicating the size of the

algorithm's input. But there are many algorithms for which running time depends

not only on an input size but also on the specifics of a particular input. Consider,

as an example, sequential search. This is a straightforward algorithm that searches

for a given item (some search key K) in a list of n elements by checking successive

elements of the list until either a match with the search key is found or the list

is exhausted. Here is the algorithm's pseudocode, in which, for simplicity, a list is

implemented as an array. It also assumes that the second condition A[i] = K will

not be checked if the first one, which checks that the array's index does not exceed

its upper bound, fails.

ALGORITHM           SequentialSearch(A[0..n - 1], K)

//Searches for a given value in a given array by sequential search

//Input: An array A[0..n - 1] and a search key K

//Output: The index of the first element in A that matches K

//         or -1 if there are no matching elements

i0

while i < n and A[i] = K do

    i i +1

if i < n return i

else return -1

Clearly, the running time of this algorithm can be quite different for the

same list size n. In the worst case, when there are no matching elements or

the first matching element happens to be the last one on the list, the algorithm

makes the largest number of key comparisons among all possible inputs of size

n: Cworst (n) = n.

The worst-case efficiency of an algorithm is its efficiency for the worst-case

input of size n, which is an input (or inputs) of size n for which the algorithm

runs the longest among all possible inputs of that size. The way to determine

the worst-case efficiency of an algorithm is, in principle, quite straightforward:

analyze the algorithm to see what kind of inputs yield the largest value of the basic

operation's count C(n) among all possible inputs of size n and then compute this

worst-case value Cworst(n). (For sequential search, the answer was obvious. The

methods for handling less trivial situations are explained in subsequent sections of

this chapter.) Clearly, the worst-case analysis provides very important information

about an algorithm's efficiency by bounding its running time from above. In other
48  Fundamentals of the Analysis of Algorithm Efficiency

    words, it guarantees that for any instance of size n, the running time will not exceed

    Cworst(n), its running time on the worst-case inputs.

    The best-case efficiency of an algorithm is its efficiency for the best-case input

    of size n, which is an input (or inputs) of size n for which the algorithm runs the

    fastest among all possible inputs of that size. Accordingly, we can analyze the best-

    case efficiency as follows. First, we determine the kind of inputs for which the count

    C(n) will be the smallest among all possible inputs of size n. (Note that the best

    case does not mean the smallest input; it means the input of size n for which the

    algorithm runs the fastest.) Then we ascertain the value of C(n) on these most

    convenient inputs. For example, the best-case inputs for sequential search are lists

    of size n with their first element equal to a search key; accordingly, Cbest(n) = 1

    for this algorithm.

    The analysis of the best-case efficiency is not nearly as important as that

    of the worst-case efficiency. But it is not completely useless, either. Though we

    should not expect to get best-case inputs, we might be able to take advantage of

    the fact that for some algorithms a good best-case performance extends to some

    useful types of inputs close to being the best-case ones. For example, there is a

    sorting algorithm (insertion sort) for which the best-case inputs are already sorted

    arrays on which the algorithm works very fast. Moreover, the best-case efficiency

    deteriorates only slightly for almost-sorted arrays. Therefore, such an algorithm

    might well be the method of choice for applications dealing with almost-sorted

    arrays. And, of course, if the best-case efficiency of an algorithm is unsatisfactory,

    we can immediately discard it without further analysis.

    It should be clear from our discussion, however, that neither the worst-case

    analysis nor its best-case counterpart yields the necessary information about an

    algorithm's behavior on a "typical" or "random" input. This is the information that

    the average-case efficiency seeks to provide. To analyze the algorithm's average-

    case efficiency, we must make some assumptions about possible inputs of size n.

    Let's consider again sequential search. The standard assumptions are that

    (a) the probability of a successful search is equal to p (0  p  1) and (b) the

    probability of the first match occurring in the ith position of the list is the same

    for every i. Under these assumptions--the validity of which is usually difficult to

    verify, their reasonableness notwithstanding--we can find the average number

    of key comparisons Cavg(n) as follows. In the case of a successful search, the

    probability of the first match occurring in the ith position of the list is p/n for

    every i, and the number of comparisons made by the algorithm in such a situation

    is obviously i. In the case of an unsuccessful search, the number of comparisons

    will be n with the probability of such a search being (1 - p). Therefore,

    Cavg(n)  =           [1  .  p  +  2  .  p  +  .  .  .  +  i  .  p  +  .  .  .  +  n  .  p]+  n  .  (1  -  p)

                                n           n                       n                       n

             = p [1 + 2 + . . . + i + . . . + n] + n(1 - p)
                         n

             = p n(n + 1) + n(1 - p) = p(n + 1) + n(1 - p).
                         n         2                                   2
                                    2.1  The Analysis Framework                         49

This general formula yields some quite reasonable answers. For example, if p = 1

(the search must be successful), the average number of key comparisons made

by sequential search is (n + 1)/2; that is, the algorithm will inspect, on average,

about half of the list's elements. If p = 0 (the search must be unsuccessful), the

average number of key comparisons will be n because the algorithm will inspect

all n elements on all such inputs.

As you can see from this very elementary example, investigation of the

average-case efficiency is considerably more difficult than investigation of the

worst-case and best-case efficiencies. The direct approach for doing this involves

dividing all instances of size n into several classes so that for each instance of the

class the number of times the algorithm's basic operation is executed is the same.

(What were these classes for sequential search?) Then a probability distribution

of inputs is obtained or assumed so that the expected value of the basic operation's

count can be found.

The technical implementation of this plan is rarely easy, however, and prob-

abilistic assumptions underlying it in each particular case are usually difficult to

verify. Given our quest for simplicity, we will mostly quote known results about

the average-case efficiency of algorithms under discussion. If you are interested

in derivations of these results, consult such books as [Baa00], [Sed96], [KnuI],

[KnuII], and [KnuIII].

It should be clear from the preceding discussion that the average-case ef-

ficiency cannot be obtained by taking the average of the worst-case and the

best-case efficiencies. Even though this average does occasionally coincide with

the average-case cost, it is not a legitimate way of performing the average-case

analysis.

Does one really need the average-case efficiency information? The answer is

unequivocally yes: there are many important algorithms for which the average-

case efficiency is much better than the overly pessimistic worst-case efficiency

would lead us to believe. So, without the average-case analysis, computer scientists

could have missed many important algorithms.

Yet another type of efficiency is called amortized efficiency. It applies not to

a single run of an algorithm but rather to a sequence of operations performed

on the same data structure. It turns out that in some situations a single operation

can be expensive, but the total time for an entire sequence of n such operations is

always significantly better than the worst-case efficiency of that single operation

multiplied by n. So we can "amortize" the high cost of such a worst-case occur-

rence over the entire sequence in a manner similar to the way a business would

amortize the cost of an expensive item over the years of the item's productive life.

This sophisticated approach was discovered by the American computer scientist

Robert Tarjan, who used it, among other applications, in developing an interest-

ing variation of the classic binary search tree (see [Tar87] for a quite readable

nontechnical discussion and [Tar85] for a technical account). We will see an ex-

ample of the usefulness of amortized efficiency in Section 9.2, when we consider

algorithms for finding unions of disjoint sets.
50  Fundamentals of the Analysis of Algorithm Efficiency

    Recapitulation of the Analysis Framework

    Before we leave this section, let us summarize the main points of the framework

    outlined above.

        Both time and space efficiencies are measured as functions of the algorithm's

        input size.

        Time efficiency is measured by counting the number of times the algorithm's

        basic operation is executed. Space efficiency is measured by counting the

        number of extra memory units consumed by the algorithm.

        The efficiencies of some algorithms may differ significantly for inputs of the

        same size. For such algorithms, we need to distinguish between the worst-case,

        average-case, and best-case efficiencies.

        The framework's primary interest lies in the order of growth of the algorithm's

        running time (extra memory units consumed) as its input size goes to infinity.

        In the next section, we look at formal means to investigate orders of growth. In

    Sections 2.3 and 2.4, we discuss particular methods for investigating nonrecursive

    and recursive algorithms, respectively. It is there that you will see how the analysis

    framework outlined here can be applied to investigating the efficiency of specific

    algorithms. You will encounter many more examples throughout the rest of the

    book.

    Exercises 2.1

    1.  For each of the following algorithms, indicate (i) a natural size metric for its

        inputs, (ii) its basic operation, and (iii) whether the basic operation count can

        be different for inputs of the same size:

        a.  computing the sum of n numbers

        b. computing n!

        c.  finding the largest element in a list of n numbers

        d. Euclid's algorithm

        e.  sieve of Eratosthenes

        f.  pen-and-pencil algorithm for multiplying two n-digit decimal integers

    2.  a.  Consider the definition-based algorithm for adding two n × n matrices.

            What is its basic operation? How many times is it performed as a function

            of the matrix order n? As a function of the total number of elements in the

            input matrices?

        b.  Answer the same questions for the definition-based algorithm for matrix

            multiplication.
                                      2.1   The Analysis Framework                          51

3.  Consider a variation of sequential search that scans a list to return the number

    of occurrences of a given search key in the list. Does its efficiency differ from

    the efficiency of classic sequential search?

4.  a.  Glove selection         There are 22 gloves in a drawer: 5 pairs of red gloves, 4

        pairs of yellow, and 2 pairs of green. You select the gloves in the dark and

        can check them only after a selection has been made. What is the smallest

        number of gloves you need to select to have at least one matching pair in

        the best case? In the worst case?

    b.  Missing socks           Imagine that after washing 5 distinct pairs of socks, you

        discover that two socks are missing. Of course, you would like to have

        the largest number of complete pairs remaining. Thus, you are left with

        4 complete pairs in the best-case scenario and with 3 complete pairs in

        the worst case. Assuming that the probability of disappearance for each

        of the 10 socks is the same, find the probability of the best-case scenario;

        the probability of the worst-case scenario; the number of pairs you should

        expect in the average case.

5.  a.  Prove formula (2.1) for the number of bits in the binary representation of

        a positive decimal integer.

    b. Prove the alternative formula for the number of bits in the binary repre-

        sentation of a positive integer n:

                                      b=        log2(n + 1) .

    c.  What would be the analogous formulas for the number of decimal digits?

    d. Explain why, within the accepted analysis framework, it does not matter

        whether we use binary or decimal digits in measuring n's size.

6.  Suggest how any sorting algorithm can be augmented in a way to make the

    best-case count of its key comparisons equal to just n - 1 (n is a list's size,

    of course). Do you think it would be a worthwhile addition to any sorting

    algorithm?

7.  Gaussian elimination, the classic algorithm for solving systems of n linear

    equations   in     n  unknowns,   requires  about   1  n3  multiplications,  which  is  the

    algorithm's basic operation.                        3

    a.  How much longer should you expect Gaussian elimination to work on a

        system of 1000 equations versus a system of 500 equations?

    b.  You are considering buying a computer that is 1000 times faster than the

        one you currently have. By what factor will the faster computer increase

        the sizes of systems solvable in the same amount of time as on the old

        computer?

8.  For each of the following functions, indicate how much the function's value

    will  change   if  its  argument  is increased fourfold.

          a. log2  n        b.  n     c. n      d.  n2     e.  n3  f.  2n
52       Fundamentals of the Analysis of Algorithm Efficiency

         9.   For each of the following pairs of functions, indicate whether the first function

              of each of the following pairs has a lower, same, or higher order of growth (to

              within a constant multiple) than the second function.

                  a. n(n + 1) and 2000n2  b.  100n2 and 0.01n3

                  c. log2 n and ln n      d.  log22 n and log2 n2

                  e.  2n-1 and 2n         f. (n - 1)! and n!

         10.  Invention of chess

              a.  According to a well-known legend, the game of chess was invented many

                  centuries ago in northwestern India by a certain sage. When he took his

                  invention to his king, the king liked the game so much that he offered the

                  inventor any reward he wanted. The inventor asked for some grain to be

                  obtained as follows: just a single grain of wheat was to be placed on the

                  first square of the chessboard, two on the second, four on the third, eight

                  on the fourth, and so on, until all 64 squares had been filled. If it took just

                  1 second to count each grain, how long would it take to count all the grain

                  due to him?

              b. How long would it take if instead of doubling the number of grains for each

                  square of the chessboard, the inventor asked for adding two grains?

    2.2  Asymptotic Notations and Basic Efficiency Classes

         As pointed out in the previous section, the efficiency analysis framework con-

         centrates on the order of growth of an algorithm's basic operation count as the

         principal indicator of the algorithm's efficiency. To compare and rank such orders

         of growth, computer scientists use three notations: O (big oh),  (big omega), and

         (big theta). First, we introduce these notations informally, and then, after sev-

         eral examples, formal definitions are given. In the following discussion, t (n) and

         g(n) can be any nonnegative functions defined on the set of natural numbers. In

         the context we are interested in, t (n) will be an algorithm's running time (usually

         indicated by its basic operation count C(n)), and g(n) will be some simple function

         to compare the count with.

         Informal Introduction

         Informally, O(g(n)) is the set of all functions with a lower or same order of growth

         as g(n) (to within a constant multiple, as n goes to infinity). Thus, to give a few

         examples, the following assertions are all true:

                      n  O(n2),       100n + 5  O(n2),         1 n(n - 1)  O(n2).
                                                               2
2.2           Asymptotic Notations and Basic Efficiency Classes                          53

Indeed, the first two functions are linear and hence have a lower order of growth

than g(n) = n2, while the last one is quadratic and hence has the same order of

growth as n2. On the other hand,

n3  O(n2),                0.00001n3  O(n2),     n4 + n + 1  O(n2).

Indeed, the functions n3 and 0.00001n3 are both cubic and hence have a higher

order of growth than n2, and so has the fourth-degree polynomial n4 + n + 1.

The second notation,         (g(n)), stands for the set of all functions with a higher

or same order of growth as g(n) (to within a constant multiple, as n goes to infinity).

For example,

n3            (n2),       1n(n - 1)     (n2),   but 100n + 5     (n2).
                          2

Finally,      (g(n)) is the set of all functions that have the same order of growth

as g(n) (to within a constant multiple, as n goes to infinity). Thus, every quadratic

function an2 + bn + c with a > 0 is in  (n2), but so are, among infinitely many

others, n2 + sin n and n2 + log n. (Can you explain why?)

Hopefully, this informal introduction has made you comfortable with the idea

behind the three asymptotic notations. So now come the formal definitions.

O -notation

DEFINITION    A function t (n) is said to be in O(g(n)), denoted t (n)  O(g(n)),

if t (n) is bounded above by some constant multiple of g(n) for all large n, i.e., if

there exist some positive constant c and some nonnegative integer n0 such that

                          t (n)  cg(n)  for all n  n0.

The definition is illustrated in Figure 2.1 where, for the sake of visual clarity, n is

extended to be a real number.

As an example, let us formally prove one of the assertions made in the

introduction: 100n + 5  O(n2). Indeed,

              100n + 5  100n + n (for all n  5) = 101n  101n2.

Thus, as values of the constants c and n0 required by the definition, we can take

101 and 5, respectively.

Note that the definition gives us a lot of freedom in choosing specific values

for constants c and n0. For example, we could also reason that

              100n + 5  100n + 5n (for all n  1) = 105n

to complete the proof with c = 105 and n0 = 1.
54  Fundamentals of the Analysis of Algorithm Efficiency

                                                                   cg (n )

                                                                   t (n )

                 doesn't

                 matter

                          n0                                              n

    FIGURE  2.1  Big-oh notation: t (n)  O(g(n)).

                                                                   t (n )

                                                                   cg (n )

                 doesn't

                 matter

                          n0                                              n

    FIGURE  2.2  Big-omega notation:  t (n)       (g(n)).

    -notation

    DEFINITION   A function t (n) is said to be in         (g(n)), denoted t (n)   (g(n)), if

    t (n) is bounded below by some positive constant multiple of g(n) for all large n,

    i.e., if there exist some positive constant c and some nonnegative integer n0 such

    that

                          t (n)  cg(n)             for all n  n0.

    The definition is illustrated in Figure 2.2.

          Here is an example of the formal proof that n3           (n2):

                              n3  n2              for all n  0,

    i.e., we can select c = 1 and n0 = 0.
        2.2    Asymptotic Notations and Basic Efficiency Classes                                                  55

                                                                                       c1g (n )

                                                                                       t (n )

                                                                                       c2g (n )

                          doesn't

                          matter

                                   n0                                                     n

FIGURE  2.3    Big-theta notation: t (n)                 (g(n)).

-notation

DEFINITION        A function t (n) is said to be in                         (g(n)), denoted t (n)            (g(n)),

if t (n) is bounded both above and below by some positive constant multiples of

g(n) for all large n, i.e., if there exist some positive constants c1 and c2 and some

nonnegative integer n0 such that

                           c2g(n)  t (n)  c1g(n)                      for all n  n0.

The definition is illustrated in Figure 2.3.

For example, let us prove                    that     1  n(n   -  1)        (n2). First, we prove            the  right

inequality (the upper bound):                         2

                       1  n(n  -   1)    =   1 n2  -     1  n      1 n2     for all n  0.

                       2                     2           2        2

Second, we prove the left inequality (the lower bound):

            1  n(n  -  1)  =   1 n2   -  1   n        1 n2  -   1  n  1  n  (for  all  n       2)  =  1 n2.

            2                  2         2            2         2     2                               4

Hence,  we   can  select   c2  =   1  ,  c1  =  1  ,  and   n0    =   2.

                                   4            2

Useful Property Involving the Asymptotic Notations

Using the formal definitions of the asymptotic notations, we can prove their

general properties (see Problem 7 in this section's exercises for a few simple

examples). The following property, in particular, is useful in analyzing algorithms

that comprise two consecutively executed parts.
56  Fundamentals of the Analysis of Algorithm Efficiency

    THEOREM         If t1(n)  O(g1(n)) and t2(n)  O(g2(n)), then

                          t1(n) + t2(n)  O(max{g1(n), g2(n)}).

    (The analogous assertions are true for the             and   notations as well.)

    PROOF     The proof extends to orders of growth the following simple fact about

    four arbitrary real numbers a1, b1, a2, b2: if a1  b1 and a2  b2, then a1 + a2 

    2 max{b1, b2}.

    Since t1(n)  O(g1(n)), there exist some positive constant c1 and some non-

    negative integer n1 such that

                          t1(n)  c1g1(n)            for all n  n1.

    Similarly, since t2(n)  O(g2(n)),

                          t2(n)  c2g2(n)            for all n  n2.

    Let us denote c3 = max{c1, c2} and consider n  max{n1, n2} so that we can use

    both inequalities. Adding them yields the following:

                    t1(n) + t2(n)  c1g1(n) + c2g2(n)

                                    c3g1(n) + c3g2(n) = c3[g1(n) + g2(n)]

                                    c32 max{g1(n), g2(n)}.

    Hence, t1(n) + t2(n)  O(max{g1(n), g2(n)}), with the constants c and n0 required

    by the O definition being 2c3 = 2 max{c1, c2} and max{n1, n2}, respectively.

    So what does this property imply for an algorithm that comprises two consec-

    utively executed parts? It implies that the algorithm's overall efficiency is deter-

    mined by the part with a higher order of growth, i.e., its least efficient part:

                 t1(n)  O(g1(n))           t1(n) + t2(n)  O(max{g1(n), g2(n)}).

                 t2(n)  O(g2(n))

    For example, we can check whether an array has equal elements by the following

    two-part algorithm: first, sort the array by applying some known sorting algorithm;

    second, scan the sorted array to check its consecutive elements for equality. If, for

    example,  a  sorting  algorithm  used  in  the  first  part  makes  no  more  than  1  n(n  -  1)

                                                                                        2
    comparisons (and hence is in O(n2)) while the second part makes no more than

    n - 1 comparisons (and hence is in O(n)), the efficiency of the entire algorithm

    will be in O(max{n2, n}) = O(n2).

    Using Limits for Comparing Orders of Growth

    Though the formal definitions of O,        , and       are indispensable for proving their

    abstract properties, they are rarely used for comparing the orders of growth of

    two specific functions. A much more convenient method for doing so is based on
                  2.2     Asymptotic Notations and Basic Efficiency Classes                                                 57

    computing the limit of the ratio of two functions in question. Three principal cases

    may arise:

                               0     implies that t (n) has a smaller order of growth than g(n),
                          
             t (n)
        lim            =       c     implies that t (n) has the same order of growth as g(n),
    n        g(n)         
                                     implies that t (n) has a larger order of growth than g(n).3

    Note that the first two cases mean that t (n)  O(g(n)), the last two mean that

    t (n)    (g(n)), and the second case means that t (n)                                     (g(n)).

        The limit-based approach is often more convenient than the one based on

    the definitions because it can take advantage of the powerful calculus techniques

    developed for computing limits, such as L'Ho^ pital's rule

                                                 lim     t (n) = lim        t (n)

                                                 n g(n)          n g (n)

    and Stirling's formula

                                                        n  n

                                     n!      2 n        e        for large values of n.

        Here are three examples of using the limit-based approach to comparing

    orders of growth of two functions.

    EXAMPLE 1                Compare      the    orders    of    growth  of       1  n(n   -  1)  and  n2.  (This  is  one  of

                                                                                  2
    the examples we used at the beginning of this section to illustrate the definitions.)

                                  1  n(n  -  1)      1           n2 - n        1                    1) =  1

                          lim     2              =      lim              =              lim (1 -             .

                       n             n2              2  n        n2            2     n              n     2

    Since the limit is equal to a positive constant, the functions have the same order

    of  growth    or,     symbolically,      1   n(n  -    1)    (n2).

                                             2

    EXAMPLE 2                Compare the orders of growth of log2 n and                                n. (Unlike Exam-

    ple 1, the answer here is not immediately obvious.)

                  log2 n =                   log2 n              log2 e              1                       1

             lim                     lim                = lim                        n  =  2  log2  e  lim         = 0.

        n                 n       n              n         n             1                             n        n

                                                                     2      n

    Since the limit is       equal   to zero, log2 n has a smaller order                   of growth      than     n. (Since

    limn          log2 n  =  0, we        can use the so-called little-oh                  notation:      log2  n   o(      n).
                    n

    Unlike the big-Oh, the little-oh notation is rarely used in analysis of algorithms.)

3.  The fourth case, in which such a limit does not exist, rarely happens in the actual practice of analyzing

    algorithms. Still, this possibility makes the limit-based approach to comparing orders of growth less

    general than the one based on the definitions of O,          , and         .
58  Fundamentals of the Analysis of Algorithm Efficiency

    EXAMPLE 3  Compare the orders of growth of n! and 2n. (We discussed this

    informally in Section 2.1.) Taking advantage of Stirling's formula, we get

                            2 n      n  n                 nn                         n

         lim  n! = lim               e     =  lim    2 n        =  lim      2 n  n      = .

        n 2n   n                 2n           n           2nen     n             2e

    Thus, though 2n grows very fast, n!grows still faster. We can write symbolically that

    n!       (2n); note, however, that while the big-Omega notation does not preclude

    the possibility that n! and 2n have the same order of growth, the limit computed

    here certainly does.

    Basic Efficiency Classes

    Even though the efficiency analysis framework puts together all the functions

    whose orders of growth differ by a constant multiple, there are still infinitely many

    such classes. (For example, the exponential functions an have different orders of

    growth for different values of base a.) Therefore, it may come as a surprise that

    the time efficiencies of a large number of algorithms fall into only a few classes.

    These classes are listed in Table 2.2 in increasing order of their orders of growth,

    along with their names and a few comments.

         You could raise a concern that classifying algorithms by their asymptotic effi-

    ciency would be of little practical use since the values of multiplicative constants

    are usually left unspecified. This leaves open the possibility of an algorithm in a

    worse efficiency class running faster than an algorithm in a better efficiency class

    for inputs of realistic sizes. For example, if the running time of one algorithm is n3

    while the running time of the other is 106n2, the cubic algorithm will outperform

    the quadratic algorithm unless n exceeds 106. A few such anomalies are indeed

    known. Fortunately, multiplicative constants usually do not differ that drastically.

    As a rule, you should expect an algorithm from a better asymptotic efficiency class

    to outperform an algorithm from a worse class even for moderately sized inputs.

    This observation is especially true for an algorithm with a better than exponential

    running time versus an exponential (or worse) algorithm.

    Exercises 2.2

    1.   Use the most appropriate notation among O,                , and    to indicate the time

         efficiency class of sequential search (see Section 2.1)

         a.  in the worst case.

         b. in the best case.

         c.  in the average case.

    2.   Use the informal definitions of O,        , and  to determine whether the follow-

         ing assertions are true or false.
        2.2      Asymptotic Notations and Basic Efficiency Classes                                 59

    TABLE 2.2    Basic asymptotic efficiency classes

    Class        Name                 Comments

    1            constant             Short of best-case efficiencies, very few reasonable

                                      examples can be given since an algorithm's running

                                      time typically goes to infinity when its input size grows

                                      infinitely large.

    log n        logarithmic          Typically, a result of cutting a problem's size by a

                                      constant factor on each iteration of the algorithm (see

                                      Section 4.4). Note that a logarithmic algorithm cannot

                                      take into account all its input or even a fixed fraction

                                      of it: any algorithm that does so will have at least linear

                                      running time.

    n            linear               Algorithms that scan a list of size n (e.g., sequential

                                      search) belong to this class.

    n log n      linearithmic         Many divide-and-conquer algorithms (see Chapter 5),

                                      including mergesort and quicksort in the average case,

                                      fall into this category.

    n2           quadratic            Typically, characterizes efficiency of algorithms with

                                      two embedded loops (see the next section). Elemen-

                                      tary sorting algorithms and certain operations on n × n

                                      matrices are standard examples.

    n3           cubic                Typically, characterizes efficiency of algorithms with

                                      three embedded loops (see the next section). Several

                                      nontrivial algorithms from linear algebra fall into this

                                      class.

    2n           exponential          Typical for algorithms that generate all subsets of an

                                      n-element set. Often, the term "exponential" is used

                                      in a broader sense to include this and larger orders of

                                      growth as well.

    n!           factorial            Typical for algorithms that generate all permutations

                                      of an n-element set.

        a. n(n + 1)/2  O(n3)             b.   n(n + 1)/2  O(n2)

        c. n(n + 1)/2          (n3)      d.   n(n + 1)/2        (n)

3.  For each of the following functions, indicate the class               (g(n)) the function

    belongs to. (Use the simplest g(n) possible in your answers.) Prove your

    assertions.

        a.  (n2 + 1)10                                   b.  10n2 + 7n + 3

        c.   2n  lg(n  +  2)2  +  (n  +  2)2  lg  n      d.  2n+1 + 3n-1

                                                  2

        e.   log2 n
60  Fundamentals of the Analysis of Algorithm Efficiency

    4.   a.  Table 2.1 contains values of several functions that often arise in the analysis

             of algorithms. These values certainly suggest that the functions

                            log n,      n,  n log2 n,        n2,  n3,  2n,  n!

             are listed in increasing order of their order of growth. Do these values

             prove this fact with mathematical certainty?

         b.  Prove that the functions are indeed listed in increasing order of their order

             of growth.

    5.   List the following functions according to their order of growth from the lowest

         to the highest:

             (n - 2)!,    5 lg(n + 100)10,  22n,       0.001n4 + 3n3 + 1,   ln2 n,  3 n,  3n.

    6.   a.  Prove that every polynomial of degree k, p(n) = aknk + ak-1nk-1 + . . . + a0

             with ak > 0, belongs to        (nk).

         b.  Prove that exponential functions an have different orders of growth for

             different values of base a > 0.

    7.   Prove the following assertions by using the definitions of the notations in-

         volved, or disprove them by giving a specific counterexample.

         a.  If t (n)  O(g(n)), then g(n)          (t (n)).

         b.  (g(n)) =       (g(n)), where  > 0.

         c.  (g(n)) = O(g(n))           (g(n)).

         d. For any two nonnegative functions t (n) and g(n) defined on the set of

             nonnegative integers, either t (n)  O(g(n)), or t (n)          (g(n)), or both.

    8.   Prove the section's theorem for

             a.  notation.          b.      notation.

    9.   We mentioned in this section that one can check whether all elements of an

         array are distinct by a two-part algorithm based on the array's presorting.

         a.  If the presorting is done by an algorithm with a time efficiency in    (n log n),

             what will be a time-efficiency class of the entire algorithm?

         b. If the sorting algorithm used for presorting needs an extra array of size n,

             what will be the space-efficiency class of the entire algorithm?

    10.  The range of a finite nonempty set of n real numbers S is defined as the differ-

         ence between the largest and smallest elements of S. For each representation

         of S given below, describe in English an algorithm to compute the range. Indi-

         cate the time efficiency classes of these algorithms using the most appropriate

         notation (O,     , or  ).

         a.  An unsorted array

         b. A sorted array

         c.  A sorted singly linked list

         d. A binary search tree
               2.3  Mathematical Analysis of Nonrecursive Algorithms                         61

     11.  Lighter or heavier?    You have n > 2 identical-looking coins and a two-pan

          balance scale with no weights. One of the coins is a fake, but you do not know

          whether it is lighter or heavier than the genuine coins, which all weigh the

          same. Design a  (1) algorithm to determine whether the fake coin is lighter

          or heavier than the others.

     12.  Door in a wall  You are facing a wall that stretches infinitely in both direc-

          tions. There is a door in the wall, but you know neither how far away nor in

          which direction. You can see the door only when you are right next to it. De-

          sign an algorithm that enables you to reach the door by walking at most O(n)

          steps where n is the (unknown to you) number of steps between your initial

          position and the door. [Par95]

2.3  Mathematical Analysis of Nonrecursive Algorithms

     In this section, we systematically apply the general framework outlined in Section

     2.1 to analyzing the time efficiency of nonrecursive algorithms. Let us start with

     a very simple example that demonstrates all the principal steps typically taken in

     analyzing such algorithms.

     EXAMPLE 1      Consider the problem of finding the value of the largest element

     in a list of n numbers. For simplicity, we assume that the list is implemented as

     an array. The following is pseudocode of a standard algorithm for solving the

     problem.

     ALGORITHM      MaxElement(A[0..n - 1])

          //Determines the value of the largest element in a given array

          //Input: An array A[0..n - 1] of real numbers

          //Output: The value of the largest element in A

          maxval  A[0]

          for i  1 to n - 1 do

          if A[i] > maxval

                    maxval  A[i]

          return maxval

          The obvious measure of an input's size here is the number of elements in the

     array, i.e., n. The operations that are going to be executed most often are in the

     algorithm's for loop. There are two operations in the loop's body: the comparison

     A[i] > maxval and the assignment maxval  A[i]. Which of these two operations

     should we consider basic? Since the comparison is executed on each repetition

     of the loop and the assignment is not, we should consider the comparison to be

     the algorithm's basic operation. Note that the number of comparisons will be the

     same for all arrays of size n; therefore, in terms of this metric, there is no need to

     distinguish among the worst, average, and best cases here.
62      Fundamentals of the Analysis of Algorithm Efficiency

            Let us denote C(n) the number of times this comparison is executed and try

        to find a formula expressing it as a function of size n. The algorithm makes one

        comparison on each execution of the loop, which is repeated for each value of the

        loop's variable i within the bounds 1 and n - 1, inclusive. Therefore, we get the

        following sum for C(n):

                                                       n-1

                                              C(n) =         1.

                                                       i=1

        This is an easy sum to compute because it is nothing other than 1 repeated n - 1

        times. Thus,

                                              n-1

                                 C(n) =            1=n-1                       (n).

                                              i=1

            Here is a general plan to follow in analyzing nonrecursive algorithms.

        General Plan for Analyzing the Time Efficiency of Nonrecursive Algorithms

        1.  Decide on a parameter (or parameters) indicating an input's size.

        2.  Identify the algorithm's basic operation. (As a rule, it is located in the inner-

            most loop.)

        3.  Check whether the number of times the basic operation is executed depends

            only on the size of an input. If it also depends on some additional property,

            the worst-case, average-case, and, if necessary, best-case efficiencies have to

            be investigated separately.

        4.  Set up a sum expressing the number of times the algorithm's basic operation

            is executed.4

        5.  Using standard formulas and rules of sum manipulation, either find a closed-

            form formula for the count or, at the very least, establish its order of growth.

            Before proceeding with further examples, you may want to review Appen-

        dix A, which contains a list of summation formulas and rules that are often useful

        in analysis of algorithms. In particular, we use especially frequently two basic rules

        of sum manipulation

                                         u                u

                                              cai = c        ai ,                    (R1)

                                         i=l           i=l

                                 u                     u                    u

                                      (ai ± bi) =            ai ±              bi ,  (R2)

                                 i=l               i=l             i=l

    4.  Sometimes, an analysis of a nonrecursive algorithm requires setting up not a sum but a recurrence

        relation for the number of times its basic operation is executed. Using recurrence relations is much

        more typical for analyzing recursive algorithms (see Section 2.4).
     2.3      Mathematical Analysis of Nonrecursive Algorithms                         63

and two summation formulas

u

     1=u-l +1          where l  u are some lower and upper integer limits,            (S1)

i=l

n         n   i = 1 + 2 + . . . + n = n(n + 1)  1 n2 

     i=                                      2      2         (n2).                   (S2)

i=0      i=1

Note that the formula  n-1  1  =  n  -   1,  which  we  used  in  Example  1,  is  a  special

                       i=1
case of formula (S1) for l = 1 and u = n - 1.

EXAMPLE 2     Consider the element uniqueness problem: check whether all the

elements in a given array of n elements are distinct. This problem can be solved

by the following straightforward algorithm.

ALGORITHM     UniqueElements(A[0..n - 1])

//Determines whether all the elements in a given array are distinct

//Input: An array A[0..n - 1]

//Output: Returns "true" if all the elements in A are distinct

//            and "false" otherwise

for i  0 to n - 2 do

     for j  i + 1 to n - 1 do

           if A[i] = A[j ] return false

return true

The natural measure of the input's size here is again n, the number of elements

in the array. Since the innermost loop contains a single operation (the comparison

of two elements), we should consider it as the algorithm's basic operation. Note,

however, that the number of element comparisons depends not only on n but also

on whether there are equal elements in the array and, if there are, which array

positions they occupy. We will limit our investigation to the worst case only.

By definition, the worst case input is an array for which the number of element

comparisons Cworst(n) is the largest among all arrays of size n. An inspection of

the innermost loop reveals that there are two kinds of worst-case inputs--inputs

for which the algorithm does not exit the loop prematurely: arrays with no equal

elements and arrays in which the last two elements are the only pair of equal

elements. For such inputs, one comparison is made for each repetition of the

innermost loop, i.e., for each value of the loop variable j between its limits i + 1

and n - 1; this is repeated for each value of the outer loop, i.e., for each value of

the loop variable i between its limits 0 and n - 2. Accordingly, we get
64  Fundamentals of the Analysis of Algorithm Efficiency

                        n-2   n-1            n-2                                             n-2

    Cworst (n) =                     1=           [(n - 1) - (i + 1) + 1] =                       (n - 1 - i)

                        i=0 j =i+1           i=0                                             i=0

                        n-2                  n-2                    n-2            (n  -  2)(n     -  1)

                    =         (n -   1)  -        i  =  (n   -  1)        1-                 2

                        i=0                  i=0                    i=0

                    = (n - 1)2 - (n - 2)(n - 1) = (n - 1)n  1 n2                                      (n2).
                                                    2                     2              2

    We also could have computed the sum                     ni=-02(n - 1 - i) faster as follows:

               n-2                                                                       (n  -     1)n

                    (n  -  1  -  i)  =   (n  -  1)   +  (n   -  2)  +  .  .  .  +  1  =                 ,

               i=0                                                                              2

    where the last equality is obtained by applying summation formula (S2). Note

    that this result was perfectly predictable: in the worst case, the algorithm needs to

    compare all n(n - 1)/2 distinct pairs of its n elements.

    EXAMPLE 3  Given two n × n matrices A and B, find the time efficiency of the

    definition-based algorithm for computing their product C = AB. By definition, C

    is an n × n matrix whose elements are computed as the scalar (dot) products of

    the rows of matrix A and the columns of matrix B:

                                 A                           B                               C

                                             *                               =

    row i                                                                          C [i, j]

                                                     col. j

    where C[i, j ] = A[i, 0]B[0, j ] + . . . + A[i, k]B[k, j ] + . . . + A[i, n - 1]B[n - 1, j ]

    for every pair of indices 0  i, j  n - 1.

    ALGORITHM       MatrixMultiplication(A[0..n - 1, 0..n - 1], B[0..n - 1, 0..n - 1])

    //Multiplies two square matrices of order n by the definition-based algorithm

    //Input: Two n × n matrices A and B

    //Output: Matrix C = AB

    for i  0 to n - 1 do

    for j  0 to n - 1 do

               C[i, j ]  0.0

               for k  0 to n - 1 do

                    C[i, j ]  C[i, j ] + A[i, k]  B[k, j ]

    return C
2.3              Mathematical Analysis of Nonrecursive Algorithms                            65

We measure an input's size by matrix order n. There are two arithmetical

operations in the innermost loop here--multiplication and addition--that, in

principle, can compete for designation as the algorithm's basic operation. Actually,

we do not have to choose between them, because on each repetition of the

innermost loop each of the two is executed exactly once. So by counting one

we automatically count the other. Still, following a well-established tradition, we

consider multiplication as the basic operation (see Section 2.1). Let us set up a sum

for the total number of multiplications M(n) executed by the algorithm. (Since this

count depends only on the size of the input matrices, we do not have to investigate

the worst-case, average-case, and best-case efficiencies separately.)

Obviously, there is just one multiplication executed on each repetition of the

algorithm's innermost loop, which is governed by the variable k ranging from the

lower bound 0 to the upper bound n - 1. Therefore, the number of multiplications

made for every pair of specific values of variables i and j is

                                        n-1

                                             1,

                                        k=0

and the total    number  of  multiplications M(n)      is  expressed        by  the  following

triple sum:

                                        n-1 n-1 n-1

                             M(n) =                    1.

                                        i=0 j =0 k=0

Now, we can compute this sum by using formula (S1) and rule (R1) given

above. Starting with the innermost sum       n-1  1,   which  is  equal  to  n  (why?),  we  get

                                             k=0

                         n-1 n-1 n-1       n-1 n-1         n-1

                 M(n) =                1=              n=         n2 = n3.

                         i=0 j =0 k=0        i=0 j =0      i=0

This example is simple enough so that we could get this result without all

the summation machinations. How? The algorithm computes n2 elements of the

product matrix. Each of the product's elements is computed as the scalar (dot)

product of an n-element row of the first matrix and an n-element column of the

second matrix, which takes n multiplications. So the total number of multiplica-

tions is n . n2 = n3. (It is this kind of reasoning that we expected you to employ

when answering this question in Problem 2 of Exercises 2.1.)

If we now want to estimate the running time of the algorithm on a particular

machine, we can do it by the product

                             T (n)  cmM(n) = cmn3,

where cm is the time of one multiplication on the machine in question. We would

get a more accurate estimate if we took into account the time spent on the

additions, too:

               T (n)  cmM(n) + caA(n) = cmn3 + can3 = (cm + ca)n3,
66  Fundamentals of the Analysis of Algorithm Efficiency

    where ca is the time of one addition. Note that the estimates differ only by their

    multiplicative constants and not by their order of growth.

    You should not have the erroneous impression that the plan outlined above

    always succeeds in analyzing a nonrecursive algorithm. An irregular change in a

    loop variable, a sum too complicated to analyze, and the difficulties intrinsic to

    the average case analysis are just some of the obstacles that can prove to be insur-

    mountable. These caveats notwithstanding, the plan does work for many simple

    nonrecursive algorithms, as you will see throughout the subsequent chapters of

    the book.

    As a last example, let us consider an algorithm in which the loop's variable

    changes in a different manner from that of the previous examples.

    EXAMPLE 4  The following algorithm finds the number of binary digits in the

    binary representation of a positive decimal integer.

    ALGORITHM     Binary(n)

    //Input: A positive decimal integer n

    //Output: The number of binary digits in n's binary representation

    count  1

    while n > 1 do

    count  count + 1

    n          n/2

    return count

    First, notice that the most frequently executed operation here is not inside the

    while loop but rather the comparison n > 1 that determines whether the loop's

    body will be executed. Since the number of times the comparison will be executed

    is larger than the number of repetitions of the loop's body by exactly 1, the choice

    is not that important.

    A more significant feature of this example is the fact that the loop variable

    takes on only a few values between its lower and upper limits; therefore, we

    have to use an alternative way of computing the number of times the loop is

    executed. Since the value of n is about halved on each repetition of the loop,

    the answer should be about log2 n. The exact formula for the number of times

    the comparison n > 1 will be executed is actually     log2 n  + 1--the number of bits

    in the binary representation of n according to formula (2.1). We could also get

    this answer by applying the analysis technique based on recurrence relations; we

    discuss this technique in the next section because it is more pertinent to the analysis

    of recursive algorithms.
        2.3       Mathematical Analysis of Nonrecursive Algorithms                         67

Exercises 2.3

1.  Compute the following sums.

    a.  1 + 3 + 5 + 7 + . . . + 999

    b.  2 + 4 + 8 + 16 + . . . + 1024

    c.  n+1    1          d.  n+1  i                    e.       n-1  i(i    +     1)

        i=3                   i=3                                i=0

    f.  n      3j  +1     g.  n       n     ij          h.       n    1/ i(i       +   1)

        j =1                  i=1     j =1                       i=1

2.  Find the order of growth of the following sums. Use the                                (g(n)) notation with

    the simplest function g(n) possible.

    a.  ni =-01(i 2 +1)2      b.      n-1   lg   i2

                                      i=2        ij-=10(i + j )
                                      n-1
    c.  in=1(i + 1)2i-1       d.
                                      i=0

3.  The sample variance of n measurements x1, . . . , xn can be computed as either

                              in=1(xi - x¯)2                                    n      xi

                                                     where x¯ =                 i=1

                                  n-1                                           n

    or

                                      n     xi2  -   (      n    xi  )2/  n

                                      i=1                   i=1              .

                                                 n-1

    Find and compare the number of divisions, multiplications, and additions/

    subtractions (additions and subtractions are usually bunched together) that

    are required for computing the variance according to each of these formulas.

4.  Consider the following algorithm.

    ALGORITHM             Mystery(n)

        //Input: A nonnegative integer n

        S0

        for i  1 to n do

               SS+ii

        return S

    a.  What does this algorithm compute?

    b.  What is its basic operation?

    c.  How many times is the basic operation executed?

    d.  What is the efficiency class of this algorithm?

    e.  Suggest an improvement, or a better algorithm altogether, and indicate its

        efficiency class. If you cannot do it, try to prove that, in fact, it cannot be

        done.
68  Fundamentals of the Analysis of Algorithm Efficiency

    5. Consider the following algorithm.

        ALGORITHM    Secret(A[0..n - 1])

        //Input: An array A[0..n - 1] of n real numbers

        minval  A[0]; maxval  A[0]

        for i  1 to n - 1 do

        if A[i] < minval

        minval  A[i]

        if A[i] > maxval

        maxval  A[i]

        return maxval - minval

        Answer questions (a)­(e) of Problem 4 about this algorithm.

    6.  Consider the following algorithm.

        ALGORITHM    Enigma(A[0..n - 1, 0..n - 1])

        //Input: A matrix A[0..n - 1, 0..n - 1] of real numbers

        for i  0 to n - 2 do

        for j  i + 1 to n - 1 do

        if A[i, j ] = A[j, i]

                     return false

        return true

        Answer questions (a)­(e) of Problem 4 about this algorithm.

    7.  Improve the implementation of the matrix multiplication algorithm (see Ex-

        ample 3) by reducing the number of additions made by the algorithm. What

        effect will this change have on the algorithm's efficiency?

    8.  Determine the asymptotic order of growth for the total number of times all

        the doors are toggled in the locker doors puzzle (Problem 12 in Exercises 1.1).

    9.  Prove the formula

                           n    i = 1 + 2 + . . . + n = n(n + 1)

                           i=1                            2

        either by mathematical induction or by following the insight of a 10-year-old

        school boy named Carl Friedrich Gauss (1777­1855) who grew up to become

        one of the greatest mathematicians of all times.
         2.3  Mathematical Analysis of Nonrecursive Algorithms                        69

10.  Mental arithmetic        A 10×10 table is filled with repeating numbers on its

     diagonals as shown below. Calculate the total sum of the table's numbers in

     your head (after [Cra07, Question 1.33]).

                         1     2   3           ...              9   10

                         2     3                            9   10  11

                         3                          9       10  11

                                                9   10      11

                                           9    10  11

                         ...           9   10   11                  ...

                                   9   10  11

                               9   10  11                           17

                         9     10  11                           17  18

                         10    11              ...          17  18  19

11.  Consider the following version of an important algorithm that we will study

     later in the book.

     ALGORITHM  GE(A[0..n - 1, 0..n])

         //Input: An n × (n + 1) matrix A[0..n - 1, 0..n] of real numbers

         for i  0 to n - 2 do

              for j  i + 1 to n - 1 do

              for k  i to n do

                         A[j, k]  A[j, k] - A[i, k]  A[j, i] / A[i, i]

     a.  Find the time efficiency class of this algorithm.

     b.  What glaring inefficiency does this pseudocode contain and how can it be

         eliminated to speed the algorithm up?

12.  von Neumann's neighborhood            Consider the algorithm that starts with a

     single square and on each of its n iterations adds new squares all around the

     outside. How many one-by-one squares are there after n iterations? [Gar99]

     (In the parlance of cellular automata theory, the answer is the number of cells

     in the von Neumann neighborhood of range n.) The results for n = 0, 1, and

     2 are illustrated below.
70       Fundamentals of the Analysis of Algorithm Efficiency

                    n=0                         n=1                         n=2

         13.  Page numbering     Find the total number of decimal digits needed for num-

              bering pages in a book of 1000 pages. Assume that the pages are numbered

              consecutively starting with 1.

    2.4  Mathematical Analysis of Recursive Algorithms

         In this section, we will see how to apply the general framework for analysis

         of algorithms to recursive algorithms. We start with an example often used to

         introduce novices to the idea of a recursive algorithm.

         EXAMPLE 1  Compute the factorial function F (n) = n! for an arbitrary nonneg-

         ative integer n. Since

                    n! = 1 . . . . . (n - 1) . n = (n - 1)! . n   for n  1

         and 0! = 1 by definition, we can compute F (n) = F (n - 1) . n with the following

         recursive algorithm.

         ALGORITHM  F(n)

              //Computes n! recursively

              //Input: A nonnegative integer n

              //Output: The value of n!

              if n = 0 return 1

              else return F (n - 1)  n

              For simplicity, we consider n itself as an indicator of this algorithm's input size

         (rather than the number of bits in its binary expansion). The basic operation of the

         algorithm is multiplication,5 whose number of executions we denote M(n). Since

         the function F (n) is computed according to the formula

                                 F (n) = F (n - 1) . n  for n > 0,

    5.   Alternatively, we could count the number of times the comparison n = 0 is executed, which is the same

         as counting the total number of calls made by the algorithm (see Problem 2 in this section's exercises).
            2.4  Mathematical Analysis of Recursive Algorithms                             71

the number of multiplications M(n) needed to compute it must satisfy the equality

                 M(n) = M(n - 1)       +       1        for n > 0.

                           to compute     to multiply

                           F (n-1)        F (n-1) by n

Indeed, M(n - 1) multiplications are spent to compute F (n - 1), and one more

multiplication is needed to multiply the result by n.

The last equation defines the sequence M(n) that we need to find. This equa-

tion defines M(n) not explicitly, i.e., as a function of n, but implicitly as a function

of its value at another point, namely n - 1. Such equations are called recurrence

relations or, for brevity, recurrences. Recurrence relations play an important role

not only in analysis of algorithms but also in some areas of applied mathematics.

They are usually studied in detail in courses on discrete mathematics or discrete

structures; a very brief tutorial on them is provided in Appendix B. Our goal now

is to solve the recurrence relation M(n) = M(n - 1) + 1, i.e., to find an explicit

formula for M(n) in terms of n only.

Note, however, that there is not one but infinitely many sequences that satisfy

this recurrence. (Can you give examples of, say, two of them?) To determine a

solution uniquely, we need an initial condition that tells us the value with which

the sequence starts. We can obtain this value by inspecting the condition that

makes the algorithm stop its recursive calls:

                             if n = 0 return 1.

This tells us two things. First, since the calls stop when n = 0, the smallest value

of n for which this algorithm is executed and hence M(n) defined is 0. Second, by

inspecting the pseudocode's exiting line, we can see that when n = 0, the algorithm

performs no multiplications. Therefore, the initial condition we are after is

                             M(0) = 0.

the calls stop when n = 0                               no multiplications when n = 0

Thus, we succeeded in setting up the recurrence relation and initial condition

for the algorithm's number of multiplications M(n):

                 M(n) = M(n - 1) + 1              for n > 0,                           (2.2)

                 M(0) = 0.

Before we embark on a discussion of how to solve this recurrence, let us

pause to reiterate an important point. We are dealing here with two recursively

defined functions. The first is the factorial function F (n) itself; it is defined by the

recurrence

                 F (n) = F (n - 1) . n         for every n > 0,

                 F (0) = 1.

The second is the number of multiplications M(n) needed to compute F (n) by the

recursive algorithm whose pseudocode was given at the beginning of the section.
72  Fundamentals of the Analysis of Algorithm Efficiency

    As we just showed, M(n) is defined by recurrence (2.2). And it is recurrence (2.2)

    that we need to solve now.

        Though it is not difficult to "guess" the solution here (what sequence starts

    with 0 when n = 0 and increases by 1 on each step?), it will be more useful to

    arrive at it in a systematic fashion. From the several techniques available for

    solving recurrence relations, we use what can be called the method of backward

    substitutions. The method's idea (and the reason for the name) is immediately

    clear from the way it applies to solving our particular recurrence:

        M(n) = M(n - 1) + 1                        substitute M(n - 1) = M(n - 2) + 1

        = [M(n - 2) + 1] + 1 = M(n - 2) + 2        substitute M(n - 2) = M(n - 3) + 1

        = [M(n - 3) + 1] + 2 = M(n - 3) + 3.

    After inspecting the first three lines, we see an emerging pattern, which makes it

    possible to predict not only the next line (what would it be?) but also a general

    formula for the pattern: M(n) = M(n - i) + i. Strictly speaking, the correctness of

    this formula should be proved by mathematical induction, but it is easier to get to

    the solution as follows and then verify its correctness.

        What remains to be done is to take advantage of the initial condition given.

    Since it is specified for n = 0, we have to substitute i = n in the pattern's formula

    to get the ultimate result of our backward substitutions:

        M(n) = M(n - 1) + 1 = . . . = M(n - i) + i = . . . = M(n - n) + n = n.

        You should not be disappointed after exerting so much effort to get this

    "obvious" answer. The benefits of the method illustrated in this simple example

    will become clear very soon, when we have to solve more difficult recurrences.

    Also, note that the simple iterative algorithm that accumulates the product of n

    consecutive integers requires the same number of multiplications, and it does so

    without the overhead of time and space used for maintaining the recursion's stack.

        The issue of time efficiency is actually not that important for the problem of

    computing n!, however. As we saw in Section 2.1, the function's values get so large

    so fast that we can realistically compute exact values of n! only for very small n's.

    Again, we use this example just as a simple and convenient vehicle to introduce

    the standard approach to analyzing recursive algorithms.

        Generalizing our experience with investigating the recursive algorithm for

    computing n!, we can now outline a general plan for investigating recursive algo-

    rithms.

    General Plan for Analyzing the Time Efficiency of Recursive Algorithms

    1.  Decide on a parameter (or parameters) indicating an input's size.

    2.  Identify the algorithm's basic operation.
              2.4  Mathematical Analysis of Recursive Algorithms                         73

3.  Check whether the number of times the basic operation is executed can vary

    on different inputs of the same size; if it can, the worst-case, average-case, and

    best-case efficiencies must be investigated separately.

4.  Set up a recurrence relation, with an appropriate initial condition, for the

    number of times the basic operation is executed.

5.  Solve the recurrence or, at least, ascertain the order of growth of its solution.

EXAMPLE 2     As our next example, we consider another educational workhorse

of recursive algorithms: the Tower of Hanoi puzzle. In this puzzle, we (or mythical

monks, if you do not like to move disks) have n disks of different sizes that can

slide onto any of three pegs. Initially, all the disks are on the first peg in order of

size, the largest on the bottom and the smallest on top. The goal is to move all the

disks to the third peg, using the second one as an auxiliary, if necessary. We can

move only one disk at a time, and it is forbidden to place a larger disk on top of a

smaller one.

    The problem has an elegant recursive solution, which is illustrated in Fig-

ure 2.4. To move n > 1 disks from peg 1 to peg 3 (with peg 2 as auxiliary), we first

move recursively n - 1 disks from peg 1 to peg 2 (with peg 3 as auxiliary), then

move the largest disk directly from peg 1 to peg 3, and, finally, move recursively

n - 1 disks from peg 2 to peg 3 (using peg 1 as auxiliary). Of course, if n = 1, we

simply move the single disk directly from the source peg to the destination peg.

                         1                            3

                                           2

FIGURE  2.4   Recursive  solution to  the  Tower  of  Hanoi puzzle.
74  Fundamentals of the Analysis of Algorithm Efficiency

    Let us apply the general plan outlined above to the Tower of Hanoi problem.

    The number of disks n is the obvious choice for the input's size indicator, and so is

    moving one disk as the algorithm's basic operation. Clearly, the number of moves

    M(n) depends on n only, and we get the following recurrence equation for it:

                 M(n) = M(n - 1) + 1 + M(n - 1)           for n > 1.

    With the obvious initial condition M(1) = 1, we have the following recurrence

    relation for the number of moves M(n):

                          M(n) = 2M(n - 1) + 1            for n > 1,              (2.3)

                          M(1) = 1.

    We solve this recurrence by the same method of backward substitutions:

    M(n) = 2M(n - 1) + 1                                  sub. M(n - 1) = 2M(n - 2) + 1

    = 2[2M(n - 2) + 1] + 1 = 22M(n - 2) + 2 + 1           sub. M(n - 2) = 2M(n - 3) + 1

    = 22[2M(n - 3) + 1] + 2 + 1 = 23M(n - 3) + 22 + 2 + 1.

    The pattern of the first three sums on the left suggests that the next one will be

    24M(n - 4) + 23 + 22 + 2 + 1, and generally, after i substitutions, we get

    M(n) = 2iM(n - i) + 2i-1 + 2i-2 + . . . + 2 + 1 = 2iM(n - i) + 2i - 1.

    Since the initial condition is specified for n = 1, which is achieved for i = n - 1, we

    get the following formula for the solution to recurrence (2.3):

    M(n) = 2n-1M(n - (n - 1)) + 2n-1 - 1

                 = 2n-1M(1) + 2n-1 - 1 = 2n-1 + 2n-1 - 1 = 2n - 1.

    Thus, we have an exponential algorithm, which will run for an unimaginably

    long time even for moderate values of n (see Problem 5 in this section's exercises).

    This is not due to the fact that this particular algorithm is poor; in fact, it is not

    difficult to prove that this is the most efficient algorithm possible for this problem.

    It is the problem's intrinsic difficulty that makes it so computationally hard. Still,

    this example makes an important general point:

    One should be careful with recursive algorithms because their succinctness

    may mask their inefficiency.

    When a recursive algorithm makes more than a single call to itself, it can be

    useful for analysis purposes to construct a tree of its recursive calls. In this tree,

    nodes correspond to recursive calls, and we can label them with the value of the

    parameter (or, more generally, parameters) of the calls. For the Tower of Hanoi

    example, the tree is given in Figure 2.5. By counting the number of nodes in the

    tree, we can get the total number of calls made by the Tower of Hanoi algorithm:

            n-1

    C(n) =       2l (where l is the level in the tree in Figure 2.5) = 2n - 1.

            l=0
            2.4     Mathematical Analysis of Recursive Algorithms                         75

                                                   n

                      n­1                                           n­1

               n­2                 n­2                  n­2                   n­2

   2                  2                                             2                  2

1           1    1              1                       1                1    1           1

FIGURE 2.5  Tree of recursive      calls made  by  the  recursive  algorithm  for the  Tower of

            Hanoi puzzle.

The number agrees, as it should, with the move count obtained earlier.

EXAMPLE 3        As our next example, we investigate a recursive version of the

algorithm discussed at the end of Section 2.3.

ALGORITHM        BinRec(n)

   //Input: A positive decimal integer n

   //Output: The number of binary digits in n's binary representation

   if n = 1 return 1

   else return BinRec( n/2 ) + 1

   Let us set up a recurrence and an initial condition for the number of addi-

tions A(n) made by the algorithm. The number of additions made in computing

BinRec( n/2 ) is A( n/2 ), plus one more addition is made by the algorithm to

increase the returned value by 1. This leads to the recurrence

                           A(n) = A( n/2 ) + 1          for n > 1.                        (2.4)

Since the recursive calls end when n is equal to 1 and there are no additions made

then, the initial condition is

                                        A(1) = 0.

   The presence of    n/2       in the function's argument makes the method of back-

ward substitutions stumble on values of n that are not powers of 2. Therefore, the

standard approach to solving such a recurrence is to solve it only for n = 2k and

then take advantage of the theorem called the smoothness rule (see Appendix B),

which claims that under very broad assumptions the order of growth observed for

n = 2k gives a correct answer about the order of growth for all values of n. (Alter-

natively, after getting a solution for powers of 2, we can sometimes fine-tune this

solution to get a formula valid for an arbitrary n.) So let us apply this recipe to our

recurrence, which for n = 2k takes the form
76  Fundamentals of the Analysis of Algorithm Efficiency

                          A(2k) = A(2k-1) + 1             for k > 0,

                          A(20) = 0.

    Now backward substitutions encounter no problems:

        A(2k) = A(2k-1) + 1                       substitute A(2k-1) = A(2k-2) + 1

            = [A(2k-2) + 1] + 1 = A(2k-2) + 2     substitute A(2k-2) = A(2k-3) + 1

            = [A(2k-3) + 1] + 2 = A(2k-3) + 3                         ...

                          ...

                                 = A(2k-i) + i

                          ...

                                 = A(2k-k) + k.

    Thus, we end up with

                                  A(2k) = A(1) + k = k,

    or, after returning to the original variable n = 2k and hence k = log2 n,

                                  A(n) = log2 n       (log n).

    In fact, one can prove (Problem 7 in this section's exercises) that the exact solution

    for an arbitrary value of n is given by just a slightly more refined formula A(n) =

    log2 n .

        This section provides an introduction to the analysis of recursive algorithms.

    These techniques will be used throughout the book and expanded further as

    necessary. In the next section, we discuss the Fibonacci numbers; their analysis

    involves more difficult recurrence relations to be solved by a method different

    from backward substitutions.

    Exercises 2.4

    1.  Solve the following recurrence relations.

        a.  x(n) = x(n - 1) + 5   for n > 1,    x(1) = 0

        b. x(n) = 3x(n - 1) for n > 1,      x(1) = 4

        c.  x(n) = x(n - 1) + n for n > 0,      x(0) = 0

        d.  x(n) = x(n/2) + n for n > 1,       x(1) = 1 (solve for n = 2k)

        e.  x(n) = x(n/3) + 1 for n > 1,       x(1) = 1 (solve for n = 3k)

    2.  Set up and solve a recurrence relation for the number of calls made by F (n),

        the recursive algorithm for computing n!.

    3.  Consider the following recursive algorithm for computing the sum of the first

        n cubes: S(n) = 13 + 23 + . . . + n3.
        2.4  Mathematical Analysis of Recursive Algorithms                              77

    ALGORITHM       S(n)

        //Input: A positive integer n

        //Output: The sum of the first n cubes

        if n = 1 return 1

        else return S(n - 1) + n  n  n

    a.  Set up and solve a recurrence relation for the number of times the algo-

        rithm's basic operation is executed.

    b.  How does this algorithm compare with the straightforward nonrecursive

        algorithm for computing this sum?

4.  Consider the following recursive algorithm.

    ALGORITHM       Q(n)

        //Input: A positive integer n

        if n = 1 return 1

        else return Q(n - 1) + 2  n - 1

    a.  Set up a recurrence relation for this function's values and solve it to deter-

        mine what this algorithm computes.

    b.  Set up a recurrence relation for the number of multiplications made by this

        algorithm and solve it.

    c.  Set up a recurrence relation for the number of additions/subtractions made

        by this algorithm and solve it.

5.  Tower of Hanoi

    a.  In the original version of the Tower of Hanoi puzzle, as it was published in

        the 1890s by E´ douard Lucas, a French mathematician, the world will end

        after 64 disks have been moved from a mystical Tower of Brahma. Estimate

        the number of years it will take if monks could move one disk per minute.

        (Assume that monks do not eat, sleep, or die.)

    b. How many moves are made by the ith largest disk (1  i  n) in this

        algorithm?

    c.  Find a nonrecursive algorithm for the Tower of Hanoi puzzle and imple-

        ment it in the language of your choice.

6.  Restricted Tower of Hanoi    Consider the version of the Tower of Hanoi

    puzzle in which n disks have to be moved from peg A to peg C using peg

    B so that any move should either place a disk on peg B or move a disk from

    that peg. (Of course, the prohibition of placing a larger disk on top of a smaller

    one remains in place, too.) Design a recursive algorithm for this problem and

    find the number of moves made by it.
78  Fundamentals of the Analysis of Algorithm Efficiency

    7.   a.  Prove that the exact number of additions made by the recursive algorithm

             BinRec(n) for an arbitrary positive decimal integer n is     log2 n .

         b.  Set up a recurrence relation for the number of additions made by the

             nonrecursive version of this algorithm (see Section 2.3, Example 4) and

             solve it.

    8.   a.  Design a recursive algorithm for computing 2n for any nonnegative integer

             n that is based on the formula 2n = 2n-1 + 2n-1.

         b.  Set up a recurrence relation for the number of additions made by the

             algorithm and solve it.

         c.  Draw a tree of recursive calls for this algorithm and count the number of

             calls made by the algorithm.

         d.  Is it a good algorithm for solving this problem?

    9.   Consider the following recursive algorithm.

         ALGORITHM      Riddle(A[0..n - 1])

             //Input: An array A[0..n - 1] of real numbers

             if n = 1 return A[0]

             else temp  Riddle(A[0..n - 2])

                   if temp  A[n - 1] return temp

                   else return A[n - 1]

         a.  What does this algorithm compute?

         b.  Set up a recurrence relation for the algorithm's basic operation count and

             solve it.

    10.  Consider the following algorithm to check whether a graph defined by its

         adjacency matrix is complete.

         ALGORITHM      GraphComplete(A[0..n - 1, 0..n - 1])

             //Input: Adjacency matrix A[0..n - 1, 0..n - 1]) of an undirected graph G

             //Output: 1 (true) if G is complete and 0 (false) otherwise

             if n = 1 return 1    //one-vertex graph is complete by definition

             else

                   if not GraphComplete(A[0..n - 2, 0..n - 2]) return 0

                   else for j  0 to n - 2 do

                        if A[n - 1, j ] = 0 return 0

                        return 1

         What is the algorithm's efficiency class in the worst case?

    11.  The determinant of an n × n matrix
              2.4  Mathematical Analysis of Recursive Algorithms                       79

                                        a0 0           ...  a0 n-1       

                               A =      a1... 0        ...  a1 n... -1    ,

                                     an-1 0            ...  an-1 n-1

     denoted det A, can be defined as a00 for n = 1 and, for n > 1, by the recursive

     formula

                                                 n-1

                               det A =                 sj a0 j det Aj ,

                                                 j =0

     where sj is +1 if j is even and -1 if j is odd, a0 j is the element in row 0 and

     column j , and Aj is the (n - 1) × (n - 1) matrix obtained from matrix A by

     deleting its row 0 and column j .

     a.  Set up a recurrence relation for the number of multiplications made by the

         algorithm implementing this recursive definition.

     b. Without solving the recurrence, what can you say about the solution's order

         of growth as compared to n!?

12.  von Neumann's neighborhood revisited                   Find the number of cells in the von

     Neumann neighborhood of range n (Problem 12 in Exercises 2.3) by setting

     up and solving a recurrence relation.

13.  Frying hamburgers    There are n hamburgers to be fried on a small grill that

     can hold only two hamburgers at a time. Each hamburger has to be fried

     on both sides; frying one side of a hamburger takes 1 minute, regardless of

     whether one or two hamburgers are fried at the same time. Consider the

     following recursive algorithm for executing this task in the minimum amount

     of time. If n  2, fry the hamburger or the two hamburgers together on each

     side. If n > 2, fry any two hamburgers together on each side and then apply

     the same procedure recursively to the remaining n - 2 hamburgers.

     a.  Set up and solve the recurrence for the amount of time this algorithm needs

         to fry n hamburgers.

     b. Explain why this algorithm does not fry the hamburgers in the minimum

         amount of time for all n > 0.

     c.  Give a correct recursive algorithm that executes the task in the minimum

         amount of time.

14.  Celebrity problem    A celebrity among a group of n people is a person who

     knows nobody but is known by everybody else. The task is to identify a

     celebrity by only asking questions to people of the form "Do you know

     him/her?" Design an efficient algorithm to identify a celebrity or determine

     that the group has no such person. How many questions does your algorithm

     need in the worst case?
80       Fundamentals of the Analysis of Algorithm Efficiency

    2.5  Example: Computing the nth Fibonacci Number

         In this section, we consider the Fibonacci numbers, a famous sequence

                                  0,    1,  1,  2,  3,   5,    8,  13,  21,    34, . . .                    (2.5)

         that can be defined by the simple recurrence

                                  F (n) = F (n - 1) + F (n - 2)           for n > 1                         (2.6)

         and two initial conditions

                                            F (0) = 0,             F (1) = 1.                               (2.7)

         The Fibonacci numbers were introduced by Leonardo Fibonacci in 1202 as

         a solution to a problem about the size of a rabbit population (Problem 2 in this

         section's exercises). Many more examples of Fibonacci-like numbers have since

         been discovered in the natural world, and they have even been used in predicting

         the prices of stocks and commodities. There are some interesting applications of

         the Fibonacci numbers in computer science as well. For example, worst-case inputs

         for Euclid's algorithm discussed in Section 1.1 happen to be consecutive elements

         of the Fibonacci sequence. In this section, we briefly consider algorithms for

         computing the nth element of this sequence. Among other benefits, the discussion

         will provide us with an opportunity to introduce another method for solving

         recurrence relations useful for analysis of recursive algorithms.

         To start, let us get an explicit formula for F (n). If we try to apply the method

         of backward substitutions to solve recurrence (2.6), we will fail to get an easily

         discernible pattern. Instead, we can take advantage of a theorem that describes

         solutions to a homogeneous second-order linear recurrence with constant co-

         efficients

                                        ax(n) + bx(n - 1) + cx(n - 2) = 0,                                  (2.8)

         where a, b, and c are some fixed real numbers (a = 0) called the coefficients of

         the recurrence and x(n) is the generic term of an unknown sequence to be found.

         Applying this theorem to our recurrence with the initial conditions given--see

         Appendix B--we obtain the formula

                                                F (n) =  1  5  (n - ^n),                                    (2.9)

         where       =   (1  +    5)/2    1.61803   and  ^  =  -1/      -0.61803.6        It  is  hard  to  believe

         that formula (2.9), which includes arbitrary integer powers of irrational numbers,

         yields nothing else but all the elements of Fibonacci sequence (2.5), but it does!

         One of the benefits of formula (2.9) is that it immediately implies that F (n)

         grows exponentially (remember Fibonacci's rabbits?), i.e., F (n)                               (n). This

    6.   Constant  is known as the golden ratio. Since antiquity, it has been considered the most pleasing ratio

         of a rectangle's two sides to the human eye and might have been consciously used by ancient architects

         and sculptors.
            2.5  Example: Computing the nth Fibonacci Number                               81

follows from the observation that ^ is a fraction between -1 and 0, and hence

^n gets infinitely small as n goes to infinity. In fact, one can prove that the impact

of the second term 1 ^n on the value of F (n) can be obtained by rounding off the
                         5

value of the first term to the nearest integer. In other words, for every nonnegative

integer n,

                 F (n) =    1   n  rounded to the nearest integer.               (2.10)

                             5

       In the algorithms that follow, we consider, for the sake of simplicity, such oper-

ations as additions and multiplications at unit cost. Since the Fibonacci numbers

grow infinitely large (and grow very rapidly), a more detailed analysis than the

one offered here is warranted. In fact, it is the size of the numbers rather than a

time-efficient method for computing them that should be of primary concern here.

Still, these caveats notwithstanding, the algorithms we outline and their analysis

provide useful examples for a student of the design and analysis of algorithms.

       To begin with, we can use recurrence (2.6) and initial conditions (2.7) for the

obvious recursive algorithm for computing F (n).

ALGORITHM        F (n)

       //Computes the nth Fibonacci number recursively by using its definition

       //Input: A nonnegative integer n

       //Output: The nth Fibonacci number

       if n  1 return n

       else return F (n - 1) + F (n - 2)

       Before embarking on its formal analysis, can you tell whether this is an effi-

cient algorithm? Well, we need to do a formal analysis anyway. The algorithm's ba-

sic operation is clearly addition, so let A(n) be the number of additions performed

by the algorithm in computing F (n). Then the numbers of additions needed for

computing F (n - 1) and F (n - 2) are A(n - 1) and A(n - 2), respectively, and

the algorithm needs one more addition to compute their sum. Thus, we get the

following recurrence for A(n):

                 A(n) = A(n - 1) + A(n - 2) + 1   for n > 1,                     (2.11)

                 A(0) = 0,         A(1) = 0.

The recurrence A(n) - A(n - 1) - A(n - 2) = 1 is quite similar to recurrence

F (n) - F (n - 1) - F (n - 2) = 0, but its right-hand side is not equal to zero. Such

recurrences are called inhomogeneous. There are general techniques for solving

inhomogeneous recurrences (see Appendix B or any textbook on discrete mathe-

matics), but for this particular recurrence, a special trick leads to a faster solution.

We can reduce our inhomogeneous recurrence to a homogeneous one by rewriting

it as

                 [A(n) + 1] - [A(n - 1) + 1] - [A(n - 2) + 1] = 0

and substituting B(n) = A(n) + 1:
82  Fundamentals of the Analysis of Algorithm Efficiency

                                    B(n) - B(n - 1) - B(n - 2) = 0,

                                    B(0) = 1,            B(1) = 1.

    This homogeneous recurrence can be solved exactly in the same manner as recur-

    rence (2.6) was solved to find an explicit formula for F (n). But it can actually be

    avoided by noting that B(n) is, in fact, the same recurrence as F (n) except that it

    starts with two 1's and thus runs one step ahead of F (n). So B(n) = F (n + 1), and

                    A(n) = B(n) - 1 = F (n + 1) - 1 =           1   5  (n+1 - ^n+1) - 1.

    Hence, A(n)       (n), and if we measure the size of n by the number of bits

    b=  log2 n      + 1 in its binary representation, the efficiency class will be even worse,

    namely, doubly exponential: A(b)              (2b).

        The poor efficiency class of the algorithm could be anticipated by the nature of

    recurrence (2.11). Indeed, it contains two recursive calls with the sizes of smaller

    instances only slightly smaller than size n. (Have you encountered such a situation

    before?) We can also see the reason behind the algorithm's inefficiency by looking

    at a recursive tree of calls tracing the algorithm's execution. An example of such

    a tree for n = 5 is given in Figure 2.6. Note that the same values of the function

    are being evaluated here again and again, which is clearly extremely inefficient.

        We can obtain a much faster algorithm by simply computing the successive

    elements of the Fibonacci sequence iteratively, as is done in the following algo-

    rithm.

    ALGORITHM         Fib(n)

        //Computes the nth Fibonacci number iteratively by using its definition

        //Input: A nonnegative integer n

        //Output: The nth Fibonacci number

        F [0]  0; F [1]  1

        for i  2 to n do

               F [i]  F [i - 1] + F [i - 2]

        return F [n]

                                                                F (5)

                                    F (4)                                                F (3)

                      F (3)                       F (2)                    F (2)                 F (1)

               F (2)         F (1)         F (1)         F (0)      F (1)         F (0)

        F (1)         F (0)

    FIGURE     2.6  Tree of recursive calls for computing           the  5th  Fibonacci  number  by the

                    definition-based algorithm.
               2.5  Example: Computing the nth Fibonacci Number                          83

This algorithm clearly makes n - 1 additions. Hence, it is linear as a function

of n and "only" exponential as a function of the number of bits b in n's binary

representation. Note that using an extra array for storing all the preceding ele-

ments of the Fibonacci sequence can be avoided: storing just two values is neces-

sary to accomplish the task (see Problem 8 in this section's exercises).

    The third alternative for computing the nth Fibonacci number lies in using

formula (2.10). The efficiency of the algorithm will obviously be determined by

the efficiency of an exponentiation algorithm used for computing n. If it is done

by simply multiplying  by itself n - 1 times, the algorithm will be in    (n) =    (2b).

There are faster algorithms for the exponentiation problem. For example, we

will discuss   (log n) =     (b) algorithms for this problem in Chapters 4 and 6.

Note also that special care should be exercised in implementing this approach

to computing the nth Fibonacci number. Since all its intermediate results are

irrational numbers, we would have to make sure that their approximations in the

computer are accurate enough so that the final round-off yields a correct result.

    Finally, there exists a     (log n) algorithm for computing the nth Fibonacci

number that manipulates only integers. It is based on the equality

                     F (n - 1)   F (n)           0  1  n

                     F (n)      F (n + 1)  =     1  1        for n  1

and an efficient way of computing matrix powers.

Exercises 2.5

1.  Find a Web site dedicated to applications of the Fibonacci numbers and

    study it.

2.  Fibonacci's rabbits problem  A man put a pair of rabbits in a place sur-

    rounded by a wall. How many pairs of rabbits will be there in a year if the

    initial pair of rabbits (male and female) are newborn and all rabbit pairs are

    not fertile during their first month of life but thereafter give birth to one new

    male/female pair at the end of every month?

3.  Climbing stairs  Find the number of different ways to climb an n-stair stair-

    case if each step is either one or two stairs. For example, a 3-stair staircase can

    be climbed three ways: 1-1-1, 1-2, and 2-1.

4.  How many even numbers are there among the first n Fibonacci numbers, i.e.,

    among the numbers F (0), F (1), . . . , F (n - 1)? Give a closed-form formula

    valid for every n > 0.

5.  Check by direct substitutions that the function 1 (n - ^n) indeed satisfies
                                                          5

    recurrence (2.6) and initial conditions (2.7).

6.  The maximum values of the Java primitive types int and long are 231 - 1 and

    263 - 1, respectively. Find the smallest n for which the nth Fibonacci number

    is not going to fit in a memory allocated for
84       Fundamentals of the Analysis of Algorithm Efficiency

              a. the type int.           b.  the type long.

         7.   Consider the recursive definition-based algorithm for computing the nth Fi-

              bonacci number F (n). Let C(n) and Z(n) be the number of times F (1) and

              F (0) are computed, respectively. Prove that

              a. C(n) = F (n).           b. Z(n) = F (n - 1).

         8.   Improve algorithm F ib of the text so that it requires only  (1) space.

         9.   Prove the equality

                           F (n - 1)         F (n)             0  1  n

                                  F (n)      F (n + 1)  =      1  1     for n  1.

         10.  How many modulo divisions are made by Euclid's algorithm on two consec-

              utive Fibonacci numbers F (n) and F (n - 1) as the algorithm's input?

         11.  Dissecting a Fibonacci rectangle  Given a rectangle whose sides are two con-

              secutive Fibonacci numbers, design an algorithm to dissect it into squares with

              no more than two squares being the same size. What is the time efficiency class

              of your algorithm?

         12.  In the language of your choice, implement two algorithms for computing the

              last five digits of the nth Fibonacci number that are based on (a) the recursive

              definition-based algorithm F(n); (b) the iterative definition-based algorithm

              Fib(n). Perform an experiment to find the largest value of n for which your

              programs run under 1 minute on your computer.

    2.6  Empirical Analysis of Algorithms

         In Sections 2.3 and 2.4, we saw how algorithms, both nonrecursive and recursive,

         can be analyzed mathematically. Though these techniques can be applied success-

         fully to many simple algorithms, the power of mathematics, even when enhanced

         with more advanced techniques (see [Sed96], [Pur04], [Gra94], and [Gre07]), is

         far from limitless. In fact, even some seemingly simple algorithms have proved

         to be very difficult to analyze with mathematical precision and certainty. As we

         pointed out in Section 2.1, this is especially true for the average-case analysis.

              The principal alternative to the mathematical analysis of an algorithm's ef-

         ficiency is its empirical analysis. This approach implies steps spelled out in the

         following plan.

         General Plan for the Empirical Analysis of Algorithm Time Efficiency

         1.   Understand the experiment's purpose.

         2.   Decide on the efficiency metric M to be measured and the measurement unit

              (an operation count vs. a time unit).

         3.   Decide on characteristics of the input sample (its range, size, and so on).

         4.   Prepare a program implementing the algorithm (or algorithms) for the exper-

              imentation.
                   2.6              Empirical Analysis of Algorithms                                     85

    5.  Generate a sample of inputs.

    6.  Run the algorithm (or algorithms) on the sample's inputs and record the data

        observed.

    7.  Analyze the data obtained.

        Let us discuss these steps one at a time. There are several different goals

    one can pursue in analyzing algorithms empirically. They include checking the

    accuracy of a theoretical assertion about the algorithm's efficiency, comparing the

    efficiency of several algorithms for solving the same problem or different imple-

    mentations of the same algorithm, developing a hypothesis about the algorithm's

    efficiency class, and ascertaining the efficiency of the program implementing the

    algorithm on a particular machine. Obviously, an experiment's design should de-

    pend on the question the experimenter seeks to answer.

        In particular, the goal of the experiment should influence, if not dictate, how

    the algorithm's efficiency is to be measured. The first alternative is to insert a

    counter (or counters) into a program implementing the algorithm to count the

    number of times the algorithm's basic operation is executed. This is usually a

    straightforward operation; you should only be mindful of the possibility that

    the basic operation is executed in several places in the program and that all its

    executions need to be accounted for. As straightforward as this task usually is,

    you should always test the modified program to ensure that it works correctly, in

    terms of both the problem it solves and the counts it yields.

        The second alternative is to time the program implementing the algorithm in

    question. The easiest way to do this is to use a system's command, such as the time

    command in UNIX. Alternatively, one can measure the running time of a code

    fragment by asking for the system time right before the fragment's start (tstart) and

    just after its completion (tfinish), and then computing the difference between the

    two (tfinish- tstart).7 In C and C++, you can use the function clock for this purpose;

    in Java, the method currentTimeMillis() in the System class is available.

        It is important to keep several facts in mind, however. First, a system's time

    is typically not very accurate, and you might get somewhat different results on

    repeated runs of the same program on the same inputs. An obvious remedy is

    to make several such measurements and then take their average (or the median)

    as the sample's observation point. Second, given the high speed of modern com-

    puters, the running time may fail to register at all and be reported as zero. The

    standard trick to overcome this obstacle is to run the program in an extra loop

    many times, measure the total running time, and then divide it by the number of

    the loop's repetitions. Third, on a computer running under a time-sharing system

    such as UNIX, the reported time may include the time spent by the CPU on other

    programs, which obviously defeats the purpose of the experiment. Therefore, you

    should take care to ask the system for the time devoted specifically to execution of

7.  If the system time is given in units called "ticks," the difference should be divided by a constant

    indicating the number of ticks per time unit.
86  Fundamentals of the Analysis of Algorithm Efficiency

    your program. (In UNIX, this time is called the "user time," and it is automatically

    provided by the time command.)

    Thus, measuring the physical running time has several disadvantages, both

    principal (dependence on a particular machine being the most important of them)

    and technical, not shared by counting the executions of a basic operation. On the

    other hand, the physical running time provides very specific information about

    an algorithm's performance in a particular computing environment, which can

    be of more importance to the experimenter than, say, the algorithm's asymptotic

    efficiency class. In addition, measuring time spent on different segments of a

    program can pinpoint a bottleneck in the program's performance that can be

    missed by an abstract deliberation about the algorithm's basic operation. Getting

    such data--called profiling--is an important resource in the empirical analysis of

    an algorithm's running time; the data in question can usually be obtained from

    the system tools available in most computing environments.

    Whether you decide to measure the efficiency by basic operation counting or

    by time clocking, you will need to decide on a sample of inputs for the experiment.

    Often, the goal is to use a sample representing a "typical" input; so the challenge

    is to understand what a "typical" input is. For some classes of algorithms--e.g., for

    algorithms for the traveling salesman problem that we are going to discuss later in

    the book--researchers have developed a set of instances they use for benchmark-

    ing. But much more often than not, an input sample has to be developed by the

    experimenter. Typically, you will have to make decisions about the sample size (it

    is sensible to start with a relatively small sample and increase it later if necessary),

    the range of instance sizes (typically neither trivially small nor excessively large),

    and a procedure for generating instances in the range chosen. The instance sizes

    can either adhere to some pattern (e.g., 1000, 2000, 3000, . . . , 10,000 or 500, 1000,

    2000, 4000, . . . , 128,000) or be generated randomly within the range chosen.

    The principal advantage of size changing according to a pattern is that its

    impact is easier to analyze. For example, if a sample's sizes are generated by

    doubling, you can compute the ratios M(2n)/M(n) of the observed metric M

    (the count or the time) to see whether the ratios exhibit a behavior typical of

    algorithms in one of the basic efficiency classes discussed in Section 2.2. The

    major disadvantage of nonrandom sizes is the possibility that the algorithm under

    investigation exhibits atypical behavior on the sample chosen. For example, if all

    the sizes in a sample are even and your algorithm runs much more slowly on odd-

    size inputs, the empirical results will be quite misleading.

    Another  important  issue  concerning        sizes    in  an  experiment's  sample        is

    whether several instances of the same size should be included. If you expect the

    observed metric to vary considerably on instances of the same size, it would be

    probably wise to include several instances for every size in the sample. (There

    are well-developed methods in statistics to help the experimenter make such de-

    cisions; you will find no shortage of books on this subject.) Of course, if several

    instances of the same size are included in the sample, the averages or medians of

    the observed values for each size should be computed and investigated instead of

    or in addition to individual sample points.
                      2.6  Empirical Analysis of Algorithms                              87

Much more often than not, an empirical analysis requires generating random

numbers. Even if you decide to use a pattern for input sizes, you will typically

want instances themselves generated randomly. Generating random numbers on

a digital computer is known to present a difficult problem because, in principle,

the problem can be solved only approximately. This is the reason computer scien-

tists prefer to call such numbers pseudorandom. As a practical matter, the easiest

and most natural way of getting such numbers is to take advantage of a random

number generator available in computer language libraries. Typically, its output

will be a value of a (pseudo)random variable uniformly distributed in the interval

between 0 and 1. If a different (pseudo)random variable is desired, an appro-

priate transformation needs to be made. For example, if x is a continuous ran-

dom variable uniformly distributed on the interval 0  x < 1, the variable y = l+

x(r - l)  will be uniformly distributed among the integer values between integers

l and r - 1 (l < r).

Alternatively, you can implement one of several known algorithms for gener-

ating (pseudo)random numbers. The most widely used and thoroughly studied of

such algorithms is the linear congruential method.

ALGORITHM  Random(n, m, seed, a, b)

//Generates a sequence of n pseudorandom numbers according to the linear

//        congruential method

//Input: A positive integer n and positive integer parameters m, seed, a, b

//Output: A sequence r1, . . . , rn of n pseudorandom integers uniformly

//        distributed among integer values between 0 and m - 1

//Note: Pseudorandom numbers between 0 and 1 can be obtained

//        by treating the integers generated as digits after the decimal point

r0  seed

for i  1 to n do

          ri  (a  ri-1 + b) mod m

The simplicity of this pseudocode is misleading because the devil lies in the

details of choosing the algorithm's parameters. Here is a partial list of recommen-

dations based on the results of a sophisticated mathematical analysis (see [KnuII,

pp. 184­185] for details): seed may be chosen arbitrarily and is often set to the

current date and time; m should be large and may be conveniently taken as 2w,

where w is the computer's word size; a should be selected as an integer between

0.01m and 0.99m with no particular pattern in its digits but such that a mod 8 = 5;

and the value of b can be chosen as 1.

The empirical data obtained as the result of an experiment need to be recorded

and then presented for an analysis. Data can be presented numerically in a table or

graphically in a scatterplot, i.e., by points in a Cartesian coordinate system. It is a

good idea to use both these options whenever it is feasible because both methods

have their unique strengths and weaknesses.
88  Fundamentals of the Analysis of Algorithm Efficiency

        The principal advantage of tabulated data lies in the opportunity to manip-

    ulate it easily. For example, one can compute the ratios M(n)/g(n) where g(n) is

    a candidate to represent the efficiency class of the algorithm in question. If the

    algorithm is indeed in  (g(n)), most likely these ratios will converge to some pos-

    itive constant as n gets large. (Note that careless novices sometimes assume that

    this constant must be 1, which is, of course, incorrect according to the definition

    of  (g(n)).) Or one can compute the ratios M(2n)/M(n) and see how the running

    time reacts to doubling of its input size. As we discussed in Section 2.2, such ratios

    should change only slightly for logarithmic algorithms and most likely converge

    to 2, 4, and 8 for linear, quadratic, and cubic algorithms, respectively--to name

    the most obvious and convenient cases.

        On the other hand, the form of a scatterplot may also help in ascertaining

    the algorithm's probable efficiency class. For a logarithmic algorithm, the scat-

    terplot will have a concave shape (Figure 2.7a); this fact distinguishes it from

    all the other basic efficiency classes. For a linear algorithm, the points will tend

    to aggregate around a straight line or, more generally, to be contained between

    two straight lines (Figure 2.7b). Scatterplots of functions in  (n lg n) and            (n2)

    will have a convex shape (Figure 2.7c), making them difficult to differentiate. A

    scatterplot of a cubic algorithm will also have a convex shape, but it will show a

    much more rapid increase in the metric's values. An exponential algorithm will

    most probably require a logarithmic scale for the vertical axis, in which the val-

    ues of loga M(n) rather than those of M(n) are plotted. (The commonly used

    logarithm base is 2 or 10.) In such a coordinate system, a scatterplot of a truly

    exponential algorithm should resemble a linear function because M(n)  can im-

    plies logb M(n)  logb c + n logb a, and vice versa.

        One of the possible applications of the empirical analysis is to predict the al-

    gorithm's performance on an instance not included in the experiment sample. For

    example, if you observe that the ratios M(n)/g(n) are close to some constant c

    for the sample instances, it could be sensible to approximate M(n) by the prod-

    uct cg(n) for other instances, too. This approach should be used with caution,

    especially for values of n outside the sample range. (Mathematicians call such

    predictions extrapolation, as opposed to interpolation, which deals with values

    within the sample range.) Of course, you can try unleashing the standard tech-

    niques of statistical data analysis and prediction. Note, however, that the majority

    of such techniques are based on specific probabilistic assumptions that may or may

    not be valid for the experimental data in question.

        It seems appropriate to end this section by pointing out the basic differ-

    ences between mathematical and empirical analyses of algorithms. The princi-

    pal strength of the mathematical analysis is its independence of specific inputs;

    its principal weakness is its limited applicability, especially for investigating the

    average-case efficiency. The principal strength of the empirical analysis lies in its

    applicability to any algorithm, but its results can depend on the particular sample

    of instances and the computer used in the experiment.
                                          2.6      Empirical Analysis of Algorithms                      89

count or time                             count or time

                                       n                                                              n

               (a)                                                        (b)

               count or time

                                                         n

                                       (c)

               FIGURE 2.7  Typical scatter plots.  (a)  Logarithmic. (b)  Linear.  (c)  One  of  the  convex

                           functions.

               Exercises 2.6

               1. Consider the following well-known sorting algorithm, which is studied later

               in the book, with a counter inserted to count the number of key comparisons.

               ALGORITHM      SortAnalysis(A[0..n - 1])

                    //Input: An array A[0..n - 1] of n orderable elements

                    //Output: The total number of key comparisons made

                    count  0

                    for i  1 to n - 1 do
90  Fundamentals of the Analysis of Algorithm Efficiency

                      v  A[i]

                      j i-1

                      while j  0 and A[j ] > v do

                       count  count + 1

                       A[j + 1]  A[j ]

                       j j -1

                      A[j + 1]  v

              return count

          Is the comparison counter inserted in the right place? If you believe it is, prove

          it; if you believe it is not, make an appropriate correction.

    2.    a.  Run the program of Problem 1, with a properly inserted counter (or coun-

              ters) for the number of key comparisons, on 20 random arrays of sizes 1000,

              2000, 3000, . . . , 20,000.

          b.  Analyze the data obtained to form a hypothesis about the algorithm's

              average-case efficiency.

          c.  Estimate the number of key comparisons we should expect for a randomly

              generated array of size 25,000 sorted by the same algorithm.

    3.    Repeat Problem 2 by measuring the program's running time in milliseconds.

    4.    Hypothesize a likely efficiency class of an algorithm based on the following

          empirical observations of its basic operation's count:

    size      1000    2000     3000   4000       5000     6000        7000      8000  9000     10000

    count     11,966  24,303  39,992  53,010  67,272      78,692   91,274    113,063  129,799  140,538

    5.    What scale transformation will make a logarithmic scatterplot look like a

          linear one?

    6.    How can one distinguish a scatterplot for an algorithm in                   (lg lg n) from a

          scatterplot for an algorithm in        (lg n)?

    7.    a.  Find empirically the largest number of divisions made by Euclid's algo-

              rithm for computing gcd(m, n) for 1 n  m  100.

          b.  For each positive integer k, find empirically the smallest pair of integers

              1 n  m  100 for which Euclid's algorithm needs to make k divisions in

              order to find gcd(m, n).

    8.    The average-case efficiency of Euclid's algorithm on inputs of size n can be

          measured by the average number of divisions Davg(n) made by the algorithm

          in computing gcd(n, 1), gcd(n, 2), . . . , gcd(n, n). For example,

                               Davg(5)     =  1 (1  +  2  +  3  +  2  +  1)  =  1.8.
                                              5
                                          2.7  Algorithm Visualization                        91

          Produce a scatterplot of Davg(n) and indicate the algorithm's likely average-

          case efficiency class.

     9.   Run an experiment to ascertain the efficiency class of the sieve of Eratos-

          thenes (see Section 1.1).

     10.  Run a timing experiment for the three algorithms for computing gcd(m, n)

          presented in Section 1.1.

2.7  Algorithm Visualization

     In addition to the mathematical and empirical analyses of algorithms, there is yet

     a third way to study algorithms. It is called algorithm visualization and can be

     defined as the use of images to convey some useful information about algorithms.

     That information can be a visual illustration of an algorithm's operation, of its per-

     formance on different kinds of inputs, or of its execution speed versus that of other

     algorithms for the same problem. To accomplish this goal, an algorithm visualiza-

     tion uses graphic elements--points, line segments, two- or three-dimensional bars,

     and so on--to represent some "interesting events" in the algorithm's operation.

          There are two principal variations of algorithm visualization:

          Static algorithm visualization

          Dynamic algorithm visualization, also called algorithm animation

          Static algorithm visualization shows an algorithm's progress through a series

     of still images. Algorithm animation, on the other hand, shows a continuous,

     movie-like presentation of an algorithm's operations. Animation is an arguably

     more sophisticated option, which, of course, is much more difficult to implement.

          Early efforts in the area of algorithm visualization go back to the 1970s. The

     watershed event happened in 1981 with the appearance of a 30-minute color sound

     film titled Sorting Out Sorting. This algorithm visualization classic was produced

     at the University of Toronto by Ronald Baecker with the assistance of D. Sherman

     [Bae81, Bae98]. It contained visualizations of nine well-known sorting algorithms

     (more than half of them are discussed later in the book) and provided quite a

     convincing demonstration of their relative speeds.

          The success of Sorting Out Sorting made sorting algorithms a perennial fa-

     vorite for algorithm animation. Indeed, the sorting problem lends itself quite

     naturally to visual presentation via vertical or horizontal bars or sticks of different

     heights or lengths, which need to be rearranged according to their sizes (Figure

     2.8). This presentation is convenient, however, only for illustrating actions of a

     typical sorting algorithm on small inputs. For larger files, Sorting Out Sorting used

     the ingenious idea of presenting data by a scatterplot of points on a coordinate

     plane, with the first coordinate representing an item's position in the file and the

     second one representing the item's value; with such a representation, the process

     of sorting looks like a transformation of a "random" scatterplot of points into the

     points along a frame's diagonal (Figure 2.9). In addition, most sorting algorithms
92  Fundamentals of the Analysis of Algorithm Efficiency

    FIGURE 2.8 Initial and final screens of a typical visualization of a sorting algorithm using

    the bar representation.

    work by comparing and exchanging two given items at a time--an event that can

    be animated relatively easily.

    Since the appearance of Sorting Out Sorting, a great number of algorithm

    animations have been created, especially after the appearance of Java and the
2.7                              Algorithm Visualization                                      93

FIGURE 2.9 Initial and final screens of a typical visualization of a sorting algorithm using

the scatterplot representation.

World Wide Web in the 1990s. They range in scope from one particular algorithm

to a group of algorithms for the same problem (e.g., sorting) or the same applica-

tion area (e.g., geometric algorithms) to general-purpose animation systems. At

the end of 2010, a catalog of links to existing visualizations, maintained under the
94  Fundamentals of the Analysis of Algorithm Efficiency

    NSF-supported AlgoVizProject, contained over 500 links. Unfortunately, a survey

    of existing visualizations found most of them to be of low quality, with the content

    heavily skewed toward easier topics such as sorting [Sha07].

    There are two principal applications of algorithm visualization: research and

    education. Potential benefits for researchers are based on expectations that algo-

    rithm visualization may help uncover some unknown features of algorithms. For

    example, one researcher used a visualization of the recursive Tower of Hanoi algo-

    rithm in which odd- and even-numbered disks were colored in two different colors.

    He noticed that two disks of the same color never came in direct contact during

    the algorithm's execution. This observation helped him in developing a better non-

    recursive version of the classic algorithm. To give another example, Bentley and

    McIlroy [Ben93] mentioned using an algorithm animation system in their work

    on improving a library implementation of a leading sorting algorithm.

    The application of algorithm visualization to education seeks to help students

    learning algorithms. The available evidence of its effectiveness is decisively mixed.

    Although some experiments did register positive learning outcomes, others failed

    to do so. The increasing body of evidence indicates that creating sophisticated

    software systems is not going to be enough. In fact, it appears that the level of

    student involvement with visualization might be more important than specific

    features of visualization software. In some experiments, low-tech visualizations

    prepared by students were more effective than passive exposure to sophisticated

    software systems.

    To summarize, although some successes in both research and education have

    been reported in the literature, they are not as impressive as one might expect. A

    deeper understanding of human perception of images will be required before the

    true potential of algorithm visualization is fulfilled.

    SUMMARY

    There are two kinds of algorithm efficiency: time efficiency and space

    efficiency. Time efficiency indicates how fast the algorithm runs; space

    efficiency deals with the extra space it requires.

    An algorithm's time efficiency is principally measured as a function of its input

    size by counting the number of times its basic operation is executed. A basic

    operation is the operation that contributes the most to running time. Typically,

    it is the most time-consuming operation in the algorithm's innermost loop.

    For some algorithms, the running time can differ considerably for inputs of

    the same size, leading to worst-case efficiency, average-case efficiency, and

    best-case efficiency.

    The established framework for analyzing time efficiency is primarily grounded

    in the order of growth of the algorithm's running time as its input size goes to

    infinity.
                                          Summary                                95

The notations O,  , and  are used to indicate and compare the asymptotic

orders of growth of functions expressing algorithm efficienci