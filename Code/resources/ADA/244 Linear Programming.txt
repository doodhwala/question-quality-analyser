Many problems of optimal decision making can be reduced to an instance of
     the linear programming problem--a problem of optimizing a linear function of
     several variables subject to constraints in the form of linear equations and linear
     inequalities.
     EXAMPLE 1      Consider a university endowment that needs to invest $100 million.
     This sum has to be split between three types of investments: stocks, bonds, and
     cash. The endowment managers expect an annual return of 10%, 7%, and 3% for
     their stock, bond, and cash investments, respectively. Since stocks are more risky
     than bonds, the endowment rules require the amount invested in stocks to be no
     more than one-third of the moneys invested in bonds. In addition, at least 25%
     of the total amount invested in stocks and bonds must be invested in cash. How
     should the managers invest the money to maximize the return?
     Let us create a mathematical model of this problem. Let x, y, and z be the
     amounts (in millions of dollars) invested in stocks, bonds, and cash, respectively.
     By using these variables, we can pose the following optimization problem:
                            maximize    0.10x + 0.07y + 0.03z
                            subject to  x + y + z = 100
                                        x    1  y
                                             3
                                        z  0.25(x + y)
                                        x  0,      y  0,  z  0.
     Although this example is both small and simple, it does show how a problem
     of optimal decision making can be reduced to an instance of the general linear
     programming problem
maximize (or minimize)           c1x1 + . . . + cnxn
                  subject to     ai1x1 + . . . + ainxn     (or  or =) bi         for i = 1, . . . , m
                                 x1  0, . . . , xn  0.
(The last group of constraints--called the nonnegativity constraints--are, strictly
speaking, unnecessary because they are special cases of more general constraints
ai1x1 + . . . + ainxn  bi, but it is convenient to treat them separately.)
    Linear programming has proved to be flexible enough to model a wide variety
of important applications, such as airline crew scheduling, transportation and
communication network planning, oil exploration and refining, and industrial
production optimization. In fact, linear programming is considered by many as
one of the most important achievements in the history of applied mathematics.
    The classic algorithm for this problem is called the simplex method (Sec-
tion 10.1). It was discovered by the U.S. mathematician George Dantzig in the
1940s [Dan63]. Although the worst-case efficiency of this algorithm is known to
be exponential, it performs very well on typical inputs. Moreover, a more recent al-
gorithm by Narendra Karmarkar [Kar84] not only has a proven polynomial worst-
case efficiency but has also performed competitively with the simplex method in
empirical tests.
    It is important to stress, however, that the simplex method and Karmarkar's
algorithm can successfully handle only linear programming problems that do not
limit its variables to integer values. When variables of a linear programming
problem are required to be integers, the linear programming problem is said
to be an integer linear programming problem. Except for some special cases
(e.g., the assignment problem and the problems discussed in Sections 10.2­10.4),
integer linear programming problems are much more difficult. There is no known
polynomial-time algorithm for solving an arbitrary instance of the general integer
linear programming problem and, as we see in Chapter 11, such an algorithm
quite possibly does not exist. Other approaches such as the branch-and-bound
technique discussed in Section 12.2 are typically used for solving integer linear
programming problems.
EXAMPLE 2         Let us see how the knapsack problem can be reduced to a linear
programming problem. Recall from Section 3.4 that the knapsack problem can
be  posed  as  follows.  Given      a  knapsack       of   capacity  W  and  n   items  of  weights
w1, . . . , wn and values v1, . . . , vn, find the most valuable subset of the items that fits
into the knapsack. We consider first the continuous (or fractional) version of the
problem, in which any fraction of any item given can be taken into the knapsack.
Let xj , j = 1, . . . , n, be a variable representing a fraction of item j taken into
the knapsack. Obviously, xj must satisfy the inequality 0  xj  1. Then the total
weight of the selected items can be expressed by the sum                     n     wj xj ,  and  their
                              n                                              j =1
total value by the sum        j =1  vj  xj  .  Thus,  the  continuous   version    of  the  knapsack
problem can be posed as the following linear programming problem:
                                     n
     maximize                              vj xj
                                     j =1
                                     n
     subject to                            wj xj   W
                                     j =1
                                     0  xj  1      for  j  =  1,  .  .  .  ,  n.
     There is no need to apply a general method for solving linear programming
     problems here: this particular problem can be solved by a simple special algorithm
     that is introduced in Section 12.3. (But why wait? Try to discover it on your
     own now.) This reduction of the knapsack problem to an instance of the linear
     programming problem is still useful, though, to prove the correctness of the
     algorithm in question.
     In the discrete (or 0-1) version of the knapsack problem, we are only allowed
     either to take a whole item or not to take it at all. Hence, we have the following
     integer linear programming problem for this version:
                                     n
                           maximize        vj xj
                                     j =1
                                     n
     subject to                            wj xj   W
                                     j =1
                                     xj  {0, 1}    for  j  =  1,  .  .  .  ,  n.
     This seemingly minor modification makes a drastic difference for the com-
     plexity of this and similar problems constrained to take only discrete values in
     their potential ranges. Despite the fact that the 0-1 version might seem to be eas-
     ier because it can ignore any subset of the continuous version that has a fractional
     value of an item, the 0-1 version is, in fact, much more complicated than its con-
     tinuous counterpart. The reader interested in specific algorithms for solving this
     problem will find a wealth of literature on the subject, including the monographs
     [Mar90] and [Kel04].
