Geometric Interpretation of Linear Programming
Before we introduce a general method for solving linear programming problems,
    let us consider a small example, which will help us to see the fundamental prop-
    erties of such problems.
    EXAMPLE 1  Consider the following linear programming problem in two vari-
    ables:
                              maximize    3x + 5y
                              subject to  x+       y4                                 (10.2)
                                          x + 3y  6
                                          x  0,    y  0.
    By definition, a feasible solution to this problem is any point (x, y) that satisfies
    all the constraints of the problem; the problem's feasible region is the set of all
    its feasible points. It is instructive to sketch the feasible region in the Cartesian
    plane. Recall that any equation ax + by = c, where coefficients a and b are not
    both equal to zero, defines a straight line. Such a line divides the plane into two
    half-planes: for all the points in one of them, ax + by < c, while for all the points
    in the other, ax + by > c. (It is easy to determine which of the two half-planes
    is which: take any point (x0, y0) not on the line ax + by = c and check which of
    the two inequalities hold, ax0 + by0 > c or ax0 + by0 < c.) In particular, the set of
    points defined by inequality x + y  4 comprises the points on and below the line
    x + y = 4, and the set of points defined by inequality x + 3y  6 comprises the
    points on and below the line x + 3y = 6. Since the points of the feasible region
    must satisfy all the constraints of the problem, the feasible region is obtained by
    the intersection of these two half-planes and the first quadrant of the Cartesian
    plane defined by the nonnegativity constraints x  0, y  0 (see Figure 10.1). Thus,
    the feasible region for problem (10.2) is the convex polygon with the vertices (0, 0),
    (4, 0), (0, 2), and (3, 1). (The last point, which is the point of intersection of the
    lines x + y = 4 and x + 3y = 6, is obtained by solving the system of these two linear
    equations.) Our task is to find an optimal solution, a point in the feasible region
    with the largest value of the objective function z = 3x + 5y.
    Are there feasible solutions for which the value of the objective function
    equals, say, 20? The points (x, y) for which the objective function z = 3x + 5y is
    equal to 20 form the line 3x + 5y = 20. Since this line does not have common points
1.  George B. Dantzig (1914­2005) has received many honors, including the National Medal of Science
    presented by the president of the United States in 1976. The citation states that the National Medal was
    awarded "for inventing linear programming and discovering methods that led to wide-scale scientific
    and technical applications to important problems in logistics, scheduling, and network optimization,
    and to the use of computers in making efficient use of the mathematical theory."
                     y
     x+      3y = 6
             (0, 2)
                                        (3, 1)
             (0, 0)                                (4, 0)                           x
                                                           x+y=4
     FIGURE  10.1 Feasible  region  of  problem (10.2).
     with the feasible region--see Figure 10.2--the answer to the posed question is no.
     On the other hand, there are infinitely many feasible points for which the objective
     function is equal to, say, 10: they are the intersection points of the line 3x + 5y = 10
     with the feasible region. Note that the lines 3x + 5y = 20 and 3x + 5y = 10 have
     the same slope, as would any line defined by equation 3x + 5y = z where z is
     some constant. Such lines are called level lines of the objective function. Thus,
     our problem can be restated as finding the largest value of the parameter z for
     which the level line 3x + 5y = z has a common point with the feasible region.
     We can find this line either by shifting, say, the line 3x + 5y = 20 south-west
     (without changing its slope!) toward the feasible region until it hits the region for
     the first time or by shifting, say, the line 3x + 5y = 10 north-east until it hits the
     feasible region for the last time. Either way, it will happen at the point (3, 1) with
     the corresponding z value 3 . 3 + 5 . 1 = 14. This means that the optimal solution
     to the linear programming problem in question is x = 3, y = 1, with the maximal
     value of the objective function equal to 14.
     Note that if we had to maximize z = 3x + 3y as the objective function in
     problem (10.2), the level line 3x + 3y = z for the largest value of z would coincide
     with the boundary line segment that has the same slope as the level lines (draw
     this line in Figure 10.2). Consequently, all the points of the line segment between
     vertices (3, 1) and (4, 0), including the vertices themselves, would be optimal
     solutions, yielding, of course, the same maximal value of the objective function.
        y
        (0, 2)
                                   (3, 1)
        (0, 0)                             (4, 0)                             x
                                                                3x + 5y = 20
                                                                3x + 5y = 14
                                                                3x + 5y = 10
FIGURE  10.2    Solving  a  two-dimensional linear programming  problem geometrically.
Does every linear programming problem have an optimal solution that can
be found at a vertex of its feasible region? Without appropriate qualifications,
the answer to this question is no. To begin with, the feasible region of a linear
programming problem can be empty. For example, if the constraints include two
contradictory requirements, such as x + y  1 and x + y  2, there can be no points
in the problem's feasible region.  Linear programming problems with the empty
feasible region are called infeasible. Obviously, infeasible problems do not have
optimal solutions.
Another complication may arise if the problem's feasible region is unbounded,
as the following example demonstrates.
EXAMPLE 2       If we reverse the inequalities in problem (10.2) to x + y  4 and
x + 3y  6, the feasible region of the new problem will become unbounded (see
Figure 10.3). If the feasible region of a linear programming problem is unbounded,
its objective function may or may not attain a finite optimal value on it. For
example, the problem of maximizing z = 3x + 5y subject to the constraints x + y 
4, x + 3y  6, x  0, y  0 has no optimal solution, because there are points in
the feasible region making 3x + 5y as large as we wish. Such problems are called
unbounded. On the other hand, the problem of minimizing z = 3x + 5y subject to
the same constraints has an optimal solution (which?).
         y
         (0, 4)
                                 (3, 1)
         (0, 0)                                         (6, 0)                                                  x
                                                                                3x + 5y = 24
                                                                       3x       + 5y = 20
                                                                3x  +  5y = 14
         FIGURE 10.3  Unbounded feasible region of a    linear programming problem with
                      constraints x + y  4, x + 3y  6,  x  0, y  0, and three level lines of
                      the function 3x + 5y.
         Fortunately, the most important features of the examples we considered above
         hold for problems with more than two variables. In particular, a feasible region of
         a typical linear programming problem is in many ways similar to convex polygons
         in the two-dimensional Cartesian plane. Specifically, it always has a finite number
         of vertices, which mathematicians prefer to call extreme points (see Section 3.3).
         Furthermore, an optimal solution to a linear programming problem can be found
         at one of the extreme points of its feasible region. We reiterate these properties
         in the following theorem.
         THEOREM (Extreme Point Theorem)              Any linear programming problem with
         a nonempty bounded feasible region has an optimal solution; moreover, an op-
         timal solution can always be found at an extreme point of the problem's feasible
         region.2
         This theorem implies that to solve a linear programming problem, at least
         in the case of a bounded feasible region, we can ignore all but a finite number of
     2.  Except for some degenerate instances (such as maximizing z = x + y subject to x + y = 1), if a linear
         programming problem with an unbounded feasible region has an optimal solution, it can also be found
         at an extreme point of the feasible region.
points in its feasible region. In principle, we can solve such a problem by computing
the value of the objective function at each extreme point and selecting the one with
the best value. There are two major obstacles to implementing this plan, however.
The first lies in the need for a mechanism for generating the extreme points of the
feasible region. As we are going to see below, a rather straightforward algebraic
procedure for this task has been discovered. The second obstacle lies in the number
of extreme points a typical feasible region has. Here, the news is bad: the number
of extreme points is known to grow exponentially with the size of the problem.
This makes the exhaustive inspection of extreme points unrealistic for most linear
programming problems of nontrivial sizes.
Fortunately, it turns out that there exists an algorithm that typically inspects
only a small fraction of the extreme points of the feasible region before reaching an
optimal one. This famous algorithm is called the simplex method. The idea of this
algorithm can be described in geometric terms as follows. Start by identifying an
extreme point of the feasible region. Then check whether one can get an improved
value of the objective function by going to an adjacent extreme point. If it is not the
case, the current point is optimal--stop; if it is the case, proceed to an adjacent
extreme point with an improved value of the objective function. After a finite
number of steps, the algorithm will either reach an extreme point where an optimal
solution occurs or determine that no optimal solution exists.
