Units for Measuring Running Time
The next issue concerns units for measuring an algorithm's running time. Of
        course, we can simply use some standard unit of time measurement--a second,
        or millisecond, and so on--to measure the running time of a program implement-
        ing the algorithm. There are obvious drawbacks to such an approach, however:
        dependence on the speed of a particular computer, dependence on the quality of
        a program implementing the algorithm and of the compiler used in generating the
        machine code, and the difficulty of clocking the actual running time of the pro-
        gram. Since we are after a measure of an algorithm's efficiency, we would like to
        have a metric that does not depend on these extraneous factors.
        One possible approach is to count the number of times each of the algorithm's
        operations is executed. This approach is both excessively difficult and, as we
        shall see, usually unnecessary. The thing to do is to identify the most important
        operation of the algorithm, called the basic operation, the operation contributing
        the most to the total running time, and compute the number of times the basic
        operation is executed.
        As a rule, it is not difficult to identify the basic operation of an algorithm: it
        is usually the most time-consuming operation in the algorithm's innermost loop.
        For example, most sorting algorithms work by comparing elements (keys) of a
        list being sorted with each other; for such algorithms, the basic operation is a key
        comparison. As another example, algorithms for mathematical problems typically
        involve some or all of the four arithmetical operations: addition, subtraction,
        multiplication, and division. Of the four, the most time-consuming operation is
        division, followed by multiplication and then addition and subtraction, with the
        last two usually considered together.2
        Thus, the established framework for the analysis of an algorithm's time ef-
        ficiency suggests measuring it by counting the number of times the algorithm's
        basic operation is executed on inputs of size n. We will find out how to compute
        such a count for nonrecursive and recursive algorithms in Sections 2.3 and 2.4,
        respectively.
        Here is an important application. Let cop be the execution time of an algo-
        rithm's basic operation on a particular computer, and let C(n) be the number of
        times this operation needs to be executed for this algorithm. Then we can estimate
    2.  On some computers, multiplication does not take longer than addition/subtraction (see, for example,
        the timing data provided by Kernighan and Pike in [Ker99, pp. 185­186]).
the running time T (n) of a program implementing this algorithm on that computer
by the formula
                                            T (n)  copC(n).
Of course, this formula should be used with caution. The count C(n) does not
contain any information about operations that are not basic, and, in fact, the
count itself is often computed only approximately. Further, the constant cop is
also an approximation whose reliability is not always easy to assess. Still, unless
n is extremely large or very small, the formula can give a reasonable estimate of
the algorithm's running time. It also makes it possible to answer such questions as
"How much faster would this algorithm run on a machine that is 10 times faster
than the one we have?" The answer is, obviously, 10 times. Or, assuming that
C(n)   =  1  n(n  -  1),  how  much     longer   will  the  algorithm  run     if  we  double  its  input
          2
size? The answer is about four times longer. Indeed, for all but very small values
of n,
                            C(n) = 1 n(n - 1) = 1 n2 - 1 n  1 n2
                                         2             2          2    2
and therefore
                                 T (2n)     copC(2n)        1  (2n)2
                                                            2          = 4.
                                 T (n)      copC(n)            1  n2
                                                               2
     Note that we were able to answer the last question without actually knowing
the  value   of   cop:  it  was  neatly     cancelled  out  in    the  ratio.  Also    note  that   1  ,  the
                                                                                                    2
multiplicative constant in the formula for the count C(n), was also cancelled out.
It is for these reasons that the efficiency analysis framework ignores multiplicative
constants and concentrates on the count's order of growth to within a constant
multiple for large-size inputs.
