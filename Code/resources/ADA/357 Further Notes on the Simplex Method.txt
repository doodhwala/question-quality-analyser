Formal proofs of validity of the simplex method steps can be found in books
devoted to a detailed discussion of linear programming (e.g., [Dan63]). A few
important remarks about the method still need to be made, however. Generally
speaking, an iteration of the simplex method leads to an extreme point of the prob-
lem's feasible region with a greater value of the objective function. In degenerate
cases, which arise when one or more basic variables are equal to zero, the simplex
method can only guarantee that the value of the objective function at the new
extreme point is greater than or equal to its value at the previous point. In turn,
this opens the door to the possibility not only that the objective function's values
"stall" for several iterations in a row but that the algorithm might cycle back to a
previously considered point and hence never terminate. The latter phenomenon
is called cycling. Although it rarely if ever happens in practice, specific examples
of problems where cycling does occur have been constructed. A simple modifica-
tion of Steps 2 and 3 of the simplex method, called Bland's rule, eliminates even
the theoretical possibility of cycling. Assuming that the variables are denoted by
a subscripted letter (e.g., x1, x2, . . . , xn), this rule can be stated as follows:
Step 2 modified Among the columns with a negative entry in the objective
        row, select the column with the smallest subscript.
Step 3 modified Resolve a tie among the smallest -ratios by selecting the
        row labeled by the basic variable with the smallest subscript.
     Another caveat deals with the assumptions made in Step 0. They are automat-
     ically satisfied if a problem is given in the form where all the constraints imposed
     on nonnegative variables are inequalities ai1x1 + . . . + ainxn  bi with bi  0 for
     i = 1, 2, . . . , m. Indeed, by adding a nonnegative slack variable xn+i into the ith
     constraint, we obtain the equality ai1x1 + . . . + ainxn + xn+i = bi, and all the re-
     quirements imposed on an initial tableau of the simplex method are satisfied for
     the obvious basic feasible solution x1 = . . . = xn = 0, xn+1 = . . . = xn+m = 1. But
     if a problem is not given in such a form, finding an initial basic feasible solution
     may present a nontrivial obstacle. Moreover, for problems with an empty feasible
     region, no initial basic feasible solution exists, and we need an algorithmic way to
     identify such problems. One of the ways to address these issues is to use an exten-
     sion to the classic simplex method called the two-phase simplex method (see, e.g.,
     [Kol95]). In a nutshell, this method adds a set of artificial variables to the equality
     constraints of a given problem so that the new problem has an obvious basic fea-
     sible solution. It then solves the linear programming problem of minimizing the
     sum of the artificial variables by the simplex method. The optimal solution to this
     problem either yields an initial tableau for the original problem or indicates that
     the feasible region of the original problem is empty.
     How efficient is the simplex method? Since the algorithm progresses through
     a sequence of adjacent points of a feasible region, one should probably expect bad
     news because the number of extreme points is known to grow exponentially with
     the problem size. Indeed, the worst-case efficiency of the simplex method has been
     shown to be exponential as well. Fortunately, more than half a century of practical
     experience with the algorithm has shown that the number of iterations in a typical
     application ranges between m and 3m, with the number of operations per iteration
     proportional to mn, where m and n are the numbers of equality constraints and
     variables, respectively.
     Since its discovery in 1947, the simplex method has been a subject of intensive
     study by many researchers. Some of them have worked on improvements to the
     original algorithm and details of its efficient implementation. As a result of these
     efforts, programs implementing the simplex method have been polished to the
     point that very large problems with hundreds of thousands of constraints and
     variables can be solved in a routine manner. In fact, such programs have evolved
     into sophisticated software packages. These packages enable the user to enter
     a problem's constraints and obtain a solution in a user-friendly form. They also
     provide tools for investigating important properties of the solution, such as its
     sensitivity to changes in the input data. Such investigations are very important for
     many applications, including those in economics. At the other end of the spectrum,
     linear programming problems of a moderate size can nowadays be solved on a
     desktop using a standard spreadsheet facility or by taking advantage of specialized
     software available on the Internet.
     Researchers have also tried to find algorithms for solving linear programming
     problems with polynomial-time efficiency in the worst case. An important mile-
     stone in the history of such algorithms was the proof by L. G. Khachian [Kha79]
     showing that the ellipsoid method can solve any linear programming problem in
polynomial time. Although the ellipsoid method was much slower than the simplex
method in practice, its better worst-case efficiency encouraged a search for alterna-
tives to the simplex method. In 1984, Narendra Karmarkar published an algorithm
that not only had a polynomial worst-case efficiency but also was competitive with
the simplex method in empirical tests as well. Although we are not going to discuss
Karmarkar's algorithm [Kar84] here, it is worth pointing out that it is also based
on the iterative-improvement idea. However, Karmarkar's algorithm generates a
sequence of feasible solutions that lie within the feasible region rather than going
through a sequence of adjacent extreme points as the simplex method does. Such
algorithms are called interior-point methods (see, e.g., [Arb93]).
Exercises 10.1
1.  Consider the following version of the post office location problem (Problem
    3 in Exercises 3.3): Given n integers x1, x2, . . . , xn representing coordinates
    of n villages located along a straight road, find a location for a post office that
    minimizes the average distance between the villages. The post office may be,
    but is not required to be, located at one of the villages. Devise an iterative-
    improvement algorithm for this problem. Is this an efficient way to solve this
    problem?
2.  Solve the following linear programming problems geometrically.
    a.          maximize                3x + y
                subject to -x + y  1
                                        2x + y  4
                                        x  0, y     0
    b.          maximize                x + 2y
                subject to 4x  y
                                        y 3+x
                                        x  0, y  0
3.  Consider the linear programming problem
                minimize                c1x  + c2y
                subject to              x    +  y4
                                        x    + 3y  6
                                        x     0, y  0
    where c1 and c2 are some real numbers not both equal to zero.
    a.  Give an example of the coefficient values c1 and c2 for which the problem
        has a unique optimal solution.
          b.  Give an example of the coefficient values c1 and c2 for which the problem
              has infinitely many optimal solutions.
          c.  Give an example of the coefficient values c1 and c2 for which the problem
              does not have an optimal solution.
     4.   Would the solution to problem (10.2) be different if its inequality constraints
          were strict, i.e., x + y < 4 and x + 3y < 6, respectively?
     5.   Trace the simplex method on
          a.  the problem of Exercise 2a.
          b. the problem of Exercise 2b.
     6.   Trace the simplex method on the problem of Example 1 in Section 6.6
          a.  by hand.
          b. by using one of the implementations available on the Internet.
     7.   Determine how many iterations the simplex method needs to solve the
          problem
                                   n
                   maximize           xj
                               j =1
                   subject to  0  xj  bj ,             where bj > 0 for j = 1, 2, . . . , n.
     8.   Can we apply the simplex method to solve the knapsack problem (see Exam-
          ple 2 in Section 6.6)? If you answer yes, indicate whether it is a good algorithm
          for the problem in question; if you answer no, explain why not.
     9.   Prove that no linear programming problem can have exactly k  1 optimal
          solutions unless k = 1.
     10.  If a linear programming problem
                                          n
                        maximize                cj xj
                                          j =1
                                          n
                        subject to              aij xj  bi    for i = 1, 2, . . . , m
                                          j =1
                                          x1, x2, . . . , xn  0
          is considered as primal, then its dual is defined as the linear programming
          problem
                                          m
                            minimize            bi yi
                                          i=1
                                          m
                        subject to              aij yi  cj    for  j  =  1,  2,  .  .  .  ,  n
                                          i=1
                                          y1, y2, . . . , ym   0.
      a.  Express the primal and dual problems in matrix notations.
      b.  Find the dual of the linear programming problem
                                              maximize          x1 + 4x2 -  x3
                                              subject to        x1 +  x2 +  x3  6
                                                                x1 -  x2 - 2x3  2
                                                                x1, x2, x3  0.
      c.  Solve the primal and dual problems and compare the optimal                          values of
          their objective functions.
