While the approach outlined above takes into account the size of a problem's
     output, the information-theoretical approach seeks to establish a lower bound
     based on the amount of information it has to produce. Consider, as an example,
     the well-known game of deducing a positive integer between 1 and n selected
     by somebody by asking that person questions with yes/no answers. The amount of
     uncertainty that any algorithm solving this problem has to resolve can be measured
     by  log2 n , the number of bits needed to specify a particular number among the
     n possibilities. We can think of each question (or, to be more accurate, an answer
     to each question) as yielding at most 1 bit of information about the algorithm's
     output, i.e., the selected number. Consequently, any such algorithm will need at
     least  log2 n  such steps before it can determine its output in the worst case.
         The approach we just exploited is called the information-theoretic argument
     because of its connection to information theory. It has proved to be quite useful
     for finding the so-called information-theoretic lower bounds for many problems
     involving comparisons, including sorting and searching. Its underlying idea can be
     realized much more precisely through the mechanism of decision trees. Because
     of the importance of this technique, we discuss it separately and in more detail in
     Section 11.2.
