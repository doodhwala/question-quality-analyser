Gaussian elimination has an interesting and very useful byproduct called LU de-
         composition of the coefficient matrix. In fact, modern commercial implementa-
         tions of Gaussian elimination are based on such a decomposition rather than on
         the basic algorithm outlined above.
         EXAMPLE  Let us return to the example in the beginning of this section, where
         we applied Gaussian elimination to the matrix
     4.  As we mentioned in Section 2.1, on some computers multiplication is not necessarily more expensive
         than addition/subtraction. For this algorithm, this point is moot since we can simply count the number
         of times the innermost loop is executed, which is, of course, exactly the same number as the number
         of multiplications and the number of subtractions there.
                                        2       -1      1   
                               A=4              1   -1  .
                                         1      1       1
Consider the lower-triangular matrix L made up of 1's on its main diagonal               and
the row multiples used in the forward elimination process
                                            1   0     0  
                                  L=2           1     0
                                            1   1     1
                                            2   2
and the upper-triangular matrix U that was the result of this elimination
                                         2      -1       1  
                               U =0             3   -3  .
                                         0      0       2
It turns out that the product LU of these matrices is equal to matrix A. (For this
particular pair of L and U , you can verify this fact by direct multiplication, but as
a general proposition, it needs, of course, a proof, which we omit here.)
Therefore, solving the system Ax = b is equivalent to solving the system
LU x = b. The latter system can be solved as follows. Denote y = U x, then Ly = b.
Solve the system Ly = b first, which is easy to do because L is a lower-triangular
matrix; then solve the system U x = y, with the upper-triangular matrix U , to find
x. Thus, for the system at the beginning of this section, we first solve Ly = b:
                          1       0   0         y1             1  
                       2           1  0   y2  =  5  .
                          1        1  1         y3             0
                          2        2
Its solution is
y1 = 1,              y2 = 5 - 2y1 = 3,                   =  0  -  1     -  1     =  -2.
                                                    y3            2 y1     2 y2
Solving U x = y means solving
                       2       -1       1       x1                1  
                     0         3      -3   x2  =                  3,
                       0       0        2       x3             -2
and the solution is
x3 = (-2)/2 = -1,    x2 = (3 - (-3)x3)/3 = 0,         x1 = (1 - x3 - (-1)x2)/2 = 1.
Note that once we have the LU decomposition of matrix A, we can solve
systems Ax = b with as many right-hand side vectors b as we want to, one at a time.
This is a distinct advantage over the classic Gaussian elimination discussed earlier.
Also note that the LU decomposition does not actually require extra memory,
because we can store the nonzero part of U in the upper-triangular part of A
     (including the main diagonal) and store the nontrivial part of L below the main
     diagonal of A.
