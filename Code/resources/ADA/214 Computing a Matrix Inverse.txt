Gaussian elimination is a very useful algorithm that tackles one of the most
     important problems of applied mathematics: solving systems of linear equations.
     In fact, Gaussian elimination can also be applied to several other problems of
     linear algebra, such as computing a matrix inverse. The inverse of an n × n matrix
     A is an n × n matrix, denoted A-1, such that
                                              AA-1 = I,
     where I is the n × n identity matrix (the matrix with all zero elements except
     the main diagonal elements, which are all ones). Not every square matrix has
     an inverse, but when it exists, the inverse is unique. If a matrix A does not have
     an inverse, it is called singular. One can prove that a matrix is singular if and
     only if one of its rows is a linear combination (a sum of some multiples) of the
     other rows. A convenient way to check whether a matrix is nonsingular is to apply
     Gaussian elimination: if it yields an upper-triangular matrix with no zeros on the
     main diagonal, the matrix is nonsingular; otherwise, it is singular. So being singular
     is a very special situation, and most square matrices do have their inverses.
     Theoretically, inverse matrices are very important because they play the role
     of reciprocals in matrix algebra, overcoming the absence of the explicit division
     operation for matrices. For example, in a complete analogy with a linear equation
     in one unknown ax = b whose solution can be written as x = a-1b (if a is not
     zero), we can express a solution to a system of n equations in n unknowns Ax = b
     as x = A-1b (if A is nonsingular) where b is, of course, a vector, not a number.
     According to the definition of the inverse matrix for a nonsingular n × n
     matrix A, to compute it we need to find n2 numbers xij , 1  i, j  n, such that
       a11       a12  ...   a1n    x11        x12  ...    x1n       1      0  ...  0
      a...21     a22  ...   a2n   x...21      x22  ...    x2n    =    0    1  ...  0  .
                                                                      ...
       an1       an2  ...   ann    xn1        xn2  ...    xnn         0    0  ...  1
     We can find the unknowns by solving n systems of linear equations that have the
     same coefficient matrix A, the vector of unknowns xj is the j th column of the
     inverse, and the right-hand side vector ej is the j th column of the identity matrix
     (1  j  n):
                                              Axj = ej .
     We can solve these systems by applying Gaussian elimination to matrix A aug-
     mented by the n × n identity matrix. Better yet, we can use forward elimina-
     tion to find the LU decomposition of A and then solve the systems LU xj = ej ,
     j = 1, . . . , n, as explained earlier.
