Gaussian Elimination
You are certainly familiar with systems of two linear equations in two unknowns:
                                                 a11x + a12y = b1
                                                 a21x + a22y = b2.
          Recall that unless the coefficients of one equation are proportional to the coef-
          ficients of the other, the system has a unique solution. The standard method for
          finding this solution is to use either equation to express one of the variables as a
          function of the other and then substitute the result into the other equation, yield-
          ing a linear equation whose solution is then used to find the value of the second
          variable.
               In  many  applications,  we       need  to  solve    a  system  of  n  equations  in             n
          unknowns:
                                  a11x1 + a12x2 + . . . + a1nxn = b1
                                  a21x1 + a22x2 + . . . + a2nxn = b2
                                  ...
                                  an1x1 + an2x2 + . . . + annxn = bn
          where n is a large number. Theoretically, we can solve such a system by general-
          izing the substitution method for solving systems of two linear equations (what
          general design technique would such a method be based upon?); however, the
          resulting algorithm would be extremely cumbersome.
               Fortunately, there is a much more elegant algorithm for solving systems of
          linear equations called Gaussian elimination.2 The idea of Gaussian elimination
          is to transform a system of n linear equations in n unknowns to an equivalent
          system (i.e., a system with the same solution as the original one) with an upper-
          triangular coefficient matrix, a matrix with all zeros below its main diagonal:
     2.   The method is named after Carl Friedrich Gauss (1777­1855), who--like other giants in the history of
          mathematics such as Isaac Newton and Leonhard Euler--made numerous fundamental contributions
          to both theoretical and computational mathematics. The method was known to the Chinese 1800 years
          before the Europeans rediscovered it.
a11x1 + a12x2 + . . . + a1nxn = b1                 a11x1 + a12x2 + . . . + a1nxn = b1
a...21x1 + a22x2 + . . . + a2nxn = b2                        a22x2 + . . . + a2nxn = b...2
an1x1 + an2x2 + . . . + annxn = bn                                            annxn = bn.
In matrix notations, we can write this as
                             Ax = b                Ax=b,
where
       a11  a12  ...  a1n              b1             a11    a12  ...    a1n   b1
A = a...21  a22  ...  a2n,      b=  b...2,      A  =    0    a22  ...    a2n, b = b...2 .
                                                        ...
       an1  an2  ...  ann              bn               0    0    ...    ann                bn
(We added primes to the matrix elements and right-hand sides of the new system
to stress the point that their values differ from their counterparts in the original
system.)
Why is the system with the upper-triangular coefficient matrix better than
a system with an arbitrary coefficient matrix? Because we can easily solve the
system with an upper-triangular coefficient matrix by back substitutions as follows.
First, we can immediately find the value of xn from the last equation; then we can
substitute this value into the next to last equation to get xn-1, and so on, until we
substitute the known values of the last n - 1 variables into the first equation, from
which we find the value of x1.
So how can we get from a system with an arbitrary coefficient matrix A to an
equivalent system with an upper-triangular coefficient matrix A ? We can do that
through a series of the so-called elementary operations:
exchanging two equations of the system
replacing an equation with its nonzero multiple
replacing an equation with a sum or difference of this equation and some
multiple of another equation
Since no elementary operation can change a solution to a system, any system that
is obtained through a series of such operations will have the same solution as the
original one.
Let us see how we can get to a system with an upper-triangular matrix. First,
we use a11 as a pivot to make all x1 coefficients zeros in the equations below
the first one. Specifically, we replace the second equation with the difference
between it and the first equation multiplied by a21/a11 to get an equation with
a zero coefficient for x1. Doing the same for the third, fourth, and finally nth
equation--with the multiples a31/a11, a41/a11, . . . , an1/a11 of the first equation,
respectively--makes all the coefficients of x1 below the first equation zero. Then
we get rid of all the coefficients of x2 by subtracting an appropriate multiple of the
second equation from each of the equations below the second one. Repeating this
     elimination for each of the first n - 1 variables ultimately yields a system with an
     upper-triangular coefficient matrix.
     Before we look at an example of Gaussian elimination, let us note that we
     can operate with just a system's coefficient matrix augmented, as its (n + 1)st
     column, with the equations' right-hand side values. In other words, we need to
     write explicitly neither the variable names nor the plus and equality signs.
     EXAMPLE 1   Solve the system by Gaussian elimination.
                                     2x1 - x2 + x3 = 1
                                     4x1 + x2 - x3 = 5
                                       x1 + x2 + x3 = 0.
                            2    -1        1     1
                            4    1   -1          5    row  2-  4  row  1
                                                               2
                                                               1
                              1  1         1     0    row  3-     row  1
                                                               2
                              2  -1        1     1  
                            0    3   -3          3
                              0  3         1  -  1    row  3-  1  row  2
                                 2         2     2             2
                              2  -1        1     1  
                            0    3   -3          3
                              0  0         2  -2
     Now we can obtain the solution by back substitutions:
     x3 = (-2)/2 = -1,  x2 = (3 - (-3)x3)/3 = 0,      and      x1 = (1 - x3 - (-1)x2)/2 = 1.
     Here is pseudocode of the first stage, called forward elimination, of                 the
     algorithm.
     ALGORITHM   ForwardElimination(A[1..n, 1..n], b[1..n])
     //Applies Gaussian elimination to matrix A of a system's coefficients,
     //augmented with vector b of the system's right-hand side values
     //Input: Matrix A[1..n, 1..n] and column-vector b[1..n]
     //Output: An equivalent upper-triangular matrix in place of A with the
     //corresponding right-hand side values in the (n + 1)st column
     for i  1 to n do A[i, n + 1]  b[i]             //augments the matrix
     for i  1 to n - 1 do
     for j  i + 1 to n do
                 for k  i to n + 1 do
                 A[j, k]  A[j, k] - A[i, k]  A[j, i] / A[i, i]
    There are two important observations to make about this pseudocode. First, it
    is not always correct: if A[i, i] = 0, we cannot divide by it and hence cannot use the
    ith row as a pivot for the ith iteration of the algorithm. In such a case, we should
    take advantage of the first elementary operation and exchange the ith row with
    some row below it that has a nonzero coefficient in the ith column. (If the system
    has a unique solution, which is the normal case for systems under consideration,
    such a row must exist.)
    Since we have to be prepared for the possibility of row exchanges anyway, we
    can take care of another potential difficulty: the possibility that A[i, i] is so small
    and consequently the scaling factor A[j, i]/A[i, i] so large that the new value of
    A[j, k] might become distorted by a round-off error caused by a subtraction of two
    numbers of greatly different magnitudes.3 To avoid this problem, we can always
    look for a row with the largest absolute value of the coefficient in the ith column,
    exchange it with the ith row, and then use the new A[i, i] as the ith iteration's
    pivot. This modification, called partial pivoting, guarantees that the magnitude
    of the scaling factor will never exceed 1.
    The second observation is the fact that the innermost loop is written with a
    glaring inefficiency. Can you find it before checking the following pseudocode,
    which both incorporates partial pivoting and eliminates this inefficiency?
    ALGORITHM       BetterForwardElimination(A[1..n, 1..n], b[1..n])
    //Implements Gaussian elimination with partial pivoting
    //Input: Matrix A[1..n, 1..n] and column-vector b[1..n]
    //Output: An equivalent upper-triangular matrix in place of A and the
    //corresponding right-hand side values in place of the (n + 1)st column
    for i  1 to n do A[i, n + 1]  b[i] //appends b to A as the last column
    for i  1 to n - 1 do
    pivotrow  i
    for j  i + 1 to n do
                    if |A[j, i]| > |A[pivotrow, i]| pivotrow  j
    for k  i to n + 1 do
                    swap(A[i, k], A[pivotrow, k])
    for j  i + 1 to n do
                    temp  A[j, i] / A[i, i]
                    for k  i to n + 1 do
                    A[j, k]  A[j, k] - A[i, k]  temp
    Let us find the time efficiency of this algorithm. Its innermost loop consists of
    a single line,
                             A[j, k]  A[j, k] - A[i, k]  temp,
3.  We discuss round-off errors in more detail in Section 11.4.
         which contains one multiplication and one subtraction. On most computers, multi-
         plication is unquestionably more expensive than addition/subtraction, and hence
         it is multiplication that is usually quoted as the algorithm's basic operation.4 The
         standard summation formulas and rules reviewed in Section 2.3 (see also Appen-
         dix A) are very helpful in the following derivation:
                  n-1       n     n+1      n-1       n                          n-1         n
         C(n)  =                       1=               (n + 1 - i + 1) =                      (n + 2 - i    )
                  i=1 j =i+1 k=i           i=1 j =i+1                           i=1 j =i+1
                  n-1                                                 n-1
               =        (n + 2 - i)(n - (i + 1) + 1) =                     (n + 2 - i)(n - i)
                  i=1                                                 i=1
               =  (n + 1)(n - 1) + n(n - 2) + . . . + 3 . 1
                  n-1                   n-1          n-1              (n  -  1)n(2n  -  1)     2 (n  -  1)n
               =        (j  +   2)j  =        j2  +        2j      =         6              +        2
                  j =1                  j =1         j =1
               =  n(n - 1)(2n + 5)  1n3                    (n3).
                               6              3
         Since the second (back substitution) stage of Gaussian elimination is in                               (n2),
         as you are asked to show in the exercises, the running time is dominated by the
         cubic elimination stage, making the entire algorithm cubic as well.
         Theoretically, Gaussian elimination always either yields an exact solution to a
         system of linear equations when the system has a unique solution or discovers that
         no such solution exists. In the latter case, the system will have either no solutions
         or infinitely many of them. In practice, solving systems of significant size on a
         computer by this method is not nearly so straightforward as the method would
         lead us to believe. The principal difficulty lies in preventing an accumulation of
         round-off errors (see Section 11.4). Consult textbooks on numerical analysis that
         analyze this and other implementation issues in great detail.
