This algorithm is based on an observation that the graph of a continuous function
         must intersect with the x-axis between two points a and b at least once if the
         function's values have opposite signs at these two points (Figure 12.17).
         The validity of this observation is proved as a theorem in calculus courses, and
         we take it for granted here. It serves as the basis of the following algorithm, called
         the bisection method, for solving equation (12.4). Starting with an interval [a, b]
         at whose endpoints f (x) has opposite signs, the algorithm computes the value of
         f (x) at the middle point xmid = (a + b)/2. If f (xmid) = 0, a root was found and the
         algorithm stops. Otherwise, it continues the search for a root either on [a, xmid] or
         on [xmid, b], depending on which of the two halves the values of f (x) have opposite
         signs at the endpoints of the new interval.
         Since we cannot expect the bisection algorithm to stumble on the exact value
         of the equation's root and stop, we need a different criterion for stopping the algo-
     4.  Ruffini's discovery was completely ignored by almost all prominent mathematicians of that time. Abel
         died young after a difficult life of poverty. Galois was killed in a duel when he was only 21 years old.
         Their results on the solution of higher-degree equations are now considered to be among the crowning
         achievements in the history of mathematics.
                                                   f (x )
                    a                        x1                  b       x
FIGURE 12.17 First iteration of the bisection method: x1 is the middle point of interval
           [a, b].
rithm. We can stop the algorithm after the interval [an, bn] bracketing some root x
becomes so small that we can guarantee that the absolute error of approximating
x by xn, the middle point of this interval, is smaller than some small preselected
number  > 0. Since xn is the middle point of [an, bn] and x lies within this interval
as well, we have
                                    |xn  -  x|    bn  -    an .               (12.5)
                                                      2
Hence, we can stop the algorithm as soon as (bn - an)/2 <  or, equivalently,
                                         xn - an < .                          (12.6)
It is not difficult to prove that
                        |xn  -  x|       b1 - a1  for n = 1, 2, . . . .       (12.7)
                                         2n
This inequality implies that the sequence of approximations {xn} can be made as
close to root x as we wish by choosing n large enough. In other words, we can say
that {xn} converges to root x. Note, however, that because any digital computer
represents extremely small values by zero (Section 11.4), the convergence asser-
tion is true in theory but not necessarily in practice. In fact, if we choose  below
a certain machine-dependent threshold, the algorithm may never stop! Another
source of potential complications is round-off errors in computing values of the
function in question. Therefore, it is a good practice to include in a program im-
plementing the bisection method a limit on the number of iterations the algorithm
is allowed to run.
Here is pseudocode of the bisection method.
ALGORITHM         Bisection(f (x), a, b, eps, N )
//Implements the bisection method for finding a root of f (x) = 0
//Input: Two real numbers a and b, a < b,
//  a continuous function f (x) on [a, b], f (a)f (b) < 0,
//  an upper bound on the absolute error eps > 0,
//  an upper bound on the number of iterations N
     //Output: An approximate      (or   exact)   value x of a root in (a, b)
     //or an interval bracketing   the   root if  the iteration number limit   is  reached
     n1         //iteration count
     while n  N do
     x  (a + b)/2
     if x - a < eps return x
     fval  f (x)
     if fval = 0 return x
     if fval f (a) < 0
                bx
     else a  x
     nn+1
     return "iteration limit", a,  b
     Note that we can use inequality (12.7) to find in advance the number of
     iterations that should suffice, at least in theory, to achieve a preselected accuracy
     level. Indeed, choosing the number of iterations n large enough to satisfy (b1 -
     a1)/2n < , i.e.,
                                   n     >  log2  b1  -  a1 ,                      (12.8)
                                                      
     does the trick.
     EXAMPLE 1         Let us consider equation
                                      x3 - x - 1 = 0.                              (12.9)
     It has one real root. (See Figure 12.18 for the graph of f (x) = x3 - x - 1.) Since
     f (0) < 0 and f (2) > 0, the root must lie within interval [0, 2]. If we choose the
     error tolerance level as  = 10-2, inequality (12.8) would require n > log2(2/10-2)
     or n  8 iterations.
     Figure 12.19 contains a trace of the first eight iterations of the bisection
     method applied to equation (12.9).
     Thus, we obtained x8 = 1.3203125 as an approximate value for the root x of
     equation (12.9), and we can guarantee that
                                   |1.3203125 - x| < 10-2.
     Moreover, if we take into account the signs of the function f (x) at a8, b8, and x8,
     we can assert that the root lies between 1.3203125 and 1.328125.
     The principal weakness of the bisection method as a general algorithm for
     solving equations is its slow rate of convergence compared with other known
     methods. It is for this reason that the method is rarely used. Also, it cannot be
     extended to solving more general equations and systems of equations. But it does
     have several strong points. It always converges to a root whenever we start with an
                         y
                                    f(x ) = x 3 ­         x­1
                                 0  2                          x
FIGURE 12.18  Graph  of function f (x) = x3 - x - 1.
n                    an  bn                           xn             f (xn)
1             0.0-       2.0+          1.0                     -1.0
2             1.0-       2.0+          1.5                           0.875
3             1.0-       1.5+          1.25                    -0.296875
4             1.25-      1.5+          1.375                         0.224609
5             1.25-      1.375+        1.3125                  -0.051514
6             1.3125-    1.375+        1.34375                       0.082611
7             1.3125-    1.34375+      1.328125                      0.014576
8             1.3125-    1.328125+     1.3203125               -0.018711
FIGURE 12.19  Trace of the bisection method for solving equation (12.8). The signs
              after the numbers in the second and third columns indicate the sign of
              f (x) = x3 - x - 1 at the corresponding endpoints of the intervals.
interval whose properties are very easy to check. And it does not use derivatives
of the function f (x) as some faster methods do.
What important algorithm does the method of bisection remind you of? If
you have found it to closely resemble binary search, you are correct. Both of
them solve variations of the searching problem, and they are both divide-by-
half algorithms. The principal difference lies in the problem's domain: discrete
for binary search and continuous for the bisection method. Also note that while
binary search requires its input array to be sorted, the bisection method does not
require its function to be nondecreasing or nonincreasing. Finally, whereas binary
search is very fast, the bisection method is relatively slow.
                                         f(x )
                        an               xn                             bn      x
     FIGURE 12.20 Iteration of the method of false position.
