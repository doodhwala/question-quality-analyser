Adversary Arguments
Let us revisit the same game of "guessing" a number used to introduce the idea of
     an information-theoretic argument. We can prove that any algorithm that solves
     this problem must ask at least  log2 n  questions in its worst case by playing the
     role of a hostile adversary who wants to make an algorithm ask as many questions
     as possible. The adversary starts by considering each of the numbers between
     1 and n as being potentially selected. (This is cheating, of course, as far as the
     game is concerned, but not as a way to prove our assertion.) After each question,
     the adversary gives an answer that leaves him with the largest set of numbers
     consistent with this and all the previously given answers. This strategy leaves
     him with at least one-half of the numbers he had before his last answer. If an
     algorithm stops before the size of the set is reduced to 1, the adversary can exhibit
     a number that could be a legitimate input the algorithm failed to identify. It is a
     simple technical matter now to show that one needs           log2 n  iterations to shrink
     an n-element set to a one-element set by halving and rounding up the size of the
     remaining set. Hence, at least  log2 n  questions need to be asked by any algorithm
     in the worst case.
         This example illustrates the adversary method for establishing lower bounds.
     It is based on following the logic of a malevolent but honest adversary: the malev-
olence makes him push the algorithm down the most time-consuming path, and
his honesty forces him to stay consistent with the choices already made. A lower
bound is then obtained by measuring the amount of work needed to shrink a set
of potential inputs to a single input along the most time-consuming path.
As another example, consider the problem of merging two sorted lists of size n
                 a1 < a2 < . . . < an   and  b1 < b2 < . . . < bn
into a single sorted list of size 2n. For simplicity, we assume that all the a's and
b's are distinct, which gives the problem a unique solution. We encountered this
problem when discussing mergesort in Section 5.1. Recall that we did merging by
repeatedly comparing the first elements in the remaining lists and outputting the
smaller among them. The number of key comparisons in the worst case for this
algorithm for merging is 2n - 1.
Is there an algorithm that can do merging faster? The answer turns out to
be no. Knuth [KnuIII, p. 198] quotes the following adversary method for proving
that 2n - 1 is a lower bound on the number of key comparisons made by any
comparison-based algorithm for this problem. The adversary will employ the
following rule: reply true to the comparison ai < bj if and only if i < j. This will
force any correct merging algorithm to produce the only combined list consistent
with this rule:
                        b1 < a1 < b2 < a2 < . . . < bn < an.
To produce this combined list, any correct algorithm will have to explicitly com-
pare 2n - 1 adjacent pairs of its elements, i.e., b1 to a1, a1 to b2, and so on. If one
of these comparisons has not been made, e.g., a1 has not been compared to b2, we
can transpose these keys to get
                        b1 < b2 < a1 < a2 < . . . < bn < an,
which is consistent with all the comparisons made but cannot be distinguished
from the correct configuration given above. Hence, 2n - 1 is, indeed, a lower
bound for the number of key comparisons needed for any merging algorithm.
