Orders of Growth
Why this emphasis on the count's order of growth for large input sizes? A differ-
ence in running times on small inputs is not what really distinguishes efficient
algorithms from inefficient ones. When we have to compute, for example, the
greatest common divisor of two small numbers, it is not immediately clear how
much more efficient Euclid's algorithm is compared to the other two algorithms
discussed in Section 1.1 or even why we should care which of them is faster and
by how much. It is only when we have to find the greatest common divisor of two
large numbers that the difference in algorithm efficiencies becomes both clear and
important. For large values of n, it is the function's order of growth that counts: just
look at Table 2.1, which contains values of a few functions particularly important
for analysis of algorithms.
     The magnitude of the numbers in Table 2.1 has a profound significance for
the analysis of algorithms. The function growing the slowest among these is the
logarithmic function. It grows so slowly, in fact, that we should expect a program
    TABLE  2.1 Values (some approximate)      of several functions important for
           analysis of algorithms
    n      log2 n  n    n log2 n              n2          n3    2n        n!
    10     3.3     101  3.3.101               102         103   103       3.6.106
    102    6.6     102  6.6.102               104         106   1.3.1030  9.3.10157
    103    10      103  1.0.104               106         109
    104    13      104  1.3.105               108         1012
    105    17      105  1.7.106               1010        1015
    106    20      106  2.0.107               1012        1018
    implementing an algorithm with a logarithmic basic-operation count to run practi-
    cally instantaneously on inputs of all realistic sizes. Also note that although specific
    values of such a count depend, of course, on the logarithm's base, the formula
                        loga n = loga b logb n
    makes it possible to switch from one base to another, leaving the count logarithmic
    but with a new multiplicative constant. This is why we omit a logarithm's base and
    write simply log n in situations where we are interested just in a function's order
    of growth to within a multiplicative constant.
    On the other end of the spectrum are the exponential function 2n and the
    factorial function n! Both these functions grow so fast that their values become
    astronomically large even for rather small values of n. (This is the reason why we
    did not include their values for n > 102 in Table 2.1.) For example, it would take
    about 4 . 1010 years for a computer making a trillion (1012) operations per second
    to execute 2100 operations. Though this is incomparably faster than it would have
    taken to execute 100! operations, it is still longer than 4.5 billion (4.5 . 109) years--
    the estimated age of the planet Earth. There is a tremendous difference between
    the orders of growth of the functions 2n and n!, yet both are often referred to as
    "exponential-growth functions" (or simply "exponential") despite the fact that,
    strictly speaking, only the former should be referred to as such. The bottom line,
    which is important to remember, is this:
    Algorithms that require an exponential number of operations are practical
    for solving only problems of very small sizes.
    Another way to appreciate the qualitative difference among the orders of
    growth of the functions in Table 2.1 is to consider how they react to, say, a
    twofold increase in the value of their argument n. The function log2 n increases in
    value by just 1 (because log2 2n = log2 2 + log2 n = 1 + log2 n); the linear function
    increases twofold, the linearithmic function n log2 n increases slightly more than
    twofold; the quadratic function n2 and cubic function n3 increase fourfold and
eightfold, respectively (because (2n)2 = 4n2 and (2n)3 = 8n3); the value of 2n gets
squared (because 22n = (2n)2); and n! increases much more than that (yes, even
mathematics refuses to cooperate to give a neat answer for n!).
