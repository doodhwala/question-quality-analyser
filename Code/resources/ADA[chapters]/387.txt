Limitations of Algorithm Power
Intellect distinguishes between the possible and the impossible; reason
distinguishes between the sensible and the senseless. Even the possible can
be senseless.
               --Max Born (1882­1970), My Life and My Views, 1968
In the preceding chapters of this book, we encountered dozens of algorithms
    for solving a variety of different problems. A fair assessment of algorithms as
problem-solving tools is inescapable: they are very powerful instruments, espe-
cially when they are executed by modern computers. But the power of algorithms
is not unlimited, and its limits are the subject of this chapter. As we shall see, some
problems cannot be solved by any algorithm. Other problems can be solved algo-
rithmically but not in polynomial time. And even when a problem can be solved
in polynomial time by some algorithms, there are usually lower bounds on their
efficiency.
    We start, in Section 11.1, with methods for obtaining lower bounds, which are
estimates on a minimum amount of work needed to solve a problem. In general,
obtaining a nontrivial lower bound even for a simple-sounding problem is a very
difficult task. As opposed to ascertaining the efficiency of a particular algorithm,
the task here is to establish a limit on the efficiency of any algorithm, known or
unknown. This also necessitates a careful description of the operations such algo-
rithms are allowed to perform. If we fail to define carefully the "rules of the game,"
so to speak, our claims may end up in the large dustbin of impossibility-related
statements as, for example, the one made by the celebrated British physicist Lord
Kelvin in 1895: "Heavier-than-air flying machines are impossible."
    Section 11.2 discusses decision trees. This technique allows us, among other
applications, to establish lower bounds on the efficiency of comparison-based
algorithms for sorting and for searching in sorted arrays. As a result, we will be
able to answer such questions as whether it is possible to invent a faster sorting
algorithm than mergesort and whether binary search is the fastest algorithm for
searching in a sorted array. (What does your intuition tell you the answers to these
questions will turn out to be?) Incidentally, decision trees are also a great vehicle
                                                                                         387
           for directing us to a solution of some puzzles, such as the coin-weighing problem
           discussed in Section 4.4.
           Section 11.3 deals with the question of intractability: which problems can
           and cannot be solved in polynomial time. This well-developed area of theoretical
           computer science is called computational complexity theory. We present the basic
           elements of this theory and discuss informally such fundamental notions as P , NP,
           and NP-complete problems, including the most important unresolved question of
           theoretical computer science about the relationship between P and NP problems.
           The last section of this chapter deals with numerical analysis. This branch
           of computer science concerns algorithms for solving problems of "continuous"
           mathematics--solving equations and systems of equations, evaluating such func-
           tions as sin x and ln x, computing integrals, and so on. The nature of such problems
           imposes two types of limitations. First, most cannot be solved exactly. Second,
           solving them even approximately requires dealing with numbers that can be rep-
           resented in a digital computer with only a limited level of precision. Manipulating
           approximate numbers without proper care can lead to very inaccurate results. We
           will see that even solving a basic quadratic equation on a computer poses sig-
           nificant difficulties that require a modification of the canonical formula for the
           equation's roots.
     11.1  Lower-Bound Arguments
           We can look at the efficiency of an algorithm two ways. We can establish its asymp-
           totic efficiency class (say, for the worst case) and see where this class stands with
           respect to the hierarchy of efficiency classes outlined in Section 2.2. For exam-
           ple, selection sort, whose efficiency is quadratic, is a reasonably fast algorithm,
           whereas the algorithm for the Tower of Hanoi problem is very slow because its ef-
           ficiency is exponential. We can argue, however, that this comparison is akin to the
           proverbial comparison of apples to oranges because these two algorithms solve
           different problems. The alternative and possibly "fairer" approach is to ask how
           efficient a particular algorithm is with respect to other algorithms for the same
           problem. Seen in this light, selection sort has to be considered slow because there
           are O(n log n) sorting algorithms; the Tower of Hanoi algorithm, on the other
           hand, turns out to be the fastest possible for the problem it solves.
           When we want to ascertain the efficiency of an algorithm with respect to other
           algorithms for the same problem, it is desirable to know the best possible efficiency
           any algorithm solving the problem may have. Knowing such a lower bound can
           tell us how much improvement we can hope to achieve in our quest for a better
           algorithm for the problem in question. If such a bound is tight, i.e., we already
           know an algorithm in the same efficiency class as the lower bound, we can hope
           for a constant-factor improvement at best. If there is a gap between the efficiency
           of the fastest algorithm and the best lower bound known, the door for possible
           improvement remains open: either a faster algorithm matching the lower bound
           could exist or a better lower bound could be proved.
In this section, we present several methods for establishing lower bounds and
illustrate them with specific examples. As we did in analyzing the efficiency of
specific algorithms in the preceding chapters, we should distinguish between a
lower-bound class and a minimum number of times a particular operation needs
to be executed. As a rule, the second problem is more difficult than the first.
For example, we can immediately conclude that any algorithm for finding the
median of n numbers must be in       (n) (why?), but it is not simple at all to prove
that any comparison-based algorithm for this problem must do at least 3(n - 1)/2
comparisons in the worst case (for odd n).
Trivial Lower Bounds
The simplest method of obtaining a lower-bound class is based on counting the
number of items in the problem's input that must be processed and the number of
output items that need to be produced. Since any algorithm must at least "read" all
the items it needs to process and "write" all its outputs, such a count yields a trivial
lower bound. For example, any algorithm for generating all permutations of n
distinct items must be in  (n!) because the size of the output is n!. And this bound
is tight because good algorithms for generating permutations spend a constant
time on each of them except the initial one (see Section 4.3).
As another example, consider the problem of evaluating a polynomial of
degree n
                           p(x) = anxn + an-1xn-1 + . . . + a0
at a given point x, given its coefficients an, an-1, . . . , a0. It is easy to see that all the
coefficients have to be processed by any polynomial-evaluation algorithm. Indeed,
if it were not the case, we could change the value of an unprocessed coefficient,
which would change the value of the polynomial at a nonzero point x. This means
that any such algorithm must be in         (n). This lower bound is tight because both
the right-to-left evaluation algorithm (Problem 2 in Exercises 6.5) and Horner's
rule (Section 6.5) are both linear.
In a similar vein, a trivial lower bound for computing the product of two
n × n matrices is  (n2) because any such algorithm has to process 2n2 elements
in the input matrices and generate n2 elements of the product. It is still unknown,
however, whether this bound is tight.
Trivial lower bounds are often too low to be useful. For example, the trivial
bound for the traveling salesman problem is  (n2), because its input is n(n - 1)/2
intercity distances and its output is a list of n + 1 cities making up an optimal tour.
But this bound is all but useless because there is no known algorithm with the
running time being a polynomial function of any degree.
There is another obstacle to deriving a meaningful lower bound by this
method. It lies in determining which part of an input must be processed by any
algorithm solving the problem in question. For example, searching for an ele-
ment of a given value in a sorted array does not require processing all its elements
(why?). As another example, consider the problem of determining connectivity of
     an undirected graph defined by its adjacency matrix. It is plausible to expect that
     any such algorithm would have to check the existence of each of the n(n - 1)/2
     potential edges, but the proof of this fact is not trivial.
     Information-Theoretic Arguments
     While the approach outlined above takes into account the size of a problem's
     output, the information-theoretical approach seeks to establish a lower bound
     based on the amount of information it has to produce. Consider, as an example,
     the well-known game of deducing a positive integer between 1 and n selected
     by somebody by asking that person questions with yes/no answers. The amount of
     uncertainty that any algorithm solving this problem has to resolve can be measured
     by  log2 n , the number of bits needed to specify a particular number among the
     n possibilities. We can think of each question (or, to be more accurate, an answer
     to each question) as yielding at most 1 bit of information about the algorithm's
     output, i.e., the selected number. Consequently, any such algorithm will need at
     least  log2 n  such steps before it can determine its output in the worst case.
         The approach we just exploited is called the information-theoretic argument
     because of its connection to information theory. It has proved to be quite useful
     for finding the so-called information-theoretic lower bounds for many problems
     involving comparisons, including sorting and searching. Its underlying idea can be
     realized much more precisely through the mechanism of decision trees. Because
     of the importance of this technique, we discuss it separately and in more detail in
     Section 11.2.
     Adversary Arguments
     Let us revisit the same game of "guessing" a number used to introduce the idea of
     an information-theoretic argument. We can prove that any algorithm that solves
     this problem must ask at least  log2 n  questions in its worst case by playing the
     role of a hostile adversary who wants to make an algorithm ask as many questions
     as possible. The adversary starts by considering each of the numbers between
     1 and n as being potentially selected. (This is cheating, of course, as far as the
     game is concerned, but not as a way to prove our assertion.) After each question,
     the adversary gives an answer that leaves him with the largest set of numbers
     consistent with this and all the previously given answers. This strategy leaves
     him with at least one-half of the numbers he had before his last answer. If an
     algorithm stops before the size of the set is reduced to 1, the adversary can exhibit
     a number that could be a legitimate input the algorithm failed to identify. It is a
     simple technical matter now to show that one needs           log2 n  iterations to shrink
     an n-element set to a one-element set by halving and rounding up the size of the
     remaining set. Hence, at least  log2 n  questions need to be asked by any algorithm
     in the worst case.
         This example illustrates the adversary method for establishing lower bounds.
     It is based on following the logic of a malevolent but honest adversary: the malev-
olence makes him push the algorithm down the most time-consuming path, and
his honesty forces him to stay consistent with the choices already made. A lower
bound is then obtained by measuring the amount of work needed to shrink a set
of potential inputs to a single input along the most time-consuming path.
As another example, consider the problem of merging two sorted lists of size n
                 a1 < a2 < . . . < an   and  b1 < b2 < . . . < bn
into a single sorted list of size 2n. For simplicity, we assume that all the a's and
b's are distinct, which gives the problem a unique solution. We encountered this
problem when discussing mergesort in Section 5.1. Recall that we did merging by
repeatedly comparing the first elements in the remaining lists and outputting the
smaller among them. The number of key comparisons in the worst case for this
algorithm for merging is 2n - 1.
Is there an algorithm that can do merging faster? The answer turns out to
be no. Knuth [KnuIII, p. 198] quotes the following adversary method for proving
that 2n - 1 is a lower bound on the number of key comparisons made by any
comparison-based algorithm for this problem. The adversary will employ the
following rule: reply true to the comparison ai < bj if and only if i < j. This will
force any correct merging algorithm to produce the only combined list consistent
with this rule:
                        b1 < a1 < b2 < a2 < . . . < bn < an.
To produce this combined list, any correct algorithm will have to explicitly com-
pare 2n - 1 adjacent pairs of its elements, i.e., b1 to a1, a1 to b2, and so on. If one
of these comparisons has not been made, e.g., a1 has not been compared to b2, we
can transpose these keys to get
                        b1 < b2 < a1 < a2 < . . . < bn < an,
which is consistent with all the comparisons made but cannot be distinguished
from the correct configuration given above. Hence, 2n - 1 is, indeed, a lower
bound for the number of key comparisons needed for any merging algorithm.
Problem Reduction
We have already encountered the problem-reduction approach in Section 6.6.
There, we discussed getting an algorithm for problem P by reducing it to another
problem Q solvable with a known algorithm. A similar reduction idea can be used
for finding a lower bound. To show that problem P is at least as hard as another
problem Q with a known lower bound, we need to reduce Q to P (not P to Q!).
In other words, we should show that an arbitrary instance of problem Q can be
transformed (in a reasonably efficient fashion) to an instance of problem P , so
any algorithm solving P would solve Q as well. Then a lower bound for Q will be
a lower bound for P . Table 11.1 lists several important problems that are often
used for this purpose.
     TABLE 11.1 Problems often used for establishing lower bounds
              by problem reduction
     Problem                                Lower bound   Tightness
     sorting                                   (n log n)  yes
     searching in a sorted array               (log n)    yes
     element uniqueness problem                (n log n)  yes
     multiplication of n-digit integers        (n)        unknown
     multiplication of n × n matrices          (n2)       unknown
     We will establish the lower bounds for sorting and searching in the next sec-
     tion. The element uniqueness problem asks whether there are duplicates among n
     given numbers. (We encountered this problem in Sections 2.3 and 6.1.) The proof
     of the lower bound for this seemingly simple problem is based on a very sophisti-
     cated mathematical analysis that is well beyond the scope of this book (see, e.g.,
     [Pre85] for a rather elementary exposition). As to the last two algebraic prob-
     lems in Table 11.1, the lower bounds quoted are trivial, but whether they can be
     improved remains unknown.
     As an example of establishing a lower bound by reduction, let us consider
     the Euclidean minimum spanning tree problem: given n points in the Cartesian
     plane, construct a tree of minimum total length whose vertices are the given
     points. As a problem with a known lower bound, we use the element uniqueness
     problem. We can transform any set x1, x2, . . . , xn of n real numbers into a set
     of n points in the Cartesian plane by simply adding 0 as the points' y coordinate:
     (x1, 0), (x2, 0), . . . , (xn, 0). Let T be a minimum spanning tree found for this set of
     points. Since T must contain a shortest edge, checking whether T contains a zero-
     length edge will answer the question about uniqueness of the given numbers. This
     reduction implies that  (n log n) is a lower bound for the Euclidean minimum
     spanning tree problem, too.
     Since the final results about the complexity of many problems are not known,
     the reduction technique is often used to compare the relative complexity of prob-
     lems. For example, the formulas
              x . y = (x + y)2 - (x - y)2      and        x2 = x . x
                                         4
     show that the problems of computing the product of two n-digit integers and
     squaring an n-digit integer belong to the same complexity class, despite the latter
     being seemingly simpler than the former.
     There are several similar results for matrix operations. For example, multi-
     plying two symmetric matrices turns out to be in the same complexity class as
     multiplying two arbitrary square matrices. This result is based on the observation
     that not only is the former problem a special case of the latter one, but also that
we can reduce the problem of multiplying two arbitrary square matrices of order
n, say, A and B, to the problem of multiplying two symmetric matrices
                        X=      0     A        and   Y=    0   BT        ,
                             AT       0                    B       0
where AT and BT are the transpose matrices of A and B (i.e., AT [i, j ] = A[j, i] and
BT [i, j ] = B[j, i]), respectively, and 0 stands for the n × n matrix whose elements
are all zeros. Indeed,
                  XY =       0     A        0  BT    =     AB         0     ,
                            AT     0        B     0        0   AT BT
from which the needed product AB can be easily extracted. (True, we will have
to multiply matrices twice the original size, but this is just a minor technical
complication with no impact on the complexity classes.)
    Though such results are interesting, we will encounter even more important
applications of the reduction approach to comparing problem complexity in Sec-
tion 11.3.
Exercises 11.1
1.  Prove that any algorithm solving the alternating-disk puzzle (Problem 14 in
    Exercises 3.1) must make at least n(n + 1)/2 moves to solve it. Is this lower
    bound tight?
2.  Prove that the classic recursive algorithm for the Tower of Hanoi puzzle
    (Section 2.4) makes the minimum number of disk moves needed to solve the
    problem.
3.  Find a trivial lower-bound class for each of the following problems and indi-
    cate, if you can, whether this bound is tight.
    a.  finding the largest element in an array
    b. checking completeness of a graph represented by its adjacency matrix
    c.  generating all the subsets of an n-element set
    d. determining whether n given real numbers are all distinct
4.  Consider the problem of identifying a lighter fake coin among n identical-
    looking   coins  with    the   help  of    a  balance  scale.     Can   we  use  the  same
    information-theoretic argument as the one in the text for the number of ques-
    tions in the guessing game to conclude that any algorithm for identifying the
    fake will need at least     log2 n   weighings in the worst case?
5.  Prove that any comparison-based algorithm for finding the largest element of
    an n-element set of real numbers must make n - 1 comparisons in the worst
    case.
           6.   Find a tight lower bound for sorting an array by exchanging its adjacent
                elements.
           7.   Give an adversary-argument proof that the time efficiency of any algorithm
                that checks connectivity of a graph with n vertices is in        (n2), provided the
                only operation allowed for an algorithm is to inquire about the presence of
                an edge between two vertices of the graph. Is this lower bound tight?
           8.   What is the minimum number of comparisons needed for a comparison-based
                sorting algorithm to merge any two sorted lists of sizes n and n + 1 elements,
                respectively? Prove the validity of your answer.
           9.   Find the product of matrices A and B through a transformation to a product
                of two symmetric matrices if
                                    A=     1  -1          and  B=  0       1  .
                                           2  3                    -1      2
           10.  a.  Can one use this section's formulas that indicate the complexity equiva-
                    lence of multiplication and squaring of integers to show the complexity
                    equivalence of multiplication and squaring of square matrices?
                b. Show that multiplication of two matrices of order n can be reduced to
                    squaring a matrix of order 2n.
           11.  Find a tight lower-bound class for the problem of finding two closest numbers
                among n real numbers x1, x2, . . . , xn.
           12.  Find a tight lower-bound class for the number placement problem (Problem 9
                in Exercises 6.1).
     11.2  Decision Trees
           Many important algorithms, especially those for sorting and searching, work by
           comparing items of their inputs. We can study the performance of such algorithms
           with a device called a decision tree. As an example, Figure 11.1 presents a decision
           tree of an algorithm for finding a minimum of three numbers. Each internal node
           of a binary decision tree represents a key comparison indicated in the node,
           e.g., k < k . The node's left subtree contains the information about subsequent
           comparisons made if k < k , and its right subtree does the same for the case of
           k > k . (For the sake of simplicity, we assume throughout this section that all input
           items are distinct.) Each leaf represents a possible outcome of the algorithm's
           run on some input of size n. Note that the number of leaves can be greater than
           the number of outcomes because, for some algorithms, the same outcome can
           be arrived at through a different chain of comparisons. (This happens to be the
           case for the decision tree in Figure 11.1.) An important point is that the number of
           leaves must be at least as large as the number of possible outcomes. The algorithm's
           work on a particular input of size n can be traced by a path from the root to a leaf
           in its decision tree, and the number of comparisons made by the algorithm on such
                   yes         a <b                       no
yes          a <c  no                                 yes     b <c         no
a                  c                                  b                        c
FIGURE 11.1  Decision tree for finding a minimum of three numbers.
a run is equal to the length of this path. Hence, the number of comparisons in the
worst case is equal to the height of the algorithm's decision tree.
The central idea behind this model lies in the observation that a tree with a
given number of leaves, which is dictated by the number of possible outcomes, has
to be tall enough to have that many leaves. Specifically, it is not difficult to prove
that for any binary tree with l leaves and height h,
                            h  log2 l  .                                          (11.1)
Indeed, a binary tree of height h with the largest number of leaves has all its leaves
on the last level (why?). Hence, the largest number of leaves in such a tree is 2h.
In other words, 2h  l, which immediately implies (11.1).
Inequality (11.1) puts a lower bound on the heights of binary decision trees
and hence the worst-case number of comparisons made by any comparison-based
algorithm for the problem in question. Such a bound is called the information-
theoretic lower bound (see Section 11.1). We illustrate this technique below on
two important problems: sorting and searching in a sorted array.
Decision Trees for Sorting
Most sorting algorithms are comparison based, i.e., they work by comparing
elements in a list to be sorted. By studying properties of decision trees for such
algorithms, we can derive important lower bounds on their time efficiencies.
We can interpret an outcome of a sorting algorithm as finding a permutation of
the element indices of an input list that puts the list's elements in ascending order.
Consider, as an example, a three-element list a, b, c of orderable items such as
real numbers or strings. For the outcome a < c < b obtained by sorting this list
(see Figure 11.2), the permutation in question is 1, 3, 2. In general, the number of
possible outcomes for sorting an arbitrary n-element list is equal to n!.
                                                          abc
                                       yes                a <b                    no
                         abc                                                                     abc
                yes      a <c          no                                         yes         b <c         no
          abc                              cba                              bac                                cba
     yes  b <c       no                b <a      no                yes      a <c        no            yes   b <a
a <b<c          a  <c<b                         c <a<b       b <a<c                  b <c<a         c <b<a
                   FIGURE 11.2      Decision tree for the tree-element selection sort. A triple above a
                                    node indicates the state of the array being sorted. Note two redundant
                                    comparisons b < a with a single possible outcome because of the results
                                    of some previously made comparisons.
                         Inequality (11.1) implies that the height of a binary decision tree for any
                   comparison-based sorting algorithm and hence the worst-case number of com-
                   parisons made by such an algorithm cannot be less than                     log2 n! :
                                                       Cworst (n)        log2 n! .                                  (11.2)
                   Using Stirling's formula for n!, we get
                     log2 n!     log2      2 n(n/e)n   =     log2     -     log2     +  log2  n  +  log2 2   n log2 n.
                                                          n        n     n        e     2             2
                   In other words, about n log2 n comparisons are necessary in the worst case to sort
                   an arbitrary n-element list by any comparison-based sorting algorithm. Note that
                   mergesort makes about this number of comparisons in its worst case and hence is
                   asymptotically optimal. This also implies that the asymptotic lower bound n log2 n
                   is tight and therefore cannot be substantially improved. We should point out,
                   however, that the lower bound of             log2 n!     can be improved for some values of
                   n. For example,     log2 12!  = 29, but it has been proved that 30 comparisons are
                   necessary (and sufficient) to sort an array of 12 elements in the worst case.
                         We can also use decision trees for analyzing the average-case efficiencies of
                   comparison-based sorting algorithms. We can compute the average number of
                   comparisons for a particular algorithm as the average depth of its decision tree's
                   leaves, i.e., as the average path length from the root to the leaves. For example, for
                                                 abc
                         yes                     a <b                    no
             abc                                                                  bac
        yes  b <c        no                                                 yes   a <c           no
a <b<c                       acb                             b     <  a  <c                          bca
                   yes   a <c           no                                              yes       b <c    no
             a  <c<b                    c <a<b                                    b <c<a                  c <b<a
                FIGURE 11.3       Decision tree for the three-element insertion sort.
                the three-element insertion sort whose decision tree is given in Figure 11.3, this
                number   is  (2   +  3  +  3  +  2  +  3  +  3)/6  =  2  2  .
                                                                         3
                   Under the standard assumption that all n! outcomes of sorting are equally
                likely, the following lower bound on the average number of comparisons Cavg
                made by any comparison-based algorithm in sorting an n-element list has been
                proved:
                                                             Cavg(n)  log2 n!.                                (11.3)
                As we saw earlier, this lower bound is about n log2 n. You might be surprised that
                the lower bounds for the average and worst cases are almost identical. Remember,
                however, that these bounds are obtained by maximizing the number of compar-
                isons made in the average and worst cases, respectively. For a particular sorting
                algorithm, the average-case efficiency can, of course, be significantly better than
                their worst-case efficiency.
                Decision Trees for Searching a Sorted Array
                In this section, we shall see how decision trees can be used for establishing lower
                bounds on the number of key comparisons in searching a sorted array of n keys:
                A[0] < A[1] < . . . < A[n - 1]. The principal algorithm for this problem is binary
                search. As we saw in Section 4.4, the number of comparisons made by binary
                search in the worst case, Cwbsorst(n), is given by the formula
                                           Cwbsorst (n) =    log2 n         +1=   log2(n + 1) .               (11.4)
                                              A [1]
                     <                        =                             >
                     A [0]                    A [1]                         A [2]
     <                  =              >             <                      =             >
     < A[0]          A [0]      (A[0], A[1])         (A [1],  A[2])         A [2]         A [3]
                                                                                   <      =       >
                                                                            (A[2], A[3])  A [3]   > A[3]
     FIGURE 11.4     Ternary decision tree    for binary search      in  a  four-element  array.
     We will use decision trees to determine whether this is the smallest possible
     number of comparisons.
        Since we are dealing here with three-way comparisons in which search key K is
     compared with some element A[i] to see whether K < A[i], K = A[i], or K > A[i],
     it is natural to try using ternary decision trees. Figure 11.4 presents such a tree for
     the case of n = 4. The internal nodes of that tree indicate the array's elements being
     compared with the search key. The leaves indicate either a matching element in
     the case of a successful search or a found interval that the search key belongs to
     in the case of an unsuccessful search.
        We can represent any algorithm for searching a sorted array by three-way
     comparisons with a ternary decision tree similar to that in Figure 11.4. For an
     array of n elements, all such decision trees will have 2n + 1 leaves (n for successful
     searches and n + 1 for unsuccessful ones). Since the minimum height h of a ternary
     tree with l leaves is  log3 l , we get the following lower bound on the number of
     worst-case comparisons:
                                Cworst (n)           log3(2n + 1) .
        This lower bound is smaller than             log2(n + 1) , the number of worst-case
     comparisons for binary search, at least for large values of n (and smaller than or
     equal to  log2(n + 1)      for every positive integer n--see Problem 7 in this section's
     exercises). Can we prove a better lower bound, or is binary search far from
     being optimal? The answer turns out to be the former. To obtain a better lower
     bound, we should consider binary rather than ternary decision trees, such as the
     one in Figure 11.5. Internal nodes in such a tree correspond to the same three-
     way comparisons as before, but they also serve as terminal nodes for successful
     searches. Leaves therefore represent only unsuccessful searches, and there are
     n + 1 of them for searching an n-element array.
                                       A [1]
              <                                                    >
              A [0]                                                A [2]
<                        >                    <                                         >
< A[0]                   (A[0], A[1])         (A[1], A[2])                       A [3]
                                                                          <                >
                                                                   (A[2], A[3])            > A[3]
FIGURE  11.5  Binary  decision tree for binary search       in  a  four-element array.
    As comparison of the decision trees in Figures 11.4 and 11.5 illustrates, the
binary decision tree is simply the ternary decision tree with all the middle subtrees
eliminated. Applying inequality (11.1) to such binary decision trees immediately
yields
                         Cworst (n)           log2(n + 1) .                                (11.5)
This inequality closes the gap between the lower bound and the number of worst-
case comparisons made by binary search, which is also                     log2(n + 1) . A much
more sophisticated analysis (see, e.g., [KnuIII, Section 6.2.1]) shows that under the
standard assumptions about searches, binary search makes the smallest number
of comparisons on the average, as well. The average number of comparisons made
by this algorithm turns out to be about log2 n - 1 and log2(n + 1) for successful
and unsuccessful searches, respectively.
Exercises 11.2
1.  Prove by mathematical induction that
    a.  h     log2 l  for any binary tree with height h and the number of leaves l.
    b.  h     log3 l  for any ternary tree with height h and the number of leaves l.
2.  Consider the problem of finding the median of a three-element set {a, b, c}
    of orderable items.
    a.  What is the information-theoretic lower bound for comparison-based al-
        gorithms solving this problem?
    b. Draw a decision tree for an algorithm solving this problem.
    c.  If the worst-case number of comparisons in your algorithm is greater
        than the information-theoretic lower bound, do you think an algorithm
              matching the lower bound exists? (Either find such an algorithm or prove
              its impossibility.)
     3.   Draw a decision tree and find the number of key comparisons in the worst
          and average cases for
          a.  the three-element basic bubble sort.
          b. the three-element enhanced bubble sort (which stops if no swaps have been
              made on its last pass).
     4.   Design a comparison-based algorithm for sorting a four-element array with
          the smallest number of element comparisons possible.
     5.   Design a comparison-based algorithm for sorting a five-element array with
          seven comparisons in the worst case.
     6.   Draw a binary decision tree for searching a four-element sorted list by sequen-
          tial search.
     7.   Compare the two lower bounds for searching a sorted array-- log3(2n + 1)
          and  log2(n + 1) --to show that
          a.   log3(2n + 1)        log2(n + 1)  for every positive integer n.
          b.   log3(2n + 1)  <     log2(n + 1)  for every positive integer n  n0.
     8.   What is the information-theoretic lower bound for finding the maximum of n
          numbers by comparison-based algorithms? Is this bound tight?
     9.   A tournament tree is a complete binary tree reflecting results of a "knockout
          tournament": its leaves represent n players entering the tournament, and
          each internal node represents a winner of a match played by the players
          represented by the node's children. Hence, the winner of the tournament is
          represented by the root of the tree.
          a.  What is the total number of games played in such a tournament?
          b. How many rounds are there in such a tournament?
          c.  Design an efficient algorithm to determine the second-best player using
              the information produced by the tournament. How many extra games does
              your algorithm require?
     10.  Advanced fake-coin problem       There are n  3 coins identical in appearance;
          either all are genuine or exactly one of them is fake. It is unknown whether
          the fake coin is lighter or heavier than the genuine one. You have a balance
          scale with which you can compare any two sets of coins. That is, by tipping to
          the left, to the right, or staying even, the balance scale will tell whether the
          sets weigh the same or which of the sets is heavier than the other, but not by
          how much. The problem is to find whether all the coins are genuine and, if
          not, to find the fake coin and establish whether it is lighter or heavier than the
          genuine ones.
           a.  Prove that any algorithm for this problem must make at least  log3(2n + 1)
               weighings in the worst case.
           b. Draw a decision tree for an algorithm that solves the problem for n = 3
               coins in two weighings.
           c.  Prove that there exists no algorithm that solves the problem for n = 4 coins
               in two weighings.
           d. Draw a decision tree for an algorithm that solves the problem for n = 4
               coins in two weighings by using an extra coin known to be genuine.
           e.  Draw a decision tree for an algorithm that solves the classic version of
               the problem--that for n = 12 coins in three weighings (with no extra coins
               being used).
      11.  Jigsaw puzzle     A jigsaw puzzle contains n pieces. A "section" of the puzzle is
           a set of one or more pieces that have been connected to each other. A "move"
           consists of connecting two sections. What algorithm will minimize the number
           of moves required to complete the puzzle?
11.3  P , NP , and NP-Complete Problems
      In the study of the computational complexity of problems, the first concern of both
      computer scientists and computing professionals is whether a given problem can
      be solved in polynomial time by some algorithm.
      DEFINITION 1        We say that an algorithm solves a problem in polynomial time
      if its worst-case time efficiency belongs to O(p(n)) where p(n) is a polynomial of
      the problem's input size n. (Note that since we are using big-oh notation here,
      problems solvable in, say, logarithmic time are solvable in polynomial time as
      well.) Problems that can be solved in polynomial time are called tractable, and
      problems that cannot be solved in polynomial time are called intractable.
           There are several reasons for drawing the intractability line in this way. First,
      the entries of Table 2.1 and their discussion in Section 2.1 imply that we cannot
      solve arbitrary instances of intractable problems in a reasonable amount of time
      unless such instances are very small. Second, although there might be a huge
      difference between the running times in O(p(n)) for polynomials of drastically
      different degrees, there are very few useful polynomial-time algorithms with the
      degree of a polynomial higher than three. In addition, polynomials that bound
      running times of algorithms do not usually have extremely large coefficients.
      Third, polynomial functions possess many convenient properties; in particular,
      both the sum and composition of two polynomials are always polynomials too.
      Fourth, the choice of this class has led to a development of an extensive theory
      called computational complexity, which seeks to classify problems according to
      their inherent difficulty. And according to this theory, a problem's intractability
     remains the same for all principal models of computations and all reasonable
     input-encoding schemes for the problem under consideration.
     We just touch on some basic notions and ideas of complexity theory in this
     section. If you are interested in a more formal treatment of this theory, you will
     have no trouble finding a wealth of textbooks devoted to the subject (e.g., [Sip05],
     [Aro09]).
     P and NP Problems
     Most problems discussed in this book can be solved in polynomial time by some
     algorithm. They include computing the product and the greatest common divisor
     of two integers, sorting a list, searching for a key in a list or for a pattern in a text
     string, checking connectivity and acyclicity of a graph, and finding a minimum
     spanning tree and shortest paths in a weighted graph. (You are invited to add
     more examples to this list.) Informally, we can think about problems that can be
     solved in polynomial time as the set that computer science theoreticians call P . A
     more formal definition includes in P only decision problems, which are problems
     with yes/no answers.
     DEFINITION 2      Class  P  is  a  class  of  decision  problems  that  can  be  solved    in
     polynomial time by (deterministic) algorithms. This class of problems is called
     polynomial.
     The restriction of P to decision problems can be justified by the following
     reasons. First, it is sensible to exclude problems not solvable in polynomial time
     because of their exponentially large output. Such problems do arise naturally--
     e.g., generating subsets of a given set or all the permutations of n distinct items--
     but it is apparent from the outset that they cannot be solved in polynomial time.
     Second, many important problems that are not decision problems in their most
     natural formulation can be reduced to a series of decision problems that are easier
     to study. For example, instead of asking about the minimum number of colors
     needed to color the vertices of a graph so that no two adjacent vertices are colored
     the same color, we can ask whether there exists such a coloring of the graph's
     vertices with no more than m colors for m = 1, 2, . . . . (The latter is called the m-
     coloring problem.) The first value of m in this series for which the decision problem
     of m-coloring has a solution solves the optimization version of the graph-coloring
     problem as well.
     It is natural to wonder whether every decision problem can be solved in
     polynomial time. The answer to this question turns out to be no. In fact, some
     decision problems cannot be solved at all by any algorithm. Such problems are
     called undecidable, as opposed to decidable problems that can be solved by an
     algorithm. A famous example of an undecidable problem was given by Alan
    Turing in 1936.1 The problem in question is called the halting problem: given a
    computer program and an input to it, determine whether the program will halt on
    that input or continue working indefinitely on it.
    Here is a surprisingly short proof of this remarkable fact. By way of contra-
    diction, assume that A is an algorithm that solves the halting problem. That is, for
    any program P and input I,
             A(P , I ) =         1,  if program P halts on input I ;
                                 0,  if program P does not halt on input I .
    We can consider program P as an input to itself and use the output of algorithm
    A for pair (P , P ) to construct a program Q as follows:
    Q(P ) =  halts,              if A(P , P ) = 0, i.e., if program P does not halt on input P ;
             does not halt,      if A(P , P ) = 1, i.e., if program P halts on input P .
    Then on substituting Q for P , we obtain
    Q(Q) =   halts,              if A(Q, Q) = 0, i.e., if program Q does not halt on input Q;
             does not halt,      if A(Q, Q) = 1, i.e., if program Q halts on input Q.
    This is a contradiction because neither of the two outcomes for program Q is
    possible, which completes the proof.
    Are there decidable but intractable problems? Yes, there are, but the number
    of known examples is surprisingly small, especially of those that arise naturally
    rather than being constructed for the sake of a theoretical argument.
    There are many important problems, however, for which no polynomial-time
    algorithm has been found, nor has the impossibility of such an algorithm been
    proved. The classic monograph by M. Garey and D. Johnson [Gar79] contains a
    list of several hundred such problems from different areas of computer science,
    mathematics, and operations research. Here is just a small sample of some of the
    best-known problems that fall into this category:
    Hamiltonian         circuit  problem  Determine whether a given graph has a
    Hamiltonian circuit--a path that starts and ends at the same vertex and passes
    through all the other vertices exactly once.
    Traveling salesman problem            Find the shortest tour through n cities with
    known positive integer distances between them (find the shortest Hamiltonian
    circuit in a complete graph with positive integer weights).
1.  This was just one of many breakthrough contributions to theoretical computer science made by the
    English mathematician and computer science pioneer Alan Turing (1912­1954). In recognition of this,
    the ACM--the principal society of computing professionals and researchers--has named after him an
    award given for outstanding contributions to theoretical computer science. A lecture given on such an
    occasion by Richard Karp [Kar86] provides an interesting historical account of the development of
    complexity theory.
     Knapsack problem     Find the most valuable subset of n items of given positive
     integer weights and values that fit into a knapsack of a given positive integer
     capacity.
     Partition problem  Given n positive integers, determine whether it is possi-
     ble to partition them into two disjoint subsets with the same sum.
     Bin-packing problem             Given n items whose sizes are positive rational num-
     bers not larger than 1, put them into the smallest number of bins of size 1.
     Graph-coloring problem          For a given graph, find its chromatic number,
     which is the smallest number of colors that need to be assigned to the graph's
     vertices so that no two adjacent vertices are assigned the same color.
     Integer linear programming problem    Find the maximum (or minimum)
     value of a linear function of several integer-valued variables subject to a finite
     set of constraints in the form of linear equalities and inequalities.
     Some of these problems are decision problems. Those that are not have
     decision-version counterparts (e.g., the m-coloring problem for the graph-coloring
     problem). What all these problems have in common is an exponential (or worse)
     growth of choices, as a function of input size, from which a solution needs to be
     found. Note, however, that some problems that also fall under this umbrella can
     be solved in polynomial time. For example, the Eulerian circuit problem--the
     problem of the existence of a cycle that traverses all the edges of a given graph
     exactly once--can be solved in O(n2) time by checking, in addition to the graph's
     connectivity, whether all the graph's vertices have even degrees. This example is
     particularly striking: it is quite counterintuitive to expect that the problem about
     cycles traversing all the edges exactly once (Eulerian circuits) can be so much
     easier than the seemingly similar problem about cycles visiting all the vertices
     exactly once (Hamiltonian circuits).
     Another common feature of a vast majority of decision problems is the fact
     that although solving such problems can be computationally difficult, checking
     whether a proposed solution actually solves the problem is computationally easy,
     i.e., it can be done in polynomial time. (We can think of such a proposed solution
     as being randomly generated by somebody leaving us with the task of verifying its
     validity.) For example, it is easy to check whether a proposed list of vertices is a
     Hamiltonian circuit for a given graph with n vertices. All we need to check is that
     the list contains n + 1 vertices of the graph in question, that the first n vertices are
     distinct whereas the last one is the same as the first, and that every consecutive
     pair of the list's vertices is connected by an edge. This general observation about
     decision problems has led computer scientists to the notion of a nondeterministic
     algorithm.
     DEFINITION 3  A nondeterministic algorithm is a two-stage procedure that
     takes as its input an instance I of a decision problem and does the following.
     Nondeterministic ("guessing") stage: An arbitrary string S is generated that
     can be thought of as a candidate solution to the given instance I (but may be
     complete gibberish as well).
Deterministic ("verification") stage: A deterministic algorithm takes both I
and S as its input and outputs yes if S represents a solution to instance I. (If S is
not a solution to instance I , the algorithm either returns no or is allowed not to
halt at all.)
We say that a nondeterministic algorithm solves a decision problem if and
only if for every yes instance of the problem it returns yes on some execu-
tion. (In other words, we require a nondeterministic algorithm to be capable
of "guessing" a solution at least once and to be able to verify its validity. And,
of course, we do not want it to ever output a yes answer on an instance for
which the answer should be no.) Finally, a nondeterministic algorithm is said to
be nondeterministic polynomial if the time efficiency of its verification stage is
polynomial.
Now we can define the class of NP problems.
DEFINITION 4     Class NP is the class of decision problems that can be solved by
nondeterministic polynomial algorithms. This class of problems is called nonde-
terministic polynomial.
Most decision problems are in NP. First of all, this class includes all the
problems in P :
                                 P  NP.
This is true because, if a problem is in P , we can use the deterministic polynomial-
time algorithm that solves it in the verification-stage of a nondeterministic algo-
rithm that simply ignores string S generated in its nondeterministic ("guessing")
stage. But NP also contains the Hamiltonian circuit problem, the partition prob-
lem, decision versions of the traveling salesman, the knapsack, graph coloring, and
many hundreds of other difficult combinatorial optimization problems cataloged
in [Gar79]. The halting problem, on the other hand, is among the rare examples
of decision problems that are known not to be in NP.
This leads to the most important open question of theoretical computer sci-
ence: Is P a proper subset of NP, or are these two classes, in fact, the same? We
can put this symbolically as
                                 P =? NP.
Note that P = NP would imply that each of many hundreds of difficult
combinatorial decision problems can be solved by a polynomial-time algorithm,
although computer scientists have failed to find such algorithms despite their per-
sistent efforts over many years. Moreover, many well-known decision problems
are known to be "NP-complete" (see below), which seems to cast more doubts
on the possibility that P = NP.
     NP -Complete Problems
     Informally, an NP-complete problem is a problem in NP that is as difficult as any
     other problem in this class because, by definition, any other problem in NP can
     be reduced to it in polynomial time (shown symbolically in Figure 11.6).
         Here are more formal definitions of these concepts.
     DEFINITION 5  A decision problem D1 is said to be polynomially reducible to
     a decision problem D2, if there exists a function t that transforms instances of D1
     to instances of D2 such that:
     1.  t maps all yes instances of D1 to yes instances of D2 and all no instances of D1
         to no instances of D2
     2.  t is computable by a polynomial time algorithm
         This definition immediately implies that if a problem D1 is polynomially
     reducible to some problem D2 that can be solved in polynomial time, then problem
     D1 can also be solved in polynomial time (why?).
     DEFINITION 6  A decision problem D is said to be NP-complete if:
     1.  it belongs to class NP
     2.  every problem in NP is polynomially reducible to D
         The fact that closely related decision problems are polynomially reducible to
     each other is not very surprising. For example, let us prove that the Hamiltonian
     circuit problem is polynomially reducible to the decision version of the traveling
                                     NP problems
                                     NP - complete problem
     FIGURE 11.6 Notion of an NP-complete problem. Polynomial-time reductions of NP
         problems to an NP-complete problem are shown by arrows.
    salesman problem. The latter can be stated as the existence problem of a Hamil-
    tonian circuit not longer than a given positive integer m in a given complete graph
    with positive integer weights. We can map a graph G of a given instance of the
    Hamiltonian circuit problem to a complete weighted graph G representing an in-
    stance of the traveling salesman problem by assigning 1 as the weight to each edge
    in G and adding an edge of weight 2 between any pair of nonadjacent vertices in
    G. As the upper bound m on the Hamiltonian circuit length, we take m = n, where
    n is the number of vertices in G (and G ). Obviously, this transformation can be
    done in polynomial time.
    Let G be a yes instance of the Hamiltonian circuit problem. Then G has a
    Hamiltonian circuit, and its image in G  will have length n, making the image a
    yes instance of the decision traveling salesman problem. Conversely, if we have a
    Hamiltonian circuit of the length not larger than n in G , then its length must be
    exactly n (why?) and hence the circuit must be made up of edges present in G,
    making the inverse image of the yes instance of the decision traveling salesman
    problem be a yes instance of the Hamiltonian circuit problem. This completes the
    proof.
    The notion of NP-completeness requires, however, polynomial reducibility of
    all problems in NP, both known and unknown, to the problem in question. Given
    the bewildering variety of decision problems, it is nothing short of amazing that
    specific examples of NP-complete problems have been actually found. Neverthe-
    less, this mathematical feat was accomplished independently by Stephen Cook
    in the United States and Leonid Levin in the former Soviet Union.2 In his 1971
    paper, Cook [Coo71] showed that the so-called CNF-satisfiability problem is NP-
    complete. The CNF-satisfiability problem deals with boolean expressions. Each
    boolean expression can be represented in conjunctive normal form, such as the
    following expression involving three boolean variables x1, x2, and x3 and their
    negations denoted x¯1, x¯2, and x¯3, respectively:
                         (x1  x¯2  x¯3)&(x¯1  x2)&(x¯1  x¯2  x¯3).
    The CNF-satisfiability problem asks whether or not one can assign values true and
    false to variables of a given boolean expression in its CNF form to make the entire
    expression true. (It is easy to see that this can be done for the above formula: if
    x1 = true, x2 = true, and x3 = false, the entire expression is true.)
    Since the Cook-Levin discovery of the first known NP-complete problems,
    computer scientists have found many hundreds, if not thousands, of other exam-
    ples. In particular, the well-known problems (or their decision versions) men-
    tioned above--Hamiltonian circuit, traveling salesman, partition, bin packing,
    and graph coloring--are all NP-complete. It is known, however, that if P = NP
    there must exist NP problems that neither are in P nor are NP-complete.
2.  As it often happens in the history of science, breakthrough discoveries are made independently and
    almost simultaneously by several scientists. In fact, Levin introduced a more general notion than NP-
    completeness, which was not limited to decision problems, but his paper [Lev73] was published two
    years after Cook's.
     For a while, the leading candidate to be such an example was the problem
     of determining whether a given integer is prime or composite. But in an im-
     portant theoretical breakthrough, Professor Manindra Agrawal and his students
     Neeraj Kayal and Nitin Saxena of the Indian Institute of Technology in Kanpur
     announced in 2002 a discovery of a deterministic polynomial-time algorithm for
     primality testing [Agr04]. Their algorithm does not solve, however, the related
     problem of factoring large composite integers, which lies at the heart of the widely
     used encryption method called the RSA algorithm [Riv78].
     Showing that a decision problem is NP-complete can be done in two steps.
     First, one needs to show that the problem in question is in NP; i.e., a randomly
     generated string can be checked in polynomial time to determine whether or not
     it represents a solution to the problem. Typically, this step is easy. The second
     step is to show that every problem in NP is reducible to the problem in question
     in polynomial time. Because of the transitivity of polynomial reduction, this step
     can be done by showing that a known NP-complete problem can be transformed
     to the problem in question in polynomial time (see Figure 11.7). Although such
     a transformation may need to be quite ingenious, it is incomparably simpler than
     proving the existence of a transformation for every problem in NP. For example,
     if we already know that the Hamiltonian circuit problem is NP-complete, its
     polynomial reducibility to the decision traveling salesman problem implies that
     the latter is also NP-complete (after an easy check that the decision traveling
     salesman problem is in class NP).
     The definition of NP-completeness immediately implies that if there exists a
     deterministic polynomial-time algorithm for just one NP-complete problem, then
     every problem in NP can be solved in polynomial time by a deterministic algo-
     rithm, and hence P = NP. In other words, finding a polynomial-time algorithm
                                        NP  problems
                                        known
                                        NP -complete
                                        problem         candidate for
                                                        NP -completeness
     FIGURE 11.7 Proving NP-completeness by reduction.
    for one NP-complete problem would mean that there is no qualitative difference
    between the complexity of checking a proposed solution and finding it in polyno-
    mial time for the vast majority of decision problems of all kinds. Such implications
    make most computer scientists believe that P = NP, although nobody has been
    successful so far in finding a mathematical proof of this intriguing conjecture. Sur-
    prisingly, in interviews with the authors of a book about the lives and discoveries
    of 15 prominent computer scientists [Sha98], Cook seemed to be uncertain about
    the eventual resolution of this dilemma whereas Levin contended that we should
    expect the P = NP outcome.
        Whatever the eventual answer to the P =? NP question proves to be, knowing
    that a problem is NP-complete has important practical implications for today. It
    means that faced with a problem known to be NP-complete, we should probably
    not aim at gaining fame and fortune3 by designing a polynomial-time algorithm
    for solving all its instances. Rather, we should concentrate on several approaches
    that seek to alleviate the intractability of such problems. These approaches are
    outlined in the next chapter of the book.
    Exercises 11.3
    1.  A game of chess can be posed as the following decision problem: given a
        legal positioning of chess pieces and information about which side is to move,
        determine whether that side can win. Is this decision problem decidable?
    2.  A certain problem can be solved by an algorithm whose running time is in
        O(nlog2 n). Which of the following assertions is true?
        a.  The problem is tractable.
        b. The problem is intractable.
        c.  Impossible to tell.
    3.  Give examples of the following graphs or explain why such examples cannot
        exist.
        a.  graph with a Hamiltonian circuit but without an Eulerian circuit
        b. graph with an Eulerian circuit but without a Hamiltonian circuit
        c.  graph with both a Hamiltonian circuit and an Eulerian circuit
        d. graph with a cycle that includes all the vertices but with neither a Hamil-
            tonian circuit nor an Eulerian circuit
3.  In 2000, The Clay Mathematics Institute (CMI) of Cambridge, Massachusetts, designated a $1 million
    prize for the solution to this problem.
     4.  For each    of the following graphs,   find  its  chromatic  number.
         a.  a         e              b.        a                     c.  a                f
                                          e                   b                d
             b         f                                                  b                g
                                             d             c                   e
             c         g                                                  c                h
             d         h
     5.  Design a polynomial-time algorithm for the graph 2-coloring problem: deter-
         mine whether vertices of a given graph can be colored in no more than two
         colors so that no two adjacent vertices are colored the same color.
     6.  Consider the following brute-force algorithm for solving the composite num-
         ber problem: Check successive integers from 2 to        n/2  as possible divisors of
         n. If one of them divides n evenly, return yes (i.e., the number is composite);
         if none of them does, return no. Why does this algorithm not put the problem
         in class P ?
     7.  State the decision version for each of the following problems and outline a
         polynomial-time algorithm that verifies whether or not a proposed solution
         solves the problem. (You may assume that a proposed solution represents a
         legitimate input to your verification algorithm.)
             a. knapsack problem             b. bin packing problem
     8.  Show that the partition problem is polynomially reducible to the decision
         version of the knapsack problem.
     9.  Show that the following three problems are polynomially reducible to each
         other.
         (i) Determine, for a given graph G =         V, E       and a positive integer m  |V |,
         whether G contains a clique of size m or more. (A clique of size k in a graph
         is its complete subgraph of k vertices.)
         (ii) Determine, for a given graph G =        V, E       and a positive integer m  |V |,
         whether there is a vertex cover of size m or less for G. (A vertex cover of size
         k for a graph G =  V, E      is a subset V    V such that |V | = k and, for each
         edge (u, v)  E, at least one of u and v belongs to V .)
         (iii) Determine, for a given graph G =       V, E       and a positive integer m  |V |,
         whether G contains an independent set of size m or more. (An independent
     set of size k for a graph G =  V, E  is a subset V   V such that |V | = k and
     for all u, v  V , vertices u and v are not adjacent in G.)
10.  Determine whether the following problem is NP-complete. Given several
     sequences of uppercase and lowercase letters, is it possible to select a letter
     from each sequence without selecting both the upper- and lowercase versions
     of any letter? For example, if the sequences are Abc, BC, aB, and ac, it is
     possible to choose A from the first sequence, B from the second and third, and
     c from the fourth. An example where there is no way to make the required
     selections is given by the four sequences AB, Ab, aB, and ab. [Kar86]
11.  Which of the following diagrams do not contradict the current state of our
     knowledge about the complexity classes P, NP, and NPC (NP-complete
     problems)?
         a.                               b.             P = NP
                 P = NP = NPC
                                                                 NPC
         c.            NP                 d.                     NP
                 P     NPC                               P       NPC
         e.            NP
                 P         NPC
12.  King Arthur expects 150 knights for an annual dinner at Camelot. Unfortu-
     nately, some of the knights quarrel with each other, and Arthur knows who
     quarrels with whom. Arthur wants to seat his guests around a table so that no
     two quarreling knights sit next to each other.
     a.  Which standard problem can be used to model King Arthur's task?
     b. As a research project, find a proof that Arthur's problem has a solution if
         each knight does not quarrel with at least 75 other knights.
     11.4  Challenges of Numerical Algorithms
           Numerical analysis is usually described as the branch of computer science con-
           cerned with algorithms for solving mathematical problems. This description needs
           an important clarification: the problems in question are problems of "continuous"
           mathematics--solving equations and systems of equations, evaluating such func-
           tions as sin x and ln x, computing integrals, and so on--as opposed to problems of
           discrete mathematics dealing with such structures as graphs, trees, permutations,
           and combinations. Our interest in efficient algorithms for mathematical problems
           stems from the fact that these problems arise as models of many real-life phe-
           nomena both in the natural world and in the social sciences. In fact, numerical
           analysis used to be the main area of research, study, and application of computer
           science. With the rapid proliferation of computers in business and everyday-life
           applications, which deal primarily with storage and retrieval of information, the
           relative importance of numerical analysis has shrunk in the last 30 years. However,
           its applications, enhanced by the power of modern computers, continue to expand
           in all areas of fundamental research and technology. Thus, wherever one's inter-
           ests lie in the wide world of modern computing, it is important to have at least
           some understanding of the special challenges posed by continuous mathematical
           problems.
           We are not going to discuss the variety of difficulties posed by modeling, the
           task of describing a real-life phenomenon in mathematical terms. Assuming that
           this has already been done, what principal obstacles to solving a mathematical
           problem do we face? The first major obstacle is the fact that most numerical analy-
           sis problems cannot be solved exactly.4 They have to be solved approximately, and
           this is usually done by replacing an infinite object by a finite approximation. For
           example, the value of ex at a given point x can be computed by approximating
           its infinite Taylor's series about x = 0 by a finite sum of its first terms, called the
           nth-degree Taylor polynomial:
                               ex            1  +           x  +    x2  +  .  .  .  +  xn     .           (11.6)
                                                                    2!                 n!
           To give another example, the definite integral of a function can be approximated
           by a finite weighted sum of its values, as in the composite trapezoidal rule that
           you might remember from your calculus class:
                         b                      h [f                       n-1
                            f  (x )d x          2              (a)  +   2           f  (xi )  +  f (b)],  (11.7)
                      a                                                    i=1
           where h = (b - a)/n, xi = a + ih for i = 0, 1, . . . , n (Figure 11.8).
           The errors of such approximations are called truncation errors. One of the
           major tasks in numerical analysis is to estimate the magnitudes of truncation
     4.    Solving a system of linear equations and polynomial evaluation, discussed in Sections 6.2 and 6.5,
           respectively, are rare exceptions to this rule.
   h                               h                h                                  h       x
a           x1              xi ­1          xi             xi +1               xn ­1       b
FIGURE 11.8 Composite trapezoidal rule.
errors. This is typically done by using calculus tools, from elementary to quite
advanced. For example, for approximation (11.6) we have
                |ex - [1 + x + x2 + . . . + xn ]|                   M         |x|n+1,          (11.8)
                                   2!                     n!        (n + 1)!
where M = max e on the segment with the endpoints at 0 and x. This formula
makes it possible to determine the degree of Taylor's polynomial needed to guar-
antee a predefined accuracy level of approximation (11.6).
For example, if we want to compute e0.5 by formula (11.6) and guarantee the
truncation error to be smaller than 10-4, we can proceed as follows. First, we
estimate M of formula (11.8):
                               M=       max         e  e0.5 < 2.
                                        0 0.5
Using this bound and the desired accuracy level of 10-4, we obtain from (11.8)
                M              |0.5|n+1 <              2      0.5n+1 < 10-4.
                (n + 1)!                       (n + 1)!
To solve the last inequality, we can compute the first few values of
                                   2          0.5n+1 =        2-n
                               (n + 1)!                       (n + 1)!
to see that the smallest value of n for which this inequality holds is 5.
Similarly, for approximation (11.7), the standard bound of the truncation error
is given by the inequality
            b               h                  n-1                            (b - a)h2
      |        f (x)dx -    2  [f  (a)  +  2        f  (xi )  +  f  (b)]|     12          M2,  (11.9)
         a                                     i=1
     where  M2 = max |f  (x)|  on    the  interval    a  x  b.  You    are  asked  to  use        this
     inequality in the exercises for this section (Problems 5 and 6).
          The other type of errors, called round-off errors, are caused by the limited
     accuracy with which we can represent real numbers in a digital computer. These
     errors arise not only for all irrational numbers (which, by definition, require an
     infinite number of digits for their exact representation) but for many rational
     numbers as well. In the overwhelming majority of situations, real numbers are
     represented as floating-point numbers,
                                     ±.d1d2 . . . dp . BE,                             (11.10)
     where B is the number base, usually 2 or 16 (or, for unsophisticated calculators,
     10); d1, d2, . . . , dp are digits (0  di < B for i = 1, 2, . . . , p and d1 > 0 unless the
     number is 0) representing together the fractional part of the number and called
     its mantissa; and E is an integer exponent with the range of values approximately
     symmetric about 0.
          The accuracy of the floating-point representation depends on the number
     of significant digits p in representation (11.10). Most computers permit two or
     even three levels of precision: single precision (typically equivalent to between
     6 and 7 significant decimal digits), double precision (13 to 14 significant decimal
     digits), and extended precision (19 to 20 significant decimal digits). Using higher-
     precision arithmetic slows computations but may help to overcome some of the
     problems caused by round-off errors. Higher precision may need to be used only
     for a particular step of the algorithm in question.
          As with an approximation of any kind, it is important to distinguish between
     the absolute error and the relative error of representing a number  by its
     approximation :
                                     absolute error = | - |,                           (11.11)
                                     relative  error  =  | - |  .                      (11.12)
                                                          ||
     (The relative error is undefined if  = 0.)
          Very large and very small numbers cannot be represented in floating-point
     arithmetic because of the phenomena called overflow and underflow, respec-
     tively. An overflow happens when an arithmetic operation yields a result out-
     side the range of the computer's floating-point numbers. Typical examples of
     overflow arise from the multiplication of large numbers or division by a very
     small number. Sometimes we can eliminate this problem by making a simple
     change in the order in which an expression is evaluated (e.g., (1029 . 1130)/1230 =
     1029 . (11/12)30), by replacing an expression with an equal one (e.g., computing
     100  not as 100!/(2!(100 - 2)!) but as (100 . 99)/2), or by computing a logarithm
     2
     of an expression instead of the expression itself.
          Underflow occurs when the result of an operation is a nonzero fraction of
     such a small magnitude that it cannot be represented as a nonzero floating-point
number. Usually, underflow numbers are replaced by zero, but a special signal is
generated by hardware to indicate such an event has occurred.
     It is important to remember that, in addition to inaccurate representation
of numbers, the arithmetic operations performed in a computer are not always
exact, either. In particular, subtracting two nearly equal floating-point numbers
may cause a large increase in relative error. This phenomenon is called subtractive
cancellation.
EXAMPLE 1      Consider two irrational numbers
      =  = 3.14159265 . . .      and   =  - 6 . 10-7 = 3.14159205 . . .
represented by floating-point numbers  = 0.3141593 . 101 and  = 0.3141592 .
101, respectively. The relative errors of these approximations are small:
               |         - |  =  0.0000003  .  .  .  <  4 10-7
                                                        3
and
               |         - |  =  0.00000005 . .   .  <  1 10-7,
                                     - 6 . 10-7         3
respectively. The relative error of representing the difference   =  -  by the
difference of the floating-point representations  =  -  is
                  |      -  |    =  10-6 - 6 . 10-7     =  2  ,
                                      6 . 10-7             3
which is very large for a relative error despite quite accurate approximations for
both  and .
     Note that we may get a significant magnification of round-off error if a low-
accuracy difference is used as a divisor. (We already encountered this problem
in discussing Gaussian elimination in Section 6.2. Our solution there was to use
partial pivoting.) Many numerical algorithms involve thousands or even millions
of arithmetic operations for typical inputs. For such algorithms, the propagation of
round-off errors becomes a major concern from both the practical and theoretical
standpoints. For some algorithms, round-off errors can propagate through the
algorithm's operations with increasing effect. This highly undesirable property
of a numerical algorithm is called instability. Some problems exhibit such a high
level of sensitivity to changes in their input that it is all but impossible to design a
stable algorithm to solve them. Such problems are called ill-conditioned.
EXAMPLE 2      Consider  the  following system of       two   linear  equations  in       two
unknowns:
                              1.001x + 0.999y = 2
                              0.999x + 1.001y = 2.
     Its only solution is x = 1, y = 1. To see how sensitive this system is to small changes
     to its right-hand side, consider the system with the same coefficient matrix but
     slightly different right-hand side values:
                                     1.001x + 0.999y = 2.002
                                     0.999x + 1.001y = 1.998.
     The only solution to this system is x = 2, y = 0, which is quite far from the solution
     to the previous system. Note that the coefficient matrix of this system is close to
     being singular (why?). Hence, a minor change in its coefficients may yield a system
     with either no solutions or infinitely many solutions, depending on its right-hand-
     side values. You can find a more formal and detailed discussion of how we can
     measure the degree of ill-condition of the coefficient matrix in numerical analysis
     textbooks (e.g., [Ger03]).
     We conclude with a well-known problem of finding real roots of the quadratic
     equation
                                        ax2 + bx + c = 0                     (11.13)
     for any real coefficients a, b, and c (a = 0). According to secondary-school algebra,
     equation (11.13) has real roots if and only if its discriminant D = b2 - 4ac is
     nonnegative, and these roots can be found by the following formula
                                     x1,2 = -b ±      b2 - 4ac  .            (11.14)
                                                      2a
     Although formula (11.14) provides a complete solution to the posed problem
     as far as a mathematician is concerned, it is far from being a complete solution for
     an algorithm designer. The first major obstacle is evaluating the square root. Even
     for most positive integers D,      D is an irrational number that can be computed
     only approximately. There is a method of computing square roots that is much
     better than the one commonly taught in secondary school. (It follows from New-
     ton's method, a very important algorithm for solving equations, which we discuss
     in Section 12.4.) This method generates the sequence {xn} of approximations to
     D, where D is a given nonnegative number, according to the formula
                          xn+1   =   1      +  D      for n = 0, 1, . . . ,  (11.15)
                                     2 (xn         )
                                               xn
     where the initial approximation x0 can be chosen, among other possibilities, as
     x0 = (1 + D)/2. It is not difficultto prove that sequence (11.15) is decreasing (if
     D = 1) and always converges to         D. We can stop generating its elements either
     when the difference between its two consecutive elements is less than a predefined
     error tolerance  >0
                                            xn - xn+1 <
    or when xn2+1is sufficiently close to D. Approximation sequence (11.15) converges
    very fast to  D for most values of D. In particular, one can prove that if 0.25 
    D < 1, then no more than four iterations are needed to guarantee that
                                             |xn  -    D|        4  .  10-15,
                                                           <
    and we can always scale a given value of d to one in the interval [0.25, 1) by the
    formula d = D2p, where p is an even integer.
    EXAMPLE 3     Let us apply Newton's algorithm to compute                        2. (For simplicity,
    we ignore scaling.) We will round off the numbers to six decimal places and use
    the standard numerical analysis notation =. to indicate the round-offs.
                                  x0  =      1(1 +     2)  =     1.500000,
                                             2
                                  x1  =      1       +     2     =.    1.416667,
                                             2 (x0            )
                                                        x0
                                      =      1       +  2        =.    1.414216,
                                  x2         2 (x1            )
                                                        x1
                                      =      1       +     2     =.    1.414214,
                                  x3         2 (x2            )
                                                        x2
                                      =      1       +  2        =.    1.414214.
                                  x4         2 (x3            )
                                                        x3
    At this point we have   to stop because x4 = x3 =. 1.414214 and hence                              all  other
    approximations will be  the same. The exact value of                          2 is 1.41421356 . .  ..
    With the issue of computing square roots squared away (I do not know
    whether or not the pun was intended), are we home free to write a program based
    on formula (11.14)? The answer is no because of the possible impact of round-off
    errors. Among other obstacles, we are faced here with the menace of subtractive
    cancellation. If b2 is much larger than 4ac,                    b2 - 4ac will be very close to |b|, and
    a root computed by formula (11.14) might have a large relative error.
    EXAMPLE 4     Let us follow a paper by George Forsythe5 [For69] and consider
    the equation
                                             x2 - 105x + 1 = 0.
    Its true roots to 11 significant digits are
                                             x1 =. 99999.999990
5.  George E. Forsythe (1917­1972), a noted numerical analyst, played a leading role in establishing
    computer science as a separate academic discipline in the United States. It is his words that are used
    as the epigraph to this book's preface.
     and
                                     x2 =. 0.000010000000001.
     If we use formula (11.14) and perform all the computations in decimal floating-
     point arithmetic with, say, seven significant digits, we obtain
          (-b)2 = 0.1000000 . 1011,
          4ac = 0.4000000 . 101,
             D =. 0.1000000 . 1011,
             D                       =.  0.1000000     .  106,
                                         -b  +      D
             x1                      =.      2a        =.    0.1000000     . 106,
             x2                      =.  -b  -      D =. 0.
                                             2a
     And although the relative error of approximating x1                   by x1   is  very  small,  for  the
     second root it is very large:
                                    |x2 - x2|    =  1        (i.e., 100%)
                                         x2
          To avoid the possibility of subtractive cancellation in formula (11.14), we can
     use instead another formula, obtained as follows:
          x1 = -b +                          b2 - 4ac
                                         2a
          = (-b +                            b2 - 4ac)(-b -            b2 - 4ac)
                                             2a(-b -         b2 - 4ac)
          =                              2c               ,
             -b -                            b2 - 4ac
     with no danger of subtractive cancellation in the denominator if b > 0. As to x2,
     it can be computed by the standard formula
                                     x2 = -b -               b2 - 4ac  ,
                                                       2a
     with no danger of cancellation either for a positive value of b.
          The case of b < 0 is symmetric: we can use the formulas
                                     x1 = -b +               b2 - 4ac
                                                          2a
and
                                   x2 =         2c        .
                                         -b +   b2 - 4ac
(The case of b = 0 can be considered with either of the other two cases.)
     There are several other obstacles to applying formula (11.14), which are re-
lated to limitations of floating-point arithmetic: if a is very small, division by a
can cause an overflow; there seems to be no way to fight the danger of subtractive
cancellation in computing b2 - 4ac other than calculating it with double precision;
and so on. These problems have been overcome by William Kahan of the Univer-
sity of Toronto (see [For69]), and his algorithm is considered to be a significant
achievement in the history of numerical analysis.
     Hopefully, this brief overview has piqued your interest enough for you to seek
more information in the many books devoted exclusively to numerical algorithms.
In this book, we discuss one more topic in the next chapter: three classic methods
for solving equations in one unknown.
Exercises 11.4
1.   Some textbooks define the number of significant digits in the approximation
     of number  by number  as the largest nonnegative integer k for which
                                      | - |     <   5  .  10-k.
                                            ||
     According to this definition, how many significant digits are there in the
     approximation of  by
         a. 3.1415?       b. 3.1417?
2.   If  = 1.5 is known to approximate some number  with the absolute error
     not exceeding 10-2, find
     a.  the range of possible values of .
     b. the range of the relative errors of these approximations.
                                         
3.   Find the approximate value of          e = 1.648721 . . . obtained by the fifth-degree
     Taylor's polynomial about 0 and compute the truncation error of this approx-
     imation. Does the result agree with the theoretical prediction made in the
     section?
4.   Derive formula (11.7) of the composite trapezoidal rule.
5.   Use the composite trapezoidal rule with n = 4 to approximate the following
     definite integrals. Find the truncation error of each approximation and com-
     pare it with the one given by formula (11.9).
         a.    1  x2d  x  b.    3  x-1d  x
               0                1
     6.   If  1  esin  xdx  is  to  be  computed  by  the  composite  trapezoidal  rule,  how  large
              0
          should the number of subintervals be to guarantee a truncation error smaller
          than 10-4? Smaller than 10-6?
     7.   Solve the two systems of linear equations and indicate whether they are ill-
          conditioned.
              a.                2x + 5y = 7                b.         2x + 5y = 7
                  2x + 5.000001y = 7.000001                    2x + 4.999999y = 7.000002
     8.   Write a computer program to solve the equation ax2 + bx + c = 0.
     9.   a.  Prove that for any nonnegative number D, the sequence of Newton's
              method for computing           D is strictly decreasing and converges to         D for
              any value of the initial approximation x0 >            D.
          b.  Prove that if 0.25  D < 1 and x0 = (1 + D)/2, no more than four iterations
              of Newton's method are needed to guarantee that
                                             |xn  -     D|     4  .  10-15.
                                                            <
     10.  Apply four iterations of Newton's method to compute                  3 and estimate the
          absolute and relative errors of this approximation.
     SUMMARY
          Given a class of algorithms for solving a particular problem, a lower bound
          indicates the best possible efficiency any algorithm from this class can have.
          A trivial lower bound is based on counting the number of items in the
          problem's input that must be processed and the number of output items
          that need to be produced.
          An information-theoretic lower bound is usually obtained through a mecha-
          nism of decision trees. This technique is particularly useful for comparison-
          based algorithms for sorting and searching. Specifically:
              Any general comparison-based sorting algorithm must perform at least
                 log2 n!     n log2 n key comparisons in the worst case.
              Any general comparison-based algorithm for searching a sorted array
              must perform at least          log2(n + 1)    key comparisons in the worst case.
          The adversary method for establishing lower bounds is based on following
          the logic of a malevolent adversary who forces the algorithm into the most
          time-consuming path.
          A lower bound can also be established by reduction, i.e., by reducing a
          problem with a known lower bound to the problem in question.
          Complexity theory seeks to classify problems according to their computational
          complexity. The principal split is between tractable and intractable problems--
problems that can and cannot be solved in polynomial time, respectively.
For purely technical reasons, complexity theory concentrates on decision
problems, which are problems with yes/no answers.
The halting problem is an example of an undecidable decision problem; i.e.,
it cannot be solved by any algorithm.
P is the class of all decision problems that can be solved in polynomial time.
NP is the class of all decision problems whose randomly guessed solutions
can be verified in polynomial time.
Many important problems in NP (such as the Hamiltonian circuit problem)
are known to be NP-complete: all other problems in NP are reducible to such
a problem in polynomial time. The first proof of a problem's NP-completeness
was published by S. Cook for the CNF-satisfiability problem.
It is not known whether P = NP or P is just a proper subset of NP. This
question is the most important unresolved issue in theoretical computer
science. A discovery of a polynomial-time algorithm for any of the thousands
of known NP-complete problems would imply that P = NP.
Numerical analysis is a branch of computer science dealing with solving
continuous mathematical problems. Two types of errors occur in solving a
majority of such problems: truncation error and round-off error. Truncation
errors stem from replacing infinite objects by their finite approximations.
Round-off errors are due to inaccuracies of representing numbers in a digital
computer.
Subtractive cancellation happens as a result of subtracting two near-equal
floating-point numbers. It may lead to a sharp increase in the relative round-
off error and therefore should be avoided (by either changing the expression's
form or by using a higher precision in computing such a difference).
Writing a general computer program for solving quadratic equations ax2 +
bx + c = 0 is a difficult task. The problem of computing square roots can be
solved by utilizing Newton's method; the problem of subtractive cancellation
can be dealt with by using different formulas depending on whether coefficient
b is positive or negative and by computing the discriminant b2 - 4ac with
double precision.

