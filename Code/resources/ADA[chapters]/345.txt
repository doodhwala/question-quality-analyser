Iterative Improvement
The most successful men in the end are those whose success is the result of
steady accretion.
                                    --Alexander Graham Bell (1835­1910)
The greedy strategy, considered in the preceding chapter, constructs a solution
    to an optimization problem piece by piece, always adding a locally optimal
piece to a partially constructed solution. In this chapter, we discuss a different
approach to designing algorithms for optimization problems. It starts with some
feasible solution (a solution that satisfies all the constraints of the problem) and
proceeds to improve it by repeated applications of some simple step. This step
typically involves a small, localized change yielding a feasible solution with an
improved value of the objective function. When no such change improves the
value of the objective function, the algorithm returns the last feasible solution as
optimal and stops.
    There can be several obstacles to the successful implementation of this idea.
First, we need an initial feasible solution. For some problems, we can always start
with a trivial solution or use an approximate solution obtained by some other (e.g.,
greedy) algorithm. But for others, finding an initial solution may require as much
effort as solving the problem after a feasible solution has been identified. Second,
it is not always clear what changes should be allowed in a feasible solution so that
we can check efficiently whether the current solution is locally optimal and, if not,
replace it with a better one. Third--and this is the most fundamental difficulty--
is an issue of local versus global extremum (maximum or minimum). Think about
the problem of finding the highest point in a hilly area with no map on a foggy day.
A logical thing to do would be to start walking "up the hill" from the point you are
at until it becomes impossible to do so because no direction would lead up. You
will have reached a local highest point, but because of a limited feasibility, there
will be no simple way to tell whether the point is the highest (global maximum
you are after) in the entire area.
    Fortunately, there are important problems that can be solved by iterative-
improvement algorithms. The most important of them is linear programming.
                                                                                       345
           We have already encountered this topic in Section 6.6. Here, in Section 10.1,
           we introduce the simplex method, the classic algorithm for linear programming.
           Discovered by the U.S. mathematician George B. Dantzig in 1947, this algorithm
           has proved to be one of the most consequential achievements in the history of
           algorithmics.
           In Section 10.2, we consider the important problem of maximizing the amount
           of flow that can be sent through a network with links of limited capacities. This
           problem is a special case of linear programming. However, its special structure
           makes it possible to solve the problem by algorithms that are more efficient than
           the simplex method. We outline the classic iterative-improvement algorithm for
           this problem, discovered by the American mathematicians L. R. Ford, Jr., and
           D. R. Fulkerson in the 1950s.
           The last two sections of the chapter deal with bipartite matching. This is
           the problem of finding an optimal pairing of elements taken from two disjoint
           sets. Examples include matching workers and jobs, high school graduates and
           colleges, and men and women for marriage. Section 10.3 deals with the problem
           of maximizing the number of matched pairs; Section 10.4 is concerned with the
           matching stability.
           We also discuss several iterative-improvement algorithms in Section 12.3,
           where we consider approximation algorithms for the traveling salesman and knap-
           sack problems. Other examples of iterative-improvement algorithms can be found
           in the algorithms textbook by Moret and Shapiro [Mor91], books on continuous
           and discrete optimization (e.g., [Nem89]), and the literature on heuristic search
           (e.g., [Mic10]).
     10.1  The Simplex Method
           We have already encountered linear programming (see Section 6.6)--the general
           problem of optimizing a linear function of several variables subject to a set of
           linear constraints:
           maximize (or minimize)        c1x1 + . . . + cnxn
                             subject to  ai1x1 + . . . + ainxn  (or  or =) bi  for i = 1, . . . , m
                                         x1  0, . . . , xn  0.                 (10.1)
           We mentioned there that many important practical problems can be modeled as
           instances of linear programming. Two researchers, L. V. Kantorovich of the former
           Soviet Union and the Dutch-American T. C. Koopmans, were even awarded the
           Nobel Prize in 1975 for their contributions to linear programming theory and
           its applications to economics. Apparently because there is no Nobel Prize in
           mathematics, the Royal Swedish Academy of Sciences failed to honor the U.S.
           mathematician G. B. Dantzig, who is universally recognized as the father of linear
    programming in its modern form and the inventor of the simplex method, the
    classic algorithm for solving such problems.1
    Geometric Interpretation of Linear Programming
    Before we introduce a general method for solving linear programming problems,
    let us consider a small example, which will help us to see the fundamental prop-
    erties of such problems.
    EXAMPLE 1  Consider the following linear programming problem in two vari-
    ables:
                              maximize    3x + 5y
                              subject to  x+       y4                                 (10.2)
                                          x + 3y  6
                                          x  0,    y  0.
    By definition, a feasible solution to this problem is any point (x, y) that satisfies
    all the constraints of the problem; the problem's feasible region is the set of all
    its feasible points. It is instructive to sketch the feasible region in the Cartesian
    plane. Recall that any equation ax + by = c, where coefficients a and b are not
    both equal to zero, defines a straight line. Such a line divides the plane into two
    half-planes: for all the points in one of them, ax + by < c, while for all the points
    in the other, ax + by > c. (It is easy to determine which of the two half-planes
    is which: take any point (x0, y0) not on the line ax + by = c and check which of
    the two inequalities hold, ax0 + by0 > c or ax0 + by0 < c.) In particular, the set of
    points defined by inequality x + y  4 comprises the points on and below the line
    x + y = 4, and the set of points defined by inequality x + 3y  6 comprises the
    points on and below the line x + 3y = 6. Since the points of the feasible region
    must satisfy all the constraints of the problem, the feasible region is obtained by
    the intersection of these two half-planes and the first quadrant of the Cartesian
    plane defined by the nonnegativity constraints x  0, y  0 (see Figure 10.1). Thus,
    the feasible region for problem (10.2) is the convex polygon with the vertices (0, 0),
    (4, 0), (0, 2), and (3, 1). (The last point, which is the point of intersection of the
    lines x + y = 4 and x + 3y = 6, is obtained by solving the system of these two linear
    equations.) Our task is to find an optimal solution, a point in the feasible region
    with the largest value of the objective function z = 3x + 5y.
    Are there feasible solutions for which the value of the objective function
    equals, say, 20? The points (x, y) for which the objective function z = 3x + 5y is
    equal to 20 form the line 3x + 5y = 20. Since this line does not have common points
1.  George B. Dantzig (1914­2005) has received many honors, including the National Medal of Science
    presented by the president of the United States in 1976. The citation states that the National Medal was
    awarded "for inventing linear programming and discovering methods that led to wide-scale scientific
    and technical applications to important problems in logistics, scheduling, and network optimization,
    and to the use of computers in making efficient use of the mathematical theory."
                     y
     x+      3y = 6
             (0, 2)
                                        (3, 1)
             (0, 0)                                (4, 0)                           x
                                                           x+y=4
     FIGURE  10.1 Feasible  region  of  problem (10.2).
     with the feasible region--see Figure 10.2--the answer to the posed question is no.
     On the other hand, there are infinitely many feasible points for which the objective
     function is equal to, say, 10: they are the intersection points of the line 3x + 5y = 10
     with the feasible region. Note that the lines 3x + 5y = 20 and 3x + 5y = 10 have
     the same slope, as would any line defined by equation 3x + 5y = z where z is
     some constant. Such lines are called level lines of the objective function. Thus,
     our problem can be restated as finding the largest value of the parameter z for
     which the level line 3x + 5y = z has a common point with the feasible region.
     We can find this line either by shifting, say, the line 3x + 5y = 20 south-west
     (without changing its slope!) toward the feasible region until it hits the region for
     the first time or by shifting, say, the line 3x + 5y = 10 north-east until it hits the
     feasible region for the last time. Either way, it will happen at the point (3, 1) with
     the corresponding z value 3 . 3 + 5 . 1 = 14. This means that the optimal solution
     to the linear programming problem in question is x = 3, y = 1, with the maximal
     value of the objective function equal to 14.
     Note that if we had to maximize z = 3x + 3y as the objective function in
     problem (10.2), the level line 3x + 3y = z for the largest value of z would coincide
     with the boundary line segment that has the same slope as the level lines (draw
     this line in Figure 10.2). Consequently, all the points of the line segment between
     vertices (3, 1) and (4, 0), including the vertices themselves, would be optimal
     solutions, yielding, of course, the same maximal value of the objective function.
        y
        (0, 2)
                                   (3, 1)
        (0, 0)                             (4, 0)                             x
                                                                3x + 5y = 20
                                                                3x + 5y = 14
                                                                3x + 5y = 10
FIGURE  10.2    Solving  a  two-dimensional linear programming  problem geometrically.
Does every linear programming problem have an optimal solution that can
be found at a vertex of its feasible region? Without appropriate qualifications,
the answer to this question is no. To begin with, the feasible region of a linear
programming problem can be empty. For example, if the constraints include two
contradictory requirements, such as x + y  1 and x + y  2, there can be no points
in the problem's feasible region.  Linear programming problems with the empty
feasible region are called infeasible. Obviously, infeasible problems do not have
optimal solutions.
Another complication may arise if the problem's feasible region is unbounded,
as the following example demonstrates.
EXAMPLE 2       If we reverse the inequalities in problem (10.2) to x + y  4 and
x + 3y  6, the feasible region of the new problem will become unbounded (see
Figure 10.3). If the feasible region of a linear programming problem is unbounded,
its objective function may or may not attain a finite optimal value on it. For
example, the problem of maximizing z = 3x + 5y subject to the constraints x + y 
4, x + 3y  6, x  0, y  0 has no optimal solution, because there are points in
the feasible region making 3x + 5y as large as we wish. Such problems are called
unbounded. On the other hand, the problem of minimizing z = 3x + 5y subject to
the same constraints has an optimal solution (which?).
         y
         (0, 4)
                                 (3, 1)
         (0, 0)                                         (6, 0)                                                  x
                                                                                3x + 5y = 24
                                                                       3x       + 5y = 20
                                                                3x  +  5y = 14
         FIGURE 10.3  Unbounded feasible region of a    linear programming problem with
                      constraints x + y  4, x + 3y  6,  x  0, y  0, and three level lines of
                      the function 3x + 5y.
         Fortunately, the most important features of the examples we considered above
         hold for problems with more than two variables. In particular, a feasible region of
         a typical linear programming problem is in many ways similar to convex polygons
         in the two-dimensional Cartesian plane. Specifically, it always has a finite number
         of vertices, which mathematicians prefer to call extreme points (see Section 3.3).
         Furthermore, an optimal solution to a linear programming problem can be found
         at one of the extreme points of its feasible region. We reiterate these properties
         in the following theorem.
         THEOREM (Extreme Point Theorem)              Any linear programming problem with
         a nonempty bounded feasible region has an optimal solution; moreover, an op-
         timal solution can always be found at an extreme point of the problem's feasible
         region.2
         This theorem implies that to solve a linear programming problem, at least
         in the case of a bounded feasible region, we can ignore all but a finite number of
     2.  Except for some degenerate instances (such as maximizing z = x + y subject to x + y = 1), if a linear
         programming problem with an unbounded feasible region has an optimal solution, it can also be found
         at an extreme point of the feasible region.
points in its feasible region. In principle, we can solve such a problem by computing
the value of the objective function at each extreme point and selecting the one with
the best value. There are two major obstacles to implementing this plan, however.
The first lies in the need for a mechanism for generating the extreme points of the
feasible region. As we are going to see below, a rather straightforward algebraic
procedure for this task has been discovered. The second obstacle lies in the number
of extreme points a typical feasible region has. Here, the news is bad: the number
of extreme points is known to grow exponentially with the size of the problem.
This makes the exhaustive inspection of extreme points unrealistic for most linear
programming problems of nontrivial sizes.
Fortunately, it turns out that there exists an algorithm that typically inspects
only a small fraction of the extreme points of the feasible region before reaching an
optimal one. This famous algorithm is called the simplex method. The idea of this
algorithm can be described in geometric terms as follows. Start by identifying an
extreme point of the feasible region. Then check whether one can get an improved
value of the objective function by going to an adjacent extreme point. If it is not the
case, the current point is optimal--stop; if it is the case, proceed to an adjacent
extreme point with an improved value of the objective function. After a finite
number of steps, the algorithm will either reach an extreme point where an optimal
solution occurs or determine that no optimal solution exists.
An Outline of the Simplex Method
Our task now is to "translate" the geometric description of the simplex method
into the more algorithmically precise language of algebra. To begin with, before
we can apply the simplex method to a linear programming problem, it has to be
represented in a special form called the standard form. The standard form has the
following requirements:
It must be a maximization problem.
All the constraints (except the nonnegativity constraints) must be in the form
of linear equations with nonnegative right-hand sides.
All the variables must be required to be nonnegative.
Thus, the general linear programming problem in standard form with m con-
straints and n unknowns (n  m) is
maximize    c1x1 + . . . + cnxn
subject to  ai1x1 + . . . + ainxn = bi,    where bi  0 for i = 1, 2, . . . , m  (10.3)
            x1  0, . . . , xn  0.
It can also be written in compact matrix notations:
                         maximize          cx
                         subject to        Ax = b
                                           x  0,
     where
                                    x1                                                b1  
                               x =  x...2  ,  A =  a...11  a1... 2  ...  a1...n  ,  b =  b...2  .
     c = [c1 c2    . . . cn],
                                    xn             am1     am2      ...  amn          bm
     Any linear programming problem can be transformed into an equivalent
     problem in standard form. If an objective function needs to be minimized, it can
     be replaced by the equivalent problem of maximizing the same objective function
     with all its coefficients cj replaced by -cj , j = 1, 2, . . . , n (see Section 6.6 for
     a more general discussion of such transformations). If a constraint is given as an
     inequality, it can be replaced by an equivalent equation by adding a slack variable
     representing the difference between the two sides of the original inequality. For
     example, the two inequalities of problem (10.2) can be transformed, respectively,
     into the following equations:
            x + y + u = 4 where u  0          and  x + 3y + v = 6 where v  0.
     Finally, in most linear programming problems, the variables are required to be
     nonnegative to begin with because they represent some physical quantities. If this
     is not the case in an initial statement of a problem, an unconstrained variable
     xj can be replaced by the difference between two new nonnegative variables:
     xj = xj - xj , xj  0, xj  0.
     Thus, problem (10.2) in standard form is the following linear programming
     problem in four variables:
                               maximize       3x + 5y + 0u + 0v
                               subject to     x+   y+      u        =4                (10.4)
                                              x + 3y +        +     v=6
                                              x, y, u, v  0.
     It is easy to see that if we find an optimal solution (x, y, u, v) to problem (10.4),
     we can obtain an optimal solution to problem (10.2) by simply ignoring its last two
     coordinates.
     The principal advantage of the standard form lies in the simple mechanism
     it provides for identifying extreme points of the feasible region. To do this for
     problem (10.4), for example, we need to set two of the four variables in the con-
     straint equations to zero to get a system of two linear equations in two unknowns
     and solve this system. For the general case of a problem with m equations in n
     unknowns (n  m), n - m variables need to be set to zero to get a system of m
     equations in m unknowns. If the system obtained has a unique solution--as any
     nondegenerate system of linear equations with the number of equations equal to
     the number of unknowns does--we have a basic solution; its coordinates set to
     zero before solving the system are called nonbasic, and its coordinates obtained by
     solving the system are called basic. (This terminology comes from linear algebra.
Specifically, we can rewrite the system of constraint equations of (10.4) as
                     x  1     +y  1   +u       1  +v  0  =  4      .
                        1         3            0      1     6
A basis in the two-dimensional vector space is composed of any two vectors that
are not proportional to each other; once a basis is chosen, any vector can be
uniquely expressed as a sum of multiples of the basis vectors. Basic and nonba-
sic variables indicate which of the given vectors are, respectively, included and
excluded in a particular basis choice.)
If all the coordinates of a basic solution are nonnegative, the basic solution is
called a basic feasible solution. For example, if we set to zero variables x and y
and solve the resulting system for u and v, we obtain the basic feasible solution
(0, 0, 4, 6); if we set to zero variables x and u and solve the resulting system for y
and v, we obtain the basic solution (0, 4, 0, -6), which is not feasible. The impor-
tance of basic feasible solutions lies in the one-to-one correspondence between
them and the extreme points of the feasible region. For example, (0, 0, 4, 6) is an
extreme point of the feasible region of problem (10.4) (with the point (0, 0) in Fig-
ure 10.1 being its projection on the x, y plane). Incidentally, (0, 0, 4, 6) is a natural
starting point for the simplex method's application to this problem.
As mentioned above, the simplex method progresses through a series of
adjacent extreme points (basic feasible solutions) with increasing values of the
objective function. Each such point can be represented by a simplex tableau, a
table storing the information about the basic feasible solution corresponding to the
extreme point. For example, the simplex tableau for (0, 0, 4, 6) of problem (10.4)
is presented below:
                           x      y       u       v
u                          1      1       1       0      4
v                          1      3       0       1      6                    (10.5)
                           ­3     ­5      0       0      0
In general, a simplex tableau for a linear programming problem in standard form
with n unknowns and m linear equality constraints (n  m) has m + 1 rows and
n + 1 columns. Each of the first m rows of the table contains the coefficients of
a corresponding constraint equation, with the last column's entry containing the
equation's right-hand side. The columns, except the last one, are labeled by the
names of the variables. The rows are labeled by the basic variables of the basic
feasible solution the tableau represents; the values of the basic variables of this
     solution are in the last column. Also note that the columns labeled by the basic
     variables form the m × m identity matrix.
     The last row of a simplex tableau is called the objective row. It is initialized
     by the coefficients of the objective function with their signs reversed (in the first
     n columns) and the value of the objective function at the initial point (in the last
     column). On subsequent iterations, the objective row is transformed the same
     way as all the other rows. The objective row is used by the simplex method to
     check whether the current tableau represents an optimal solution: it does if all
     the entries in the objective row--except, possibly, the one in the last column--are
     nonnegative. If this is not the case, any of the negative entries indicates a nonbasic
     variable that can become basic in the next tableau.
     For example, according to this criterion, the basic feasible solution (0, 0, 4, 6)
     represented by tableau (10.5) is not optimal. The negative value in the x-column
     signals the fact that we can increase the value of the objective function z = 3x +
     5y + 0u + 0v by increasing the value of the x-coordinate in the current basic
     feasible solution (0, 0, 4, 6). Indeed, since the coefficient for x in the objective
     function is positive, the larger the x value, the larger the value of this function. Of
     course, we will need to "compensate" an increase in x by adjusting the values of
     the basic variables u and v so that the new point is still feasible. For this to be the
     case, both conditions
                            x+u=4                where u  0
                            x+v=6                where v  0
     must be satisfied, which means that
                            x  min{4, 6} = 4.
     Note that if we increase the value of x from 0 to 4, the largest amount possible,
     we will find ourselves at the point (4, 0, 0, 2), an adjacent to (0, 0, 4, 6) extreme
     point of the feasible region, with z = 12.
     Similarly, the negative value in the y-column of the objective row signals the
     fact that we can also increase the value of the objective function by increasing
     the value of the y-coordinate in the initial basic feasible solution (0, 0, 4, 6). This
     requires
                            y+u=4                where u  0
                            3y + v = 6           where v  0,
     which means that
                            y             min{   4  ,  6}  =  2.
                                                 1     3
     If we increase the value of y from 0 to 2, the largest amount possible, we will find
     ourselves at the point (0, 2, 2, 0), another adjacent to (0, 0, 4, 6) extreme point,
     with z = 10.
     If there are several negative entries in the objective row, a commonly used
     rule is to select the most negative one, i.e., the negative number with the largest
absolute value. This rule is motivated by the observation that such a choice yields
the largest increase in the objective function's value per unit of change in a vari-
able's value. (In our example, an increase in the x-value from 0 to 1 at (0, 0, 4, 6)
changes the value of z = 3x + 5y + 0u + 0v from 0 to 3, while an increase in the
y-value from 0 to 1 at (0, 0, 4, 6) changes z from 0 to 5.) Note, however, that the
feasibility constraints impose different limits on how much each of the variables
may increase. In our example, in particular, the choice of the y-variable over the
x-variable leads to a smaller increase in the value of the objective function. Still,
we will employ this commonly used rule and select variable y as we continue with
our example. A new basic variable is called the entering variable, while its column
is referred to as the pivot column; we mark the pivot column by  .
Now we will explain how to choose a departing variable, i.e., a basic variable
to become nonbasic in the next tableau. (The total number of basic variables in any
basic solution must be equal to m, the number of the equality constraints.) As we
saw above, to get to an adjacent extreme point with a larger value of the objective
function, we need to increase the entering variable by the largest amount possible
to make one of the old basic variables zero while preserving the nonnegativity
of all the others. We can translate this observation into the following rule for
choosing a departing variable in a simplex tableau: for each positive entry in the
pivot column, compute the  -ratio by dividing the row's last entry by the entry in
the pivot column. For the example of tableau (10.5), these  -ratios are
                      u  =  4  =  4,       v  =  6  =  2.
                            1                    3
The row with the smallest  -ratio determines the departing variable, i.e., the
variable to become nonbasic. Ties may be broken arbitrarily. For our example, it is
variable v. We mark the row of the departing variable, called the pivot row, by -
and denote it ro--w-. Note that if there are no positive entries in the pivot column,
no -ratio can be computed, which indicates that the problem is unbounded and
the algorithm stops.
Finally, the following steps need to be taken to transform a current tableau
into the next one. (This transformation, called pivoting, is similar to the princi-
pal step of the Gauss-Jordan elimination algorithm for solving systems of linear
equations--see Problem 8 in Exercises 6.2.) First, divide all the entries of the pivot
row by the pivot, its entry in the pivot column, to obtain ro--w-new. For tableau (10.5),
we obtain
                      ro--w-new:  1     1  0     1  2.
                                  3              3
Then, replace each of the other rows, including the objective row, by the difference
                            row - c . ro--w-new,
where c is the row's entry in the pivot column. For tableau (10.5), this yields
                            row 1 - 1 . ro--w-new:     2      0  1      -  1    2,
                                                       3                   3
                 row 3 - (-5) . ro--w-new:             -4     0  0         5    10.
                                                       3                   3
     Thus,  the  simplex method transforms tableau         (10.5)   into   the  following  tableau:
                                 x  y               u      v
                 u               2  0               1  ­   1        2
                                 3                         3
                 y               1  1               0      1        2                      (10.6)
                                 3                         3
                            ­    4  0               0      5        10
                                 3                         3
     Tableau (10.6) represents the basic feasible solution (0, 2, 2, 0) with an increased
     value of the objective function, which is equal to 10. It is not optimal, however
     (why?).
     The next iteration--do it yourself as a good exercise!--yields tableau (10.7):
                                 x  y               u      v
                 x               1  0               3  ­   1        3
                                                    2      2
                 y               0  1  ­            1      1        1                      (10.7)
                                                    2      2
                                 0  0               2      1        14
     This tableau represents the basic feasible solution (3, 1, 0, 0). It is optimal because
     all the entries in the objective row of tableau (10.7) are nonnegative. The maximal
     value of the objective function is equal to 14, the last entry in the objective row.
     Let us summarize the steps of the simplex method.
     Summary of the simplex method
     Step 0      Initialization     Present a given linear programming problem in stan-
                 dard form and set up an initial tableau with nonnegative entries in the
                 rightmost column and m other columns composing the m × m identity
                 matrix. (Entries in the objective row are to be disregarded in verifying
                 these requirements.) These m columns define the basic variables of the
                 initial basic feasible solution, used as the labels of the tableau's rows.
     Step 1 Optimality test         If all the entries in the objective row (except, possibly,
                 the one in the rightmost column, which represents the value of the
        objective function) are nonnegative--stop: the tableau represents an
        optimal solution whose basic variables' values are in the rightmost
        column and the remaining, nonbasic variables' values are zeros.
Step 2  Finding the entering variable   Select a negative entry from among the
        first n elements of the objective row. (A commonly used rule is to select
        the negative entry with the largest absolute value, with ties broken
        arbitrarily.) Mark its column to indicate the entering variable and the
        pivot column.
Step 3  Finding the departing variable  For each positive entry in the pivot
        column, calculate the  -ratio by dividing that row's entry in the right-
        most column by its entry in the pivot column. (If all the entries in the
        pivot column are negative or zero, the problem is unbounded--stop.)
        Find the row with the smallest -ratio (ties may be broken arbitrarily),
        and mark this row to indicate the departing variable and the pivot row.
Step 4  Forming the next tableau       Divide all the entries in the pivot row by
        its entry in the pivot column. Subtract from each of the other rows,
        including the objective row, the new pivot row multiplied by the entry
        in the pivot column of the row in question. (This will make all the
        entries in the pivot column 0's except for 1 in the pivot row.) Replace
        the label of the pivot row by the variable's name of the pivot column
        and go back to Step 1.
Further Notes on the Simplex Method
Formal proofs of validity of the simplex method steps can be found in books
devoted to a detailed discussion of linear programming (e.g., [Dan63]). A few
important remarks about the method still need to be made, however. Generally
speaking, an iteration of the simplex method leads to an extreme point of the prob-
lem's feasible region with a greater value of the objective function. In degenerate
cases, which arise when one or more basic variables are equal to zero, the simplex
method can only guarantee that the value of the objective function at the new
extreme point is greater than or equal to its value at the previous point. In turn,
this opens the door to the possibility not only that the objective function's values
"stall" for several iterations in a row but that the algorithm might cycle back to a
previously considered point and hence never terminate. The latter phenomenon
is called cycling. Although it rarely if ever happens in practice, specific examples
of problems where cycling does occur have been constructed. A simple modifica-
tion of Steps 2 and 3 of the simplex method, called Bland's rule, eliminates even
the theoretical possibility of cycling. Assuming that the variables are denoted by
a subscripted letter (e.g., x1, x2, . . . , xn), this rule can be stated as follows:
Step 2 modified Among the columns with a negative entry in the objective
        row, select the column with the smallest subscript.
Step 3 modified Resolve a tie among the smallest -ratios by selecting the
        row labeled by the basic variable with the smallest subscript.
     Another caveat deals with the assumptions made in Step 0. They are automat-
     ically satisfied if a problem is given in the form where all the constraints imposed
     on nonnegative variables are inequalities ai1x1 + . . . + ainxn  bi with bi  0 for
     i = 1, 2, . . . , m. Indeed, by adding a nonnegative slack variable xn+i into the ith
     constraint, we obtain the equality ai1x1 + . . . + ainxn + xn+i = bi, and all the re-
     quirements imposed on an initial tableau of the simplex method are satisfied for
     the obvious basic feasible solution x1 = . . . = xn = 0, xn+1 = . . . = xn+m = 1. But
     if a problem is not given in such a form, finding an initial basic feasible solution
     may present a nontrivial obstacle. Moreover, for problems with an empty feasible
     region, no initial basic feasible solution exists, and we need an algorithmic way to
     identify such problems. One of the ways to address these issues is to use an exten-
     sion to the classic simplex method called the two-phase simplex method (see, e.g.,
     [Kol95]). In a nutshell, this method adds a set of artificial variables to the equality
     constraints of a given problem so that the new problem has an obvious basic fea-
     sible solution. It then solves the linear programming problem of minimizing the
     sum of the artificial variables by the simplex method. The optimal solution to this
     problem either yields an initial tableau for the original problem or indicates that
     the feasible region of the original problem is empty.
     How efficient is the simplex method? Since the algorithm progresses through
     a sequence of adjacent points of a feasible region, one should probably expect bad
     news because the number of extreme points is known to grow exponentially with
     the problem size. Indeed, the worst-case efficiency of the simplex method has been
     shown to be exponential as well. Fortunately, more than half a century of practical
     experience with the algorithm has shown that the number of iterations in a typical
     application ranges between m and 3m, with the number of operations per iteration
     proportional to mn, where m and n are the numbers of equality constraints and
     variables, respectively.
     Since its discovery in 1947, the simplex method has been a subject of intensive
     study by many researchers. Some of them have worked on improvements to the
     original algorithm and details of its efficient implementation. As a result of these
     efforts, programs implementing the simplex method have been polished to the
     point that very large problems with hundreds of thousands of constraints and
     variables can be solved in a routine manner. In fact, such programs have evolved
     into sophisticated software packages. These packages enable the user to enter
     a problem's constraints and obtain a solution in a user-friendly form. They also
     provide tools for investigating important properties of the solution, such as its
     sensitivity to changes in the input data. Such investigations are very important for
     many applications, including those in economics. At the other end of the spectrum,
     linear programming problems of a moderate size can nowadays be solved on a
     desktop using a standard spreadsheet facility or by taking advantage of specialized
     software available on the Internet.
     Researchers have also tried to find algorithms for solving linear programming
     problems with polynomial-time efficiency in the worst case. An important mile-
     stone in the history of such algorithms was the proof by L. G. Khachian [Kha79]
     showing that the ellipsoid method can solve any linear programming problem in
polynomial time. Although the ellipsoid method was much slower than the simplex
method in practice, its better worst-case efficiency encouraged a search for alterna-
tives to the simplex method. In 1984, Narendra Karmarkar published an algorithm
that not only had a polynomial worst-case efficiency but also was competitive with
the simplex method in empirical tests as well. Although we are not going to discuss
Karmarkar's algorithm [Kar84] here, it is worth pointing out that it is also based
on the iterative-improvement idea. However, Karmarkar's algorithm generates a
sequence of feasible solutions that lie within the feasible region rather than going
through a sequence of adjacent extreme points as the simplex method does. Such
algorithms are called interior-point methods (see, e.g., [Arb93]).
Exercises 10.1
1.  Consider the following version of the post office location problem (Problem
    3 in Exercises 3.3): Given n integers x1, x2, . . . , xn representing coordinates
    of n villages located along a straight road, find a location for a post office that
    minimizes the average distance between the villages. The post office may be,
    but is not required to be, located at one of the villages. Devise an iterative-
    improvement algorithm for this problem. Is this an efficient way to solve this
    problem?
2.  Solve the following linear programming problems geometrically.
    a.          maximize                3x + y
                subject to -x + y  1
                                        2x + y  4
                                        x  0, y     0
    b.          maximize                x + 2y
                subject to 4x  y
                                        y 3+x
                                        x  0, y  0
3.  Consider the linear programming problem
                minimize                c1x  + c2y
                subject to              x    +  y4
                                        x    + 3y  6
                                        x     0, y  0
    where c1 and c2 are some real numbers not both equal to zero.
    a.  Give an example of the coefficient values c1 and c2 for which the problem
        has a unique optimal solution.
          b.  Give an example of the coefficient values c1 and c2 for which the problem
              has infinitely many optimal solutions.
          c.  Give an example of the coefficient values c1 and c2 for which the problem
              does not have an optimal solution.
     4.   Would the solution to problem (10.2) be different if its inequality constraints
          were strict, i.e., x + y < 4 and x + 3y < 6, respectively?
     5.   Trace the simplex method on
          a.  the problem of Exercise 2a.
          b. the problem of Exercise 2b.
     6.   Trace the simplex method on the problem of Example 1 in Section 6.6
          a.  by hand.
          b. by using one of the implementations available on the Internet.
     7.   Determine how many iterations the simplex method needs to solve the
          problem
                                   n
                   maximize           xj
                               j =1
                   subject to  0  xj  bj ,             where bj > 0 for j = 1, 2, . . . , n.
     8.   Can we apply the simplex method to solve the knapsack problem (see Exam-
          ple 2 in Section 6.6)? If you answer yes, indicate whether it is a good algorithm
          for the problem in question; if you answer no, explain why not.
     9.   Prove that no linear programming problem can have exactly k  1 optimal
          solutions unless k = 1.
     10.  If a linear programming problem
                                          n
                        maximize                cj xj
                                          j =1
                                          n
                        subject to              aij xj  bi    for i = 1, 2, . . . , m
                                          j =1
                                          x1, x2, . . . , xn  0
          is considered as primal, then its dual is defined as the linear programming
          problem
                                          m
                            minimize            bi yi
                                          i=1
                                          m
                        subject to              aij yi  cj    for  j  =  1,  2,  .  .  .  ,  n
                                          i=1
                                          y1, y2, . . . , ym   0.
      a.  Express the primal and dual problems in matrix notations.
      b.  Find the dual of the linear programming problem
                                              maximize          x1 + 4x2 -  x3
                                              subject to        x1 +  x2 +  x3  6
                                                                x1 -  x2 - 2x3  2
                                                                x1, x2, x3  0.
      c.  Solve the primal and dual problems and compare the optimal                          values of
          their objective functions.
10.2  The Maximum-Flow Problem
      In this section, we consider the important problem of maximizing the flow of a ma-
      terial through a transportation network (pipeline system, communication system,
      electrical distribution system, and so on). We will assume that the transportation
      network in question can be represented by a connected weighted digraph with n
      vertices numbered from 1 to n and a set of edges E, with the following properties:
      It contains exactly one vertex with no entering edges; this vertex is called the
      source and assumed to be numbered 1.
      It contains exactly one vertex with no leaving edges; this vertex is called the
      sink and assumed to be numbered n.
      The weight uij of each directed edge (i, j ) is a positive integer, called the
      edge capacity. (This number represents the upper bound on the amount of
      the material that can be sent from i to j through a link represented by this
      edge.)
      A digraph satisfying these properties is called a flow network or simply a
      network.3 A small instance of a network is given in Figure 10.4.
      It is assumed that the source and the sink are the only source and destination
      of the material, respectively; all the other vertices can serve only as points where
      a flow can be redirected without consuming or adding any amount of the material.
      In other words, the total amount of the material entering an intermediate vertex
      must be equal to the total amount of the material leaving the vertex. This con-
      dition is called the flow-conservation requirement. If we denote the amount sent
      through edge (i, j ) by xij , then for any intermediate vertex i, the flow-conservation
      requirement can be expressed by the following equality constraint:
                          xji =                            xij  for i = 2, 3, . . . , n - 1,  (10.8)
              j : (j,i)E                      j : (i,j )E
3.    In a slightly more general model, one can consider a network with several sources and sinks and allow
      capacities uij to be infinitely large.
                                                                      5
                                                  3                             4
                      1         2            2       5                3         2       6
                             3                               1
                                             4
     FIGURE     10.4  Example of a network        graph. The vertex      numbers   are  vertex  "names";
                      the edge numbers are        edge capacities.
     where the sums in the left- and right-hand sides express the total inflow and outflow
     entering and leaving vertex i, respectively.
     Since no amount of the material can change by going through intermediate
     vertices of the network, it stands to reason that the total amount of the material
     leaving the source must end up at the sink. (This observation can also be derived
     formally from equalities (10.8), a task you will be asked to do in the exercises.)
     Thus, we have the following equality:
                                                  x1j =                  xj n.                        (10.9)
                                     j : (1,j )E             j : (j,n)E
     This quantity, the total outflow from the source--or, equivalently, the total inflow
     into the sink--is called the value of the flow. We denote it by v. It is this quantity
     that we will want to maximize over all possible flows in a network.
     Thus, a (feasible) flow is an assignment of real numbers xij to edges (i, j ) of
     a given network that satisfy flow-conservation constraints (10.8) and the capacity
     constraints
                             0  xij  uij          for every edge (i, j )  E.                          (10.10)
     The maximum-flow problem can be stated formally as the following optimization
     problem:
     maximize            v=                  x1j
                                j : (1,j )E
     subject to                      xji -                   xij = 0     for i = 2, 3, . . . , n - 1  (10.11)
                         j : (j,i)E             j : (i,j )E
                         0  xij  uij            for every edge (i, j )  E.
     We can solve linear programming problem (10.11) by the simplex method or
     by another algorithm for general linear programming problems (see Section 10.1).
     However, the special structure of problem (10.11) can be exploited to design faster
     algorithms. In particular, it is quite natural to employ the iterative-improvement
idea as follows. We can always start with the zero flow (i.e., set xij = 0 for every
edge (i, j ) in the network). Then, on each iteration, we can try to find a path
from source to sink along which some additional flow can be sent. Such a path is
called flow augmenting. If a flow-augmenting path is found, we adjust the flow
along the edges of this path to get a flow of an increased value and try to find
an augmenting path for the new flow. If no flow-augmenting path can be found,
we conclude that the current flow is optimal. This general template for solving
the maximum-flow problem is called the augmenting-path method, also known
as the Ford-Fulkerson method after L. R. Ford, Jr., and D. R. Fulkerson, who
discovered it (see [For57]).
     An actual implementation of the augmenting path idea is, however, not quite
straightforward. To see this, let us consider the network in Figure 10.4. We start
with the zero flow shown in Figure 10.5a. (In that figure, the zero amounts sent
through each edge are separated from the edge capacities by the slashes; we will
use this notation in the other examples as well.) It is natural to search for a flow-
augmenting path from source to sink by following directed edges (i, j ) for which
the current flow xij is less than the edge capacity uij . Among several possibilities,
let us assume that we identify the augmenting path 1236 first. We can
increase the flow along this path by a maximum of 2 units, which is the smallest
unused capacity of its edges. The new flow is shown in Figure 10.5b. This is as far
as our simpleminded idea about flow-augmenting paths will be able to take us.
Unfortunately, the flow shown in Figure 10.5b is not optimal: its value can still
be increased along the path 143256 by increasing the flow by 1 on
edges (1, 4), (4, 3), (2, 5), and (5, 6) and decreasing it by 1 on edge (2, 3). The flow
obtained as the result of this augmentation is shown in Figure 10.5c. It is indeed
maximal. (Can you tell why?)
     Thus, to find a flow-augmenting path for a flow x, we need to consider paths
from source to sink in the underlying undirected graph in which any two consec-
utive vertices i, j are either
i.   connected by a directed edge from i to j with some positive unused capacity
     rij = uij - xij (so that we can increase the flow through that edge by up to rij
     units), or
ii.  connected by a directed edge from j to i with some positive flow xji (so that
     we can decrease the flow through that edge by up to xji units).
Edges of the first kind are called forward edges because their tail is listed before
their head in the vertex list 1  . . . i  j . . .  n defining the path; edges of the
second kind are called backward edges because their tail is listed after their head in
the path list 1  . . . i  j . . .  n. To illustrate, for the path 143256
of the last example, (1, 4), (4, 3), (2, 5), and (5, 6) are the forward edges, and (3, 2)
is the backward edge.
     For a given flow-augmenting path, let r be the minimum of all the unused
capacities rij of its forward edges and all the flows xji of its backward edges.
It is easy to see that if we increase the current flow by r on each forward edge
and decrease it by this amount on each backward edge, we will obtain a feasible
                                                   5
                                    0/3                         0/4
                  1         0/2  2            0/5  3            0/2      6
                     0/3                      0/1
                                 4
                                              (a)
                                                   5
                                    0/3                         0/4
                  1         2/2  2            2/5  3            2/2      6
                     0/3                      0/1
                                 4
                                              (b)
                                                   5
                                    1/3                         1/4
                  1         2/2  2            1/5  3            2/2      6
                     1/3                      1/1
                                 4
                                              (c)
     FIGURE 10.5  Illustration of the augmenting-path method. Flow-augmenting paths are
                  shown in bold. The flow amounts and edge capacities are indicated by
                  the numbers before and after the slash, respectively.
     flow whose value is r units greater than the value of its predecessor. Indeed, let
     i be an intermediate vertex on a flow-augmenting path. There are four possible
     combinations of forward and backward edges incident to vertex i:
     -+r i -+r ,                 -+r i --r ,       --r i -+r ,       --r i --r .
    For each of them, the flow-conservation requirement for vertex i will still hold
    after the flow adjustments indicated above the edge arrows. Further, since r is the
    minimum among all the positive unused capacities on the forward edges and all
    the positive flows on the backward edges of the flow-augmenting path, the new
    flow will satisfy the capacity constraints as well. Finally, adding r to the flow on
    the first edge of the augmenting path will increase the value of the flow by r.
    Under the assumption that all the edge capacities are integers, r will be a
    positive integer too. Hence, the flow value increases at least by 1 on each iteration
    of the augmenting-path method. Since the value of a maximum flow is bounded
    above (e.g., by the sum of the capacities of the source edges), the augmenting-path
    method has to stop after a finite number of iterations.4 Surprisingly, the final flow
    always turns out to be maximal, irrespective of a sequence of augmenting paths.
    This remarkable result stems from the proof of the Max-Flow Min-Cut Theorem
    (see, e.g., [For62]), which we replicate later in this section.
    The augmenting-path method--as described above in its general form--does
    not indicate a specific way for generating flow-augmenting paths. A bad sequence
    of such paths may, however, have a dramatic impact on the method's efficiency.
    Consider, for example, the network in Figure 10.6a, in which U stands for some
    large positive integer. If we augment the zero flow along the path 1234,
    we shall obtain the flow of value 1 shown in Figure 10.6b. Augmenting that flow
    along the path 1324 will increase the flow value to 2 (Figure 10.6c). If we
    continue selecting this pair of flow-augmenting paths, we will need a total of 2U
    iterations to reach the maximum flow of value 2U (Figure 10.6d). Of course, we
    can obtain the maximum flow in just two iterations by augmenting the initial zero
    flow along the path 124 followed by augmenting the new flow along the path
    134. The dramatic difference between 2U and 2 iterations makes the point.
    Fortunately, there are several ways to generate flow-augmenting paths ef-
    ficiently and avoid the degradation in performance illustrated by the previous
    example. The simplest of them uses breadth-first search to generate augment-
    ing paths with the least number of edges (see Section 3.5). This version of the
    augmenting-path method, called shortest-augmenting-path or first-labeled-first-
    scanned algorithm, was suggested by J. Edmonds and R. M. Karp [Edm72]. The
    labeling refers to marking a new (unlabeled) vertex with two labels. The first label
    indicates the amount of additional flow that can be brought from the source to
    the vertex being labeled. The second label is the name of the vertex from which
    the vertex being labeled was reached. (It can be left undefined for the source.) It
    is also convenient to add the + or - sign to the second label to indicate whether
    the vertex was reached via a forward or backward edge, respectively. The source
    can be always labeled with , -. For the other vertices, the labels are computed
    as follows.
4.  If capacity upper bounds are irrational numbers, the augmenting-path method may not terminate
    (see, e.g., [Chv83, pp. 387­388], for a cleverly devised example demonstrating such a situation). This
    limitation is only of theoretical interest because we cannot store irrational numbers in a computer, and
    rational numbers can be transformed into integers by changing the capacity measurement unit.
                                  2                               2
                      0/U                 0/U          1/U                  0/U
                      1              0/1       4       1             1/1         4
                      0/U                 0/U          0/U                  1/U
                                  3                               3
                                  (a)                             (b)
                                  2                               2
                      1/U                 1/U          U/U                  U/U
                      1              0/1       4       1             0/1         4
                      1/U                 1/U          U/U                  U/U
                                  3                               3
                                  (c)                             (d)
     FIGURE     10.6  Efficiency  degradation of  the  augmenting-path method.
     If unlabeled vertex j is connected to the front vertex i of the traversal queue
     by a directed edge from i to j with positive unused capacity rij = uij - xij , then
     vertex j is labeled with lj , i+, where lj = min{li, rij }.
     If unlabeled vertex j is connected to the front vertex i of the traversal queue
     by a directed edge from j to i with positive flow xji, then vertex j is labeled with
     lj , i-, where lj = min{li, xji}.
     If this labeling-enhanced traversal ends up labeling the sink, the current
     flow can be augmented by the amount indicated by the sink's first label. The
     augmentation is performed along the augmenting path traced by following the
     vertex second labels from sink to source: the current flow quantities are increased
     on the forward edges and decreased on the backward edges of this path. If, on the
     other hand, the sink remains unlabeled after the traversal queue becomes empty,
     the algorithm returns the current flow as maximum and stops.
     ALGORITHM        ShortestAugmentingPath(G)
     //Implements the shortest-augmenting-path algorithm
     //Input: A network with single source 1, single sink n, and
     //               positive integer capacities uij on its edges (i, j )
     //Output: A maximum flow x
     assign xij = 0 to every edge (i, j ) in the network
     label the source with , - and add the source to the empty queue Q
while not Empty(Q) do
i  Front(Q);               Dequeue(Q)
for every edge from i to j do           //forward edges
              if j is unlabeled
              rij  uij - xij
              if rij > 0
                    lj  min{li, rij };  label j with lj , i+
                    Enqueue(Q, j )
for every edge from j to i do           //backward edges
              if j is unlabeled
              if xji > 0
                    lj  min{li, xji};   label j with lj , i-
                    Enqueue(Q, j )
if the sink has been labeled
              //augment along the augmenting path found
              j n   //start at the sink and move backwards using second      labels
              while j = 1  //the source hasn't been reached
              if the second label of vertex j is i+
                    xij  xij + ln
              else       //the second label of vertex j is i-
                    xji  xji - ln
              j  i;        i  the vertex indicated by i's second label
              erase all vertex labels except the ones of the source
              reinitialize Q with the source
return x //the current flow is maximum
An application of this algorithm to the network in Figure 10.4 is illustrated in
Figure 10.7.
The optimality of a final flow obtained by the augmenting-path method stems
from a theorem that relates network flows to network cuts. A cut induced by
partitioning vertices of a network into some subset X containing the source and
X¯ , the complement of X, containing the sink is the set of all the edges with a tail
in X and a head in X¯ . We denote a cut C(X, X¯ ) or simply C. For example, for the
network in Figure 10.4:
if X = {1} and hence X¯ = {2, 3, 4, 5, 6},    C(X, X¯ ) = {(1, 2), (1, 4)};
if X = {1, 2, 3, 4, 5} and hence X¯ = {6},    C(X, X¯ ) = {(3, 6), (5, 6)};
if X = {1, 2, 4} and hence X¯ = {3, 5, 6},    C(X, X¯ ) = {(2, 3), (2, 5), (4, 3)}.
The name "cut" stems from the following property: if all the edges of a cut
were deleted from the network, there would be no directed path from source to
sink. Indeed, let C(X, X¯ ) be a cut. Consider a directed path from source to sink. If
vi is the first vertex of that path which belongs to X¯ (the set of such vertices is not
                                                                                      2, 2+
                                  5                                                   5
             0/3                           0/4                              0/3                    0/4
     0/2          0/5                      0/2                  0/2  2, 1+  0/ 5                   0/ 2
1         2                       3                 6  ,  ­  1       2                3                         6  2,  3+
                                                                                      2, 2+
     0/3               0/1                                      0/3              0/1
          4                                                          4
                                                                     3, 1+
          Queue:  1    2    4  3     5  6                       Augment the flow by 2 (the sink's first label)
                                                                     along the path 1  2  3  6.
                                                                                      1, 2+
                                  5                                                   5
             0/3                           0/4                              0/3                    0/4
     2/2          2/5                      2/2                  2/2  1, 3­  2/5                    2/2
1         2                       3                 6  ,  ­  1       2                3                         6  1,  5+
                                                                                      1, 4+
     0/3               0/1                                      0/3              0/1
          4                                                          4
          Queue:  1    4    3  2     5  6                            3, 1+
                                                                Augment the flow by 1 (the sink's first label)
                                                                along the path 1  4  3  2  5  6.
                                  5                                                   5
                  1/3                      1/4                              1/3                    1/4
1    2/2  2       1/5             3        2/2      6  , ­   1  2/2  2      1/5       3            2/2          6
     1/3               1/1                                      1/3              1/1
          4                                                          4
                                                                     2, 1+
          Queue:     14                                         No augmenting path (the sink is unlabeled);
                                                                     the current flow is maximal.
                  FIGURE 10.7              Illustration of the shortest-augmenting-path algorithm. The diagrams on
                                           the left show the current flow before the next iteration begins; the
                                           diagrams on the right show the results of the vertex labeling on that
                                           iteration, the augmenting path found (in bold), and the flow before its
                                           augmentation. Vertices deleted from the queue are indicated by the 
                                           symbol.
empty, because it contains the sink), then vi is not the source and its immediate
predecessor vi-1 on that path belongs to X. Hence, the edge from vi-1 to vi must
be an element of the cut C(X, X¯ ). This proves the property in question.
       The capacity of a cut C(X, X¯ ), denoted c(X, X¯ ), is defined as the sum of
capacities of the edges that compose the cut. For the three examples of cuts given
above, the capacities are equal to 5, 6, and 9, respectively. Since the number of
different cuts in a network is nonempty and finite (why?), there always exists
a minimum cut, i.e., a cut with the smallest capacity. (What is a minimum cut
in the network of Figure 10.4?) The following theorem establishes an important
relationship between the notions of maximum flow and minimum cut.
THEOREM (Max-Flow Min-Cut Theorem)             The value of a maximum flow in a
network is equal to the capacity of its minimum cut.
PROOF  First, let x be a feasible flow of value v and let C(X, X¯ ) be a cut of
capacity c in the same network. Consider the flow across this cut defined as the
difference between the sum of the flows on the edges from X to X¯ and the sum
of the flows on the edges from X¯ to X. It is intuitively clear and can be formally
derived from the equations expressing the flow-conservation requirement and the
definition of the flow value (Problem 6b in this section's exercises) that the flow
across the cut C(X, X¯ ) is equal to v, the value of the flow:
                          v=            xij -             xj i .           (10.12)
                              iX, j X¯         j X¯ , iX
Since the second sum is nonnegative and the flow xij on any edge (i, j ) cannot
exceed the edge capacity uij , equality (10.12) implies that
                          v             xij               uij ,
                              iX, j X¯         iX, j X¯
i.e.,
                                        v  c.                              (10.13)
Thus, the value of any feasible flow in a network cannot exceed the capacity of
any cut in that network.
       Let v be the value of a final flow x obtained by the augmenting-path method.
If we now find a cut whose capacity is equal to v, we will have to conclude, in view
of inequality (10.13), that (i) the value v of the final flow is maximal among all
feasible flows, (ii) the cut's capacity is minimal among all cuts in the network, and
(iii) the maximum-flow value is equal to the minimum-cut capacity.
       To find such a cut, consider the set of vertices X that can be reached from the
source by following an undirected path composed of forward edges with positive
unused capacities (with respect to the final flow x) and backward edges with
positive flows on them. This set contains the source but does not contain the
sink: if it did, we would have an augmenting path for the flow x, which would
     contradict the assumption that the flow x is final. Consider the cut C(X, X). By
     the definition of set X, each edge (i, j ) from X to X has zero unused capacity,
     i.e., xij = uij , and each edge (j, i) from X to X has the zero flow on it (otherwise,
     j would be in X). Applying equality (10.12) to the final flow x and the set X
     defined above, we obtain
     v =                    xij -           xji =           uij - 0 = c(X, X),
                   iX, j X         j X, iX         iX, j X
     which proves the theorem.
     The proof outlined above accomplishes more than proving the equality of the
     maximum-flow value and the minimum-cut capacity. It also implies that when the
     augmenting-path method terminates, it yields both a maximum flow and a mini-
     mum cut. If labeling of the kind utilized in the shortest-augmenting-path algorithm
     is used, a minimum cut is formed by the edges from the labeled to unlabeled ver-
     tices on the last iteration of the method. Finally, the proof implies that all such
     edges must be full (i.e., the flows must be equal to the edge capacities), and all
     the edges from unlabeled vertices to labeled, if any, must be empty (i.e., have
     zero flows on them). In particular, for the network in Figure 10.7, the algorithm
     finds the cut {(1, 2), (4, 3)} of minimum capacity 3, both edges of which are full as
     required.
     Edmonds and Karp proved in their paper [Edm72] that the number of aug-
     menting paths needed by the shortest-augmenting-path algorithm never exceeds
     nm/2, where n and m are the number of vertices and edges, respectively. Since
     the time required to find a shortest augmenting path by breadth-first search is
     in O(n + m) = O(m) for networks represented by their adjacency lists, the time
     efficiency of the shortest-augmenting-path algorithm is in O(nm2).
     More efficient algorithms for the maximum-flow problem are known (see the
     monograph [Ahu93], as well as appropriate chapters in such books as [Cor09] and
     [Kle06]). Some of them implement the augmenting-path idea in a more efficient
     manner. Others are based on the concept of preflows. A preflow is a flow that
     satisfies the capacity constraints but not the flow-conservation requirement. Any
     vertex is allowed to have more flow entering the vertex than leaving it. A preflow-
     push algorithm moves the excess flow toward the sink until the flow-conservation
     requirement is reestablished for all intermediate vertices of the network. Faster al-
     gorithms of this kind have worst-case efficiency close to O(nm). Note that preflow-
     push algorithms fall outside the iterative-improvement paradigm because they do
     not generate a sequence of improving solutions that satisfy all the constraints of
     the problem.
     To conclude this section, it is worth pointing out that although the initial
     interest in studying network flows was caused by transportation applications, this
     model has also proved to be useful for many other areas. We discuss one of them
     in the next section.
Exercises 10.2
1.  Since maximum-flow algorithms require processing edges in both directions,
    it is convenient to modify the adjacency matrix representation of a network
    as follows. If there is a directed edge from vertex i to vertex j of capacity
    uij , then the element in the ith row and the j th column is set to uij , and the
    element in the j th row and the ith column is set to -uij ; if there is no edge
    between vertices i and j , both these elements are set to zero. Outline a simple
    algorithm for identifying a source and a sink in a network presented by such
    a matrix and indicate its time efficiency.
2.  Apply the shortest-augmenting path algorithm to find a maximum flow and a
    minimum cut in the following networks.
    a.                                   5               2
                                      1            2           5
                                6               4           4
                                      3  7         4     8     6
    b.                                             3
                                         2               4
                                2                                1
                           1                4         4              6
                                7        3               5       5
                                                   2
3.  a.  Does the maximum-flow problem always have a unique solution? Would
        your answer be different for networks with different capacities on all their
        edges?
    b.  Answer the same questions for the minimum-cut problem of finding a cut
        of the smallest capacity in a given network.
4.  a.  Explain  how  the  maximum-flow         problem     for   a  network  with  several
        sources and sinks can be transformed into the same problem for a network
        with a single source and a single sink.
    b.  Some networks have capacity constraints on the flow amounts that can
        flow through their intermediate vertices. Explain how the maximum-flow
        problem for such a network can be transformed to the maximum-flow
        problem for a network with edge capacity constraints only.
5.  Consider a network that is a rooted tree, with the root as its source, the leaves
    as its sinks, and all the edges directed along the paths from the root to the
    leaves. Design an efficient algorithm for finding a maximum flow in such a
    network. What is the time efficiency of your algorithm?
6.  a.  Prove equality (10.9).
                b.   Prove that for any flow in a network and any cut in it, the value of            the
                     flow is equal to the flow across the cut (see equality (10.12)). Explain        the
                     relationship between this property and equality (10.9).
           7.   a.   Express the maximum-flow problem for the network in Figure 10.4 as a
                     linear programming problem.
                b. Solve this linear programming problem by the simplex method.
           8.   As an alternative to the shortest-augmenting-path algorithm, Edmonds and
                Karp [Edm72] suggested the maximum-capacity-augmenting-path algorithm,
                in which a flow is augmented along the path that increases the flow by the
                largest amount. Implement both these algorithms in the language of your
                choice and perform an empirical investigation of their relative efficiency.
           9.   Write  a  report  on  a    more  advanced    maximum-flow         algorithm  such    as
                (i)  Dinitz's  algorithm,  (ii)  Karzanov's  algorithm,    (iii)  Malhotra-Kamar-
                Maheshwari algorithm, or (iv) Goldberg-Tarjan algorithm.
           10.  Dining problem    Several families go out to dinner together. To increase their
                social interaction, they would like to sit at tables so that no two members of
                the same family are at the same table. Show how to find a seating arrangement
                that meets this objective (or prove that no such arrangement exists) by using
                a maximum-flow problem. Assume that the dinner contingent has p families
                and that the ith family has ai members. Also assume that q tables are available
                and the j th table has a seating capacity of bj . [Ahu93]
     10.3  Maximum Matching in Bipartite Graphs
           In many situations we are faced with a problem of pairing elements of two sets.
           The traditional example is boys and girls for a dance, but you can easily think
           of more serious applications. It is convenient to represent elements of two given
           sets by vertices of a graph, with edges between vertices that can be paired. A
           matching in a graph is a subset of its edges with the property that no two edges
           share a vertex. A maximum matching--more precisely, a maximum cardinality
           matching--is a matching with the largest number of edges. (What is it for the graph
           in Figure 10.8? Is it unique?) The maximum-matching problem is the problem of
           finding a maximum matching in a given graph. For an arbitrary graph, this is a
           rather difficult problem. It was solved in 1965 by Jack Edmonds [Edm65]. (See
           [Gal86] for a good survey and more recent references.)
                We limit our discussion in this section to the simpler case of bipartite graphs. In
           a bipartite graph, all the vertices can be partitioned into two disjoint sets V and U ,
           not necessarily of the same size, so that every edge connects a vertex in one of these
           sets to a vertex in the other set. In other words, a graph is bipartite if its vertices
           can be colored in two colors so that every edge has its vertices colored in different
           colors; such graphs are also said to be 2-colorable. The graph in Figure 10.8 is
           bipartite. It is not difficult to prove that a graph is bipartite if and only if it does
           not have a cycle of an odd length. We will assume for the rest of this section that
                    V     1  2                   3  4
                    U     5  6                   7  8
FIGURE 10.8  Example of a bipartite graph.
the vertex set of a given bipartite graph has been already partitioned into sets V
and U as required by the definition (see Problem 8 in Exercises 3.5).
Let  us      apply  the   iterative-improvement     technique   to     the    maximum-
cardinality-matching problem. Let M be a matching in a bipartite graph G =
V , U, E . How can we improve it, i.e., find a new matching with more edges?
Obviously, if every vertex in either V or U is matched (has a mate), i.e., serves as
an endpoint of an edge in M, this cannot be done and M is a maximum matching.
Therefore, to have a chance at improving the current matching,         both V and U
must contain unmatched (also called free) vertices, i.e., vertices that are not inci-
dent to any edge in M. For example, for the matching Ma = {(4, 8), (5, 9)} in the
graph in Figure 10.9a, vertices 1, 2, 3, 6, 7, and 10 are free, and vertices 4, 5, 8,
and 9 are matched.
Another obvious observation is that we can immediately increase a current
matching by adding an edge between two free vertices. For example, adding (1, 6)
to the matching Ma = {(4, 8), (5, 9)} in the graph in Figure 10.9a yields a larger
matching Mb = {(1, 6), (4, 8), (5, 9)} (Figure 10.9b). Let us now try to find a
matching larger than Mb by matching vertex 2. The only way to do this would
be to include the edge (2, 6) in a new matching. This inclusion requires removal of
(1, 6), which can be compensated by inclusion of (1, 7) in the new matching. This
new matching Mc = {(1, 7), (2, 6), (4, 8), (5, 9)} is shown in Figure 10.9c.
In general, we increase the size of a current matching M by constructing a
simple path from a free vertex in V to a free vertex in U whose edges are alternately
in E - M and in M. That is, the first edge of the path does not belong to M, the
second one does, and so on, until the last edge that does not belong to M. Such a
path is called augmenting with respect to the matching M. For example, the path
2, 6, 1, 7 is an augmenting path with respect to the matching Mb in Figure 10.9b.
Since the length of an augmenting path is always odd, adding to the matching M
the path's edges in the odd-numbered positions and deleting from it the path's
edges in the even-numbered positions yields a matching with one more edge than
in M. Such a matching adjustment is called augmentation. Thus, in Figure 10.9,
the matching Mb was obtained by augmentation of the matching Ma along the
augmenting path 1, 6, and the matching Mc was obtained by augmentation of the
matching Mb along the augmenting path 2, 6, 1, 7. Moving further, 3, 8, 4, 9, 5, 10
is an augmenting path for the matching Mc (Figure 10.9c). After adding to Mc
the edges (3, 8), (4, 9), and (5, 10) and deleting (4, 8) and (5, 9), we obtain the
matching Md = {(1, 7), (2, 6), (3, 8), (4, 9), (5, 10)} shown in Figure 10.9d. The
                  V     1      2              3       4                 5
                  U     6      7              8       9                10
                                              (a)
                               Augmenting path:    1, 6
                     1      2     3                4                5
                     6      7     8                9                10
                                  (b)
                            Augmenting path:       2, 6, 1, 7
                     1      2     3                4                5
                     6      7     8                9                10
                                  (c)
                            Augmenting path:     3, 8, 4, 9, 5, 10
                     1      2     3                4                5
                     6      7     8                9                10
                                  (d)
                               Maximum matching
     FIGURE 10.9  Augmenting paths and matching augmentations.
matching Md is not only a maximum matching but also perfect, i.e., a matching
that matches all the vertices of the graph.
Before we discuss an algorithm for finding an augmenting path, let us settle
the issue of what nonexistence of such a path means. According to the theorem
discovered by the French mathematician Claude Berge, it means the current
matching is maximal.
THEOREM  A matching M is a maximum matching if and only if there exists no
augmenting path with respect to M.
PROOF  If an augmenting path with respect to a matching M exists, then the size
of the matching can be increased by augmentation. Let us prove the more difficult
part: if no augmenting path with respect to a matching M exists, then the matching
is a maximum matching. Assume that, on the contrary, this is not the case for a
certain matching M in a graph G. Let M be a maximum matching in G; by our
assumption, the number of edges in M is at least one more than the number
of edges in M, i.e., |M| > |M|. Consider the edges in the symmetric difference
M  M = (M - M)  (M - M), the set of all the edges that are either in M or
in M but not in both. Note that |M - M| > |M - M| because |M| > |M| by
assumption. Let G be the subgraph of G made up of all the edges in M  M and
their endpoints. By definition of a matching, any vertex in G  G can be incident
to no more than one edge in M and no more than one edge in M. Hence, each of
the vertices in G has degree 2 or less, and therefore every connected component
of G is either a path or an even-length cycle of alternating edges from M - M and
M - M. Since |M - M| > |M - M| and the number of edges from M - M and
M - M is the same for any even-length cycle of alternating edges in G , there must
exist at least one path of alternating edges that starts and ends with an edge from
M - M. Hence, this is an augmenting path for the matching M, which contradicts
the assumption that no such path exists.
Our discussion of augmenting paths leads to the following general method
for constructing a maximum matching in a bipartite graph. Start with some initial
matching (e.g., the empty set). Find an augmenting path and augment the current
matching along this path. When no augmenting path can be found, terminate the
algorithm and return the last matching, which is maximum.
We now give a specific algorithm implementing this general template. We will
search for an augmenting path for a matching M by a BFS-like traversal of the
graph that starts simultaneously at all the free vertices in one of the sets V and U,
say, V . (It would be logical to select the smaller of the two vertex sets, but we will
ignore this observation in the pseudocode below.) Recall that an augmenting path,
if it exists, is an odd-length path that connects a free vertex in V with a free vertex
in U and which, unless it consists of a single edge, "zigs" from a vertex in V to
another vertex' mate in U , then "zags" back to V along the uniquely defined edge
from M, and so on until a free vertex in U is reached. (Draw augmenting paths
for the matchings in Figure 10.9, for example.) Hence, any candidate to be such a
     path must have its edges alternate in the pattern just described. This motivates the
     following rules for labeling vertices during the BFS-like traversal of the graph.
     Case 1 (the queue's front vertex w is in V )          If u is a free vertex adjacent to
     w, it is used as the other endpoint of an augmenting path; so the labeling stops
     and augmentation of the matching commences. The augmenting path in question
     is obtained by moving backward along the vertex labels (see below) to alternately
     add and delete its edges to and from the current matching. If u is not free and
     connected to w by an edge not in M, label u with w unless it has been already
     labeled.
     Case 2 (the front vertex w is in U )            In this case, w must be matched and we
     label its mate in V with w.
     Here is pseudocode of the algorithm in its entirety.
     ALGORITHM          MaximumBipartiteMatching(G)
     //Finds a maximum matching in a bipartite graph by a BFS-like traversal
     //Input: A bipartite graph G =      V, U, E
     //Output: A maximum-cardinality matching M in the input graph
     initialize set M of edges with some valid matching (e.g., the empty set)
     initialize queue Q with all the free vertices in V (in any order)
     while not Empty(Q) do
               w  Front(Q);       Dequeue(Q)
               if w  V
                     for every vertex u adjacent to w do
                        if u is free
                              //augment
                              M  M  (w, u)
                              vw
                              while v is labeled do
                                  u  vertex indicated by v's label;       M  M - (v, u)
                                  v  vertex indicated by u's label;       M  M  (v, u)
                              remove all vertex labels
                              reinitialize Q with all free vertices in V
                              break   //exit the for loop
                        else  //u is matched
                              if (w, u)  M and u is unlabeled
                                  label u with w
                                  Enqueue(Q, u)
               else  //w  U (and matched)
                     label the mate v of w with w
                     Enqueue(Q, v)
     return M           //current matching is maximum
         An application of this algorithm to the matching in Figure 10.9a is shown in
Figure 10.10. Note that the algorithm finds a maximum matching that differs from
the one in Figure 10.9d.
V     1     2           3              4         5        1        2             3              4      5
U     6     7           8              9        10        6        7             8              9     10
                Queue:     1  2  3                        1
                                                                      Queue:        1  2  3
                                                                                    
                                                                      Augment from 6
                                                       6                                     8
   1     2      3                   4        5         1        2             3              4     5
   6     7      8                   9        10        6        7             8              9     10
                Queue:     2  3                        2        1             3
                                                                Queue:     2  3     6  8  1  4
                                                                                          
                                                                      Augment from 7
                                                                6                            8
   1     2      3                   4        5         1        2             3              4     5
   6     7      8                   9        10        6        7             8              9     10
                Queue: 3                               3                      3              4     4
                                                                Queue:     3  6     8  2  4  9
                                                                                          
                                                                      Augment from 10
                           1              2         3        4          5
                           6              7         8        9        10
                                 Queue:      empty  maximum matching
FIGURE   10.10  Application of the maximum-cardinality-matching algorithm. The left
                column shows a current matching and initialized queue at the next
                iteration's start; the right column shows the vertex labeling generated
                by the algorithm before augmentation is performed. Matching edges are
                shown in bold. Vertex labels indicate the vertices from which the labeling
                is done. The discovered endpoint of an augmenting path is shaded and
                labeled for clarity. Vertices deleted from the queue are indicated by .
         How efficient is the maximum-matching algorithm? Each iteration except
     the last one matches two previously free vertices--one from each of the sets V
     and U. Therefore, the total number of iterations cannot exceed  n/2     + 1, where
     n = |V | + |U | is the number of vertices in the graph. The time spent on each
     iteration is in O(n + m), where m = |E| is the number of edges in the graph. (This
     assumes that the information about the status of each vertex--free or matched and
     the vertex' mate if the latter--can be retrieved in constant time, e.g., by storing it in
     an array.) Hence, the time efficiency of the algorithm is in O(n(n + m)). Hopcroft
     and Karp [Hop73] showed how the efficiency can be improved to O(        n(n + m))
     by combining several iterations into a single stage to maximize the number of
     edges added to the matching with one search.
         We were concerned in this section with matching the largest possible number
     of vertex pairs in a bipartite graph. Some applications may require taking into ac-
     count the quality or cost of matching different pairs. For example, workers may
     execute jobs with different efficiencies, or girls may have different preferences for
     their potential dance partners. It is natural to model such situations by bipartite
     graphs with weights assigned to their edges. This leads to the problem of maxi-
     mizing the sum of the weights on edges connecting matched pairs of vertices. This
     problem is called maximum-weight matching. We encountered it under a differ-
     ent name--the assignment problem--in Section 3.4. There are several sophisti-
     cated algorithms for this problem, which are much more efficient than exhaustive
     search (see, e.g., [Pap82], [Gal86], [Ahu93]). We have to leave them outside of our
     discussion, however, because of their complexity, especially for general graphs.
     Exercises 10.3
     1.  For each matching shown below in bold, find an augmentation or explain why
         no augmentation exists.
         a.                                        b.
         1      2              3     4             1      2               3                     4
             5              6     7                5      6               7                     8
     2.  Apply the maximum-matching algorithm to the following bipartite graph:
                                  1     2              3
                                  4     5              6
3.  a.  What is the largest and what is the smallest possible cardinality of a match-
        ing in a bipartite graph G =   V, U, E       with n vertices in each vertex set V
        and U and at least n edges?
    b.  What is the largest and what is the smallest number of distinct solutions
        the maximum-cardinality-matching problem can have for a bipartite graph
        G=     V, U, E  with n vertices in each vertex set V and U and at least n
        edges?
4.  a.  Hall's Marriage Theorem asserts that a bipartite graph G =             V, U, E  has a
        matching that matches all vertices of the set V if and only if for each subset
        S  V , |R(S)|  |S| where R(S) is the set of all vertices adjacent to a vertex
        in S. Check this property for the following graph with (i) V = {1, 2, 3, 4}
        and (ii) V = {5, 6, 7}.
                                 1           2       3           4
                                    5           6             7
    b.  You have to devise an algorithm that returns yes if there is a matching in
        a bipartite graph G =       V, U, E     that matches all vertices in V and returns
        no otherwise. Would you base your algorithm on checking the condition
        of Hall's Marriage Theorem?
5.  Suppose there are five committees A, B, C, D, and E composed of six persons
    a, b, c, d, e, and f as follows: committee A's members are b and e; committee
    B's members are b, d, and e; committee C's members are a, c, d, e, and f ;
    committee D's members are b, d, and e; committee E's members are b and
    e. Is there a system of distinct representatives, i.e., is it possible to select
    a representative from each committee so that all the selected persons are
    distinct?
6.  Show how the maximum-cardinality-matching problem for a bipartite graph
    can be reduced to the maximum-flow problem discussed in Section 10.2.
7.  Consider the following greedy algorithm for finding a maximum matching
    in  a bipartite graph  G=       V , U, E . Sort  all the     vertices  in  nondecreasing
    order of their degrees. Scan this sorted list to add to the current matching
    (initially empty) the edge from the list's free vertex to an adjacent free vertex
    of the lowest degree. If the list's vertex is matched or if there are no adjacent
    free vertices for it, the vertex is simply skipped. Does this algorithm always
    produce a maximum matching in a bipartite graph?
8.  Design a linear-time algorithm for finding a maximum matching in a tree.
9.  Implement the maximum-matching algorithm of this section in the language
    of your choice. Experiment with its performance on bipartite graphs with n
    vertices in each of the vertex sets and randomly generated edges (in both
                 dense and sparse modes) to compare the observed running time with the
                 algorithm's theoretical efficiency.
           10.   Domino puzzle     A domino is a 2 × 1 tile that can be oriented either hori-
                 zontally or vertically. A tiling of a given board composed of 1 × 1 squares is
                 covering it with dominoes exactly and without overlap. Is it possible to tile with
                 dominoes an 8 × 8 board without two unit squares at its diagonally opposite
                 corners?
     10.4  The Stable Marriage Problem
           In this section, we consider an interesting version of bipartite matching called the
           stable marriage problem. Consider a set Y = {m1, m2, . . . , mn} of n men and a
           set X = {w1, w2, . . . , wn} of n women. Each man has a preference list ordering
           the women as potential marriage partners with no ties allowed. Similarly, each
           woman has a preference list of the men, also with no ties. Examples of these two
           sets of lists are given in Figures 10.11a and 10.11b. The same information can also
           be presented by an n × n ranking matrix (see Figure 10.11c). The rows and columns
           of the matrix represent the men and women of the two sets, respectively. A cell
           in row m and column w contains two rankings: the first is the position (ranking)
           of w in the m's preference list; the second is the position (ranking) of m in the w's
           preference list. For example, the pair 3, 1 in Jim's row and Ann's column in the
           matrix in Figure 10.11c indicates that Ann is Jim's third choice while Jim is Ann's
           first. Which of these two ways to represent such information is better depends on
           the task at hand. For example, it is easier to specify a match of the sets' elements
           by using the ranking matrix, whereas the preference lists might be a more efficient
           data structure for implementing a matching algorithm.
                 A marriage matching M is a set of n (m, w) pairs whose members are selected
           from disjoint n-element sets Y and X in a one-one fashion, i.e., each man m from
           Y is paired with exactly one woman w from X and vice versa. (If we represent
           Y and X as vertices of a complete bipartite graph with edges connecting possible
           marriage partners, then a marriage matching is a perfect matching in such a graph.)
                men's preferences      women's preferences                ranking matrix
                 1st       2nd    3rd        1st           2nd  3rd       Ann       Lea             Sue
           Bob:  Lea       Ann    Sue  Ann:  Jim           Tom  Bob  Bob  2,3       1,2             3,3
           Jim:  Lea       Sue    Ann  Lea:  Tom           Bob  Jim  Jim  3,1       1,3             2,1
           Tom:  Sue       Lea    Ann  Sue:  Jim           Tom  Bob  Tom  3,2       2,1              1,2
                      (a)                             (b)                      (c)
           FIGURE 10.11    Data for an instance of the stable marriage problem. (a) Men's preference
                           lists; (b) women's preference lists. (c) Ranking matrix (with the boxed
                           cells composing an unstable matching).
A pair (m, w), where m  Y, w  X, is said to be a blocking pair for a marriage
matching M if man m and woman w are not matched in M but they prefer each
other to their mates in M. For example, (Bob, Lea) is a blocking pair for the
marriage matching M = {(Bob, Ann), (Jim, Lea), (Tom, Sue)} (Figure 10.11c)
because they are not matched in M while Bob prefers Lea to Ann and Lea
prefers Bob to Jim. A marriage matching M is called stable if there is no blocking
pair  for  it;  otherwise,  M  is  called  unstable.  According  to  this  definition,  the
marriage matching in Figure 10.11c is unstable because Bob and Lea can drop their
designated mates to join in a union they both prefer. The stable marriage problem
is to find a stable marriage matching for men's and women's given preferences.
      Surprisingly, this problem always has a solution. (Can you find it for the
instance in Figure 10.11?) It can be found by the following algorithm.
Stable marriage algorithm
      Input: A set of n men and a set of n women along with rankings of the women
              by each man and rankings of the men by each woman with no ties
              allowed in the rankings
      Output: A stable marriage matching
      Step 0    Start with all the men and women being free.
      Step 1    While there are free men, arbitrarily select one of them and do the
                following:
                Proposal       The selected free man m proposes to w, the next
                woman on his preference list (who is the highest-ranked woman
                who has not rejected him before).
                Response       If w is free, she accepts the proposal to be matched
                with m. If she is not free, she compares m with her current mate. If
                she prefers m to him, she accepts m's proposal, making her former
                mate free; otherwise, she simply rejects m's proposal, leaving m
                free.
      Step 2    Return the set of n matched pairs.
      Before we analyze this algorithm, it is useful to trace it on some input. Such
an example is presented in Figure 10.12.
      Let us discuss properties of the stable marriage algorithm.
THEOREM         The stable marriage algorithm terminates after no more than n2
iterations with a stable marriage output.
PROOF         The algorithm starts with n men having the total of n2 women on their
ranking lists. On each iteration, one man makes a proposal to a woman. This
reduces the total number of women to whom the men can still propose in the
future because no man proposes to the same woman more than once. Hence, the
algorithm must stop after no more than n2 iterations.
                                 Ann   Lea   Sue
     Free men:              Bob  2, 3  1,2   3, 3  Bob proposed to Lea
     Bob, Jim, Tom          Jim  3, 1  1, 3  2, 1  Lea accepted
                            Tom  3, 2  2, 1  1, 2
                                 Ann   Lea   Sue
     Free men:              Bob  2, 3  1,2   3, 3  Jim proposed to Lea
     Jim, Tom               Jim  3, 1  1, 3  2, 1  Lea rejected
                            Tom  3, 2  2, 1  1, 2
                                 Ann   Lea   Sue
     Free men:              Bob  2, 3  1,2   3, 3  Jim proposed to Sue
     Jim, Tom               Jim  3, 1  1, 3  2,1   Sue accepted
                            Tom  3, 2  2, 1  1, 2
                                 Ann   Lea   Sue
     Free men:              Bob  2, 3  1,2   3, 3  Tom proposed to Sue
     Tom                    Jim  3, 1  1, 3  2,1   Sue rejected
                            Tom  3, 2  2, 1  1, 2
                                 Ann   Lea   Sue
     Free men:              Bob  2, 3  1, 2  3, 3  Tom proposed to Lea
     Tom                    Jim  3, 1  1, 3  2,1   Lea replaced Bob with Tom
                            Tom  3, 2  2,1   1, 2
                                 Ann   Lea   Sue
     Free men:              Bob  2,3   1, 2  3, 3  Bob proposed to Ann
     Bob                    Jim  3, 1  1, 3  2,1   Ann accepted
                            Tom  3, 2  2,1   1, 2
     FIGURE 10.12  Application of the stable marriage algorithm. An accepted proposal is
                   indicated by a boxed cell; a rejected proposal is shown by an underlined
                   cell.
     Let us now prove that the final matching M is a stable marriage matching.
     Since the algorithm stops after all the n men are one-one matched to the n women,
     the only thing that needs to be proved is the stability of M. Suppose, on the
     contrary, that M is unstable. Then there exists a blocking pair of a man m and a
     woman w who are unmatched in M and such that both m and w prefer each other
     to the persons they are matched with in M. Since m proposes to every woman on
     his ranking list in decreasing order of preference and w precedes m's match in M,
     m must have proposed to w on some iteration. Whether w refused m's proposal or
     accepted it but replaced him on a subsequent iteration with a higher-ranked match,
     w's mate in M must be higher on w's preference list than m because the rankings
     of the men matched to a given woman may only improve on each iteration of the
     algorithm. This contradicts the assumption that w prefers m to her final match
     in M.
     The stable marriage algorithm has a notable shortcoming. It is not "gender
     neutral." In the form presented above, it favors men's preferences over women's
preferences. We can easily see this by tracing the algorithm on the following
instance of the problem:
                                   woman 1        woman 2
                          man 1    1, 2           2, 1
                          man 2    2, 1           1, 2
The algorithm obviously yields the stable matching M = {(man 1, woman 1), (man
2, woman 2)}. In this matching, both men are matched to their first choices, which
is not the case for the women. One can prove that the algorithm always yields a
stable matching that is man-optimal: it assigns to each man the highest-ranked
woman possible under any stable marriage. Of course, this gender bias can be
reversed, but not eliminated, by reversing the roles played by men and women
in the algorithm, i.e., by making women propose and men accept or reject their
proposals.
    There is another important corollary to the fact that the stable marriage
algorithm always yields a gender-optimal stable matching. It is easy to prove
that a man (woman)-optimal matching is unique for a given set of participant
preferences. Therefore the algorithm's output does not depend on the order in
which the free men (women) make their proposals. Consequently, we can use any
data structure we might prefer--e.g., a queue or a stack--for representing this set
with no impact on the algorithm's outcome.
    The notion of the stable matching as well as the algorithm discussed above was
introduced by D. Gale and L. S. Shapley in the paper titled "College Admissions
and the Stability of Marriage" [Gal62]. I do not know which of the two applications
mentioned in the title you would consider more important. The point is that
stability is a matching property that can be desirable in a variety of applications.
For example, it has been used for many years in the United States for matching
medical-school graduates with hospitals for residency training. For a brief history
of this application and an in-depth discussion of the stable marriage problem and
its extensions, see the monograph by Gusfield and Irwing [Gus89].
Exercises 10.4
1.  Consider an instance of the stable marriage problem given by the following
    ranking matrix:
                                   A        B     C
                                   1, 3     2, 2  3, 1
                                   3, 1     1, 3  2, 2
                                   2, 2     3, 1  1, 3
    For each of its marriage matchings, indicate whether it is stable or not. For the
    unstable matchings, specify a blocking pair. For the stable matchings, indicate
    whether they are man-optimal, woman-optimal, or neither. (Assume that the
    Greek and Roman letters denote the men and women, respectively.)
     2.   Design a simple algorithm for checking whether a given marriage matching is
          stable and determine its time efficiency class.
     3.   Find a stable marriage matching for the instance given in Problem 1 by apply-
          ing the stable marriage algorithm
          a.  in its men-proposing version.
          b. in its women-proposing version.
     4.   Find a stable marriage matching for the instance defined by the following
          ranking matrix:
                                    A         B      C     D
                                    1, 3      2, 3   3, 2  4, 3
                                    1, 4      4, 1   3, 4  2, 2
                                    2, 2      1, 4   3, 3  4, 1
                                    4, 1      2, 2   3, 1  1, 4
     5.   Determine the time-efficiency class of the stable marriage algorithm
          a.  in the worst case.
          b. in the best case.
     6.   Prove that a man-optimal stable marriage set is always unique. Is it also true
          for a woman-optimal stable marriage matching?
     7.   Prove that in the man-optimal stable matching, each woman has the worst
          partner that she can have in any stable marriage matching.
     8.   Implement the stable-marriage algorithm given in Section 10.4 so that its
          running time is in O(n2). Run an experiment to ascertain its average-case
          efficiency.
     9.   Write a report on the college admission problem (residents-hospitals assign-
          ment) that generalizes the stable marriage problem in that a college can accept
          "proposals" from more than one applicant.
     10.  Consider the problem of the roommates, which is related to but more difficult
          than the stable marriage problem: "An even number of boys wish to divide up
          into pairs of roommates. A set of pairings is called stable if under it there are
          no two boys who are not roommates and who prefer each other to their actual
          roommates." [Gal62] Give an instance of this problem that does not have a
          stable pairing.
     SUMMARY
          The iterative-improvement technique involves finding a solution to an op-
          timization problem by generating a sequence of feasible solutions with
          improving values of the problem's objective function. Each subsequent so-
          lution in such a sequence typically involves a small, localized change in the
          previous feasible solution. When no such change improves the value of the
objective function, the algorithm returns the last feasible solution as optimal
and stops.
Important problems that can be solved exactly by iterative-improvement
algorithms include linear programming, maximizing the flow in a network,
and matching the maximum possible number of vertices in a graph.
The simplex method is the classic method for solving the general linear
programming problem. It works by generating a sequence of adjacent extreme
points of the problem's feasible region with improving values of the objective
function.
The maximum-flow problem asks to find the maximum flow possible in a
network, a weighted directed graph with a source and a sink.
The Ford-Fulkerson method is a classic template for solving the maximum-
flow problem by the iterative-improvement approach. The shortest-
augmenting-path method implements this idea by labeling network vertices
in the breadth-first search manner.
The Ford-Fulkerson method also finds a minimum cut in a given network.
A maximum cardinality matching is the largest subset of edges in a graph
such that no two edges share the same vertex. For a bipartite graph, it can be
found by a sequence of augmentations of previously obtained matchings.
The stable marriage problem is to find a stable matching for elements of two n-
element sets based on given matching preferences. This problem always has
a solution that can be found by the Gale-Shapley algorithm.

