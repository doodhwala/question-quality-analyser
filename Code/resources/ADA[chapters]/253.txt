Space and Time Trade-Offs
Things which matter most must never be at the mercy of things which
       matter less.
                     --Johann Wolfgang von Go¨ ethe (1749­1832)
       Space and time trade-offs in algorithm design are a well-known issue for both
       theoreticians and practitioners of computing. Consider, as an example, the
       problem of computing values of a function at many points in its domain. If it is
       time that is at a premium, we can precompute the function's values and store them
       in a table. This is exactly what human computers had to do before the advent of
       electronic computers, in the process burdening libraries with thick volumes of
       mathematical tables. Though such tables have lost much of their appeal with the
       widespread use of electronic computers, the underlying idea has proven to be quite
       useful in the development of several important algorithms for other problems.
       In somewhat more general terms, the idea is to preprocess the problem's input,
       in whole or in part, and store the additional information obtained to accelerate
       solving the problem afterward. We call this approach input enhancement1 and
       discuss the following algorithms based on it:
       counting methods for sorting (Section 7.1)
       Boyer-Moore algorithm for string matching and its simplified version sug-
       gested by Horspool (Section 7.2)
       The other type of technique that exploits space-for-time trade-offs simply uses
       extra space to facilitate faster and/or more flexible access to the data. We call this
       approach prestructuring. This name highlights two facets of this variation of the
       space-for-time trade-off: some processing is done before a problem in question
   1.  The standard terms used synonymously for this technique are preprocessing and preconditioning.
       Confusingly, these terms can also be applied to methods that use the idea of preprocessing but do not
       use extra space (see Chapter 6). Thus, in order to avoid confusion, we use "input enhancement" as a
       special name for the space-for-time trade-off technique being discussed here.
                                                                                                              253
          is actually solved but, unlike the input-enhancement variety, it deals with access
          structuring. We illustrate this approach by:
          hashing (Section 7.3)
          indexing with B-trees (Section 7.4)
          There is one more algorithm design technique related to the space-for-time
          trade-off idea: dynamic programming. This strategy is based on recording solu-
          tions to overlapping subproblems of a given problem in a table from which a solu-
          tion to the problem in question is then obtained. We discuss this well-developed
          technique separately, in the next chapter of the book.
          Two final comments about the interplay between time and space in algo-
          rithm design need to be made. First, the two resources--time and space--do not
          have to compete with each other in all design situations. In fact, they can align to
          bring an algorithmic solution that minimizes both the running time and the space
          consumed. Such a situation arises, in particular, when an algorithm uses a space-
          efficient data structure to represent a problem's input, which leads, in turn, to a
          faster algorithm. Consider, as an example, the problem of traversing graphs. Re-
          call that the time efficiency of the two principal traversal algorithms--depth-first
          search and breadth-first search--depends on the data structure used for repre-
          senting graphs: it is  (n2) for the adjacency matrix representation and  (n + m)
          for the adjacency list representation, where n and m are the numbers of vertices
          and edges, respectively. If input graphs are sparse, i.e., have few edges relative to
          the number of vertices (say, m  O(n)), the adjacency list representation may well
          be more efficient from both the space and the running-time points of view. The
          same situation arises in the manipulation of sparse matrices and sparse polynomi-
          als: if the percentage of zeros in such objects is sufficiently high, we can save both
          space and time by ignoring zeros in the objects' representation and processing.
          Second, one cannot discuss space-time trade-offs without mentioning the
          hugely important area of data compression. Note, however, that in data compres-
          sion, size reduction is the goal rather than a technique for solving another problem.
          We discuss just one data compression algorithm, in the next chapter. The reader
          interested in this topic will find a wealth of algorithms in such books as [Say05].
     7.1  Sorting by Counting
          As a first example of applying the input-enhancement technique, we discuss its
          application to the sorting problem. One rather obvious idea is to count, for each
          element of a list to be sorted, the total number of elements smaller than this
          element and record the results in a table. These numbers will indicate the positions
          of the elements in the sorted list: e.g., if the count is 10 for some element, it should
          be in the 11th position (with index 10, if we start counting with 0) in the sorted
          array. Thus, we will be able to sort the list by simply copying its elements to their
          appropriate positions in a new, sorted list. This algorithm is called comparison-
          counting sort (Figure 7.1).
             Array A[0..5]                            62      31     84  96      19       47
             Initially                  Count []       0      0      0       0      0     0
             After pass i     =0        Count []       3      0      1       1      0     0
             After pass i     =1        Count []              1      2       2      0     1
             After pass i     =2        Count []                     4       3      0     1
             After pass i     =3        Count []                             5      0     1
             After pass i     =4        Count []                                    0     2
             Final state                Count []       3      1      4       5      0     2
             Array S[0..5]                            19      31     47  62      84       96
FIGURE  7.1  Example of sorting         by comparison         counting.
ALGORITHM    ComparisonCountingSort(A[0..n - 1])
//Sorts an array by comparison counting
//Input: An array A[0..n - 1] of orderable elements
//Output: Array S[0..n - 1] of A's elements sorted in                            nondecreasing        order
for i  0 to n - 1 do Count[i]  0
for i  0 to n - 2 do
        for j  i + 1 to n - 1 do
             if A[i] < A[j ]
             Count[j ]  Count[j ] + 1
             else Count[i]  Count[i] + 1
for i  0 to n - 1 do S[Count[i]]  A[i]
return S
What is the time efficiency of this algorithm? It should be quadratic because
the algorithm considers all the different pairs of an n-element array. More formally,
the number of times its basic operation, the comparison A[i] < A[j ], is executed
is equal to the sum we have encountered several times already:
        n-2  n-1              n-2                                       n-2                      n(n  -  1)
C(n) =                  1  =       [(n  -  1)  -  (i  +   1)  +  1]  =       (n  -  1  -  i)  =       2      .
        i=0 j =i+1            i=0                                       i=0
Thus, the algorithm makes the same number of key comparisons as selection sort
and in addition uses a linear amount of extra space. On the positive side, the
algorithm makes the minimum number of key moves possible, placing each of
them directly in their final position in a sorted array.
The counting idea does work productively in a situation in which elements
to be sorted belong to a known small set of values. Assume, for example, that
we have to sort a list whose values can be either 1 or 2. Rather than applying a
general sorting algorithm, we should be able to take advantage of this additional
     information about values to be sorted. Indeed, we can scan the list to compute
     the number of 1's and the number of 2's in it and then, on the second pass,
     simply make the appropriate number of the first elements equal to 1 and the
     remaining elements equal to 2. More generally, if element values are integers
     between some lower bound l and upper bound u, we can compute the frequency
     of each of those values and store them in array F [0..u - l]. Then the first F [0]
     positions in the sorted list must be filled with l, the next F [1] positions with l + 1,
     and so on. All this can be done, of course, only if we can overwrite the given
     elements.
     Let us consider a more realistic situation of sorting a list of items with some
     other information associated with their keys so that we cannot overwrite the list's
     elements. Then we can copy elements into a new array S[0..n - 1]to hold the sorted
     list as follows. The elements of A whose values are equal to the lowest possible
     value l are copied into the first F [0] elements of S, i.e., positions 0 through F [0] - 1;
     the elements of value l + 1 are copied to positions from F [0] to (F [0] + F [1]) - 1;
     and so on.   Since such accumulated sums of frequencies are called a distribution
     in statistics, the method itself is known as distribution counting.
     EXAMPLE      Consider sorting the array
                                13  11  12    13      12  12
     whose values are known to come from the set {11, 12, 13} and should not be
     overwritten in the process of sorting. The frequency and distribution arrays are
     as follows:
                  Array values                    11      12  13
                  Frequencies                     1       3   2
                  Distribution values             1       4   6
     Note that the distribution values indicate the proper positions for the last occur-
     rences of their elements in the final sorted array. If we index array positions from 0
     to n - 1, the distribution values must be reduced by 1 to get corresponding element
     positions.
     It is more convenient to process the input array right to left. For the example,
     the last element is 12, and, since its distribution value is 4, we place this 12 in
     position 4 - 1 = 3 of the array S that will hold the sorted list. Then we decrease
     the 12's distribution value by 1 and proceed to the next (from the right) element
     in the given array. The entire processing of this example is depicted in Figure 7.2.
                              D [0..2]                          S [0..5]
              A [5] = 12  1          4      6                       12
              A [4] = 12  1          3      6                   12
              A [3] = 13  1          2      6                                 13
              A [2] = 12  1          2      5               12
              A [1] = 11  1          1      5      11
              A [0] = 13  0          1      5                             13
FIGURE  7.2   Example of sorting by distribution        counting. The     distribution  values  being
              decremented are shown in bold.
    Here is pseudocode of this algorithm.
ALGORITHM     DistributionCountingSort(A[0..n - 1], l, u)
    //Sorts an array of integers from a limited range by distribution counting
    //Input: An array A[0..n - 1] of integers between l and u (l  u)
    //Output: Array S[0..n - 1] of A's elements sorted in nondecreasing order
    for j  0 to u - l do D[j ]  0                                   //initialize frequencies
    for i  0 to n - 1 do D[A[i] - l]  D[A[i] - l] + 1 //compute frequencies
    for j  1 to u - l do D[j ]  D[j - 1] + D[j ]                    //reuse for distribution
    for i  n - 1 downto 0 do
        j  A[i] - l
        S[D[j ] - 1]  A[i]
        D[j ]  D[j ] - 1
    return S
    Assuming that the range of array values is fixed, this is obviously a linear
algorithm because it makes just two consecutive passes through its input array
A. This is a better time-efficiency class than that of the most efficient sorting
algorithms--mergesort, quicksort, and heapsort--we have encountered. It is im-
portant to remember, however, that this efficiency is obtained by exploiting the
specific nature of inputs for which sorting by distribution counting works, in addi-
tion to trading space for time.
Exercises 7.1
1.  Is it possible to exchange numeric values of two variables, say, u and v, without
    using any extra storage?
2.  Will the comparison-counting algorithm work correctly for arrays with equal
    values?
3.  Assuming that the set of possible list values is {a, b, c, d}, sort the following
    list in alphabetical order by the distribution-counting algorithm:
                                 b,     c,     d,  c,   b,  a,  a,  b.
          4.   Is the distribution-counting algorithm stable?
          5.   Design a one-line algorithm for sorting any array of size n whose values are n
               distinct integers from 1 to n.
          6.   The ancestry problem asks to determine whether a vertex u is an ancestor
               of vertex v in a given binary (or, more generally, rooted ordered) tree of n
               vertices. Design a O(n) input-enhancement algorithm that provides sufficient
               information to solve this problem for any pair of the tree's vertices in constant
               time.
          7.   The following technique, known as virtual initialization, provides a time-
               efficient way to initialize just some elements of a given array A[0..n - 1] so
               that for each of its elements, we can say in constant time whether it has been
               initialized and, if it has been, with which value. This is done by utilizing a
               variable counter for the number of initialized elements in A and two auxiliary
               arrays of the same size, say B[0..n - 1] and C[0..n - 1], defined as follows.
               B[0], . . . , B[counter - 1] contain the indices of the elements of A that were
               initialized: B[0] contains the index of the element initialized first, B[1] contains
               the index of the element initialized second, etc. Furthermore, if A[i] was the
               kth element (0  k  counter - 1) to be initialized, C[i] contains k.
               a.  Sketch the state of arrays A[0..7], B[0..7], and C[0..7] after the three as-
                   signments
                                      A[3]  x;     A[7]  z;            A[1]  y.
               b. In general, how can we check with this scheme whether A[i] has been
                   initialized and, if it has been, with which value?
          8.   Least distance sorting  There are 10 Egyptian stone statues standing in a row
               in an art gallery hall. A new curator wants to move them so that the statues
               are ordered by their height. How should this be done to minimize the total
               distance that the statues are moved? You may assume for simplicity that all
               the statues have different heights. [Azi10]
          9.   a.  Write a program for multiplying two sparse matrices, a p × q matrix A and
                   a q × r matrix B.
               b. Write a program for multiplying two sparse polynomials p(x) and q(x) of
                   degrees m and n, respectively.
          10.  Is it a good idea to write a program that plays the classic game of tic-tac-toe
               with the human user by storing all possible positions on the game's 3 × 3 board
               along with the best move for each of them?
     7.2  Input Enhancement in String Matching
          In this section, we see how the technique of input enhancement can be applied
          to the problem of string matching. Recall that the problem of string matching
requires finding an occurrence of a given string of m characters called the pattern
in a longer string of n characters called the text. We discussed the brute-force
algorithm for this problem in Section 3.2: it simply matches corresponding pairs
of characters in the pattern and the text left to right and, if a mismatch occurs,
shifts the pattern one position to the right for the next trial. Since the maximum
number of such trials is n - m + 1 and, in the worst case, m comparisons need to
be made on each of them, the worst-case efficiency of the brute-force algorithm is
in the O(nm) class. On average, however, we should expect just a few comparisons
before a pattern's shift, and for random natural-language texts, the average-case
efficiency indeed turns out to be in O(n + m).
Several faster algorithms have been discovered. Most of them exploit the
input-enhancement idea: preprocess the pattern to get some information about
it, store this information in a table, and then use this information during an actual
search for the pattern in a given text. This is exactly the idea behind the two best-
known algorithms of this type: the Knuth-Morris-Pratt algorithm [Knu77] and the
Boyer-Moore algorithm [Boy77].
The principal difference between these two algorithms lies in the way they
compare characters of a pattern with their counterparts in a text: the Knuth-
Morris-Pratt algorithm does it left to right, whereas the Boyer-Moore algorithm
does it right to left. Since the latter idea leads to simpler algorithms, it is the
only one that we will pursue here. (Note that the Boyer-Moore algorithm starts
by aligning the pattern against the beginning characters of the text; if the first
trial fails, it shifts the pattern to the right. It is comparisons within a trial that the
algorithm does right to left, starting with the last character in the pattern.)
Although the underlying idea of the Boyer-Moore algorithm is simple, its
actual implementation in a working method is less so. Therefore, we start our
discussion with a simplified version of the Boyer-Moore algorithm suggested by
R. Horspool [Hor80]. In addition to being simpler, Horspool's algorithm is not
necessarily less efficient than the Boyer-Moore algorithm on random strings.
Horspool's Algorithm
Consider, as an example, searching for the pattern BARBER in some text:
                     s0   ...                   c  ...  sn-1
                                B  A  R  B  E   R
Starting with the last R of the pattern and moving right to left, we compare the
corresponding pairs of characters in the pattern and the text. If all the pattern's
characters match successfully, a matching substring is found. Then the search
can be either stopped altogether or continued if another occurrence of the same
pattern is desired.
If a mismatch occurs, we need to shift the pattern to the right. Clearly, we
would like to make as large a shift as possible without risking the possibility of
missing a matching substring in the text. Horspool's algorithm determines the size
     of such a shift by looking at the character c of the text that is aligned against the
     last character of the pattern. This is the case even if character c itself matches its
     counterpart in the pattern.
     In general, the following four possibilities can occur.
     Case 1     If there are no c's in the pattern--e.g., c is letter S in our example--
     we can safely shift the pattern by its entire length (if we shift less, some character
     of the pattern would be aligned against the text's character c that is known not to
     be in the pattern):
                s0        ...                  S                        ...  sn-1
                               B   A  R  B  E  R
                                                  B  A  R  B    E  R
     Case 2 If there are occurrences of character c in the pattern but it is not the last
     one there--e.g., c is letter B in our example--the shift should align the rightmost
     occurrence of c in the pattern with the c in the text:
                    s0    ...                     B                   ...    sn-1
                                   B  A  R  B  E  R
                                         B  A  R  B  E  R
     Case 3 If c happens to be the last character in the pattern but there are no c's
     among its other m - 1 characters--e.g., c is letter R in our example--the situation
     is similar to that of Case 1 and the pattern should be shifted by the entire pattern's
     length m:
                s0        ...            M  E  R                        ...  sn-1
                               L   E  A  D  E  R
                                                  L  E  A  D    E  R
     Case 4 Finally, if c happens to be the last character in the pattern and there
     are other c's among its first m - 1 characters--e.g., c is letter R in our example--
     the situation is similar to that of Case 2 and the rightmost occurrence of c among
     the first m - 1 characters in the pattern should be aligned with the text's c:
                s0        ...                  A  R                ...     sn-1
                                R  E  O  R  D  E  R
                                         R  E  O  R  D  E    R
     These examples clearly demonstrate that right-to-left character comparisons
     can lead to farther shifts of the pattern than the shifts by only one position
always made by the brute-force algorithm. However, if such an algorithm had
to check all the characters of the pattern on every trial, it would lose much
of this superiority. Fortunately, the idea of input enhancement makes repetitive
comparisons unnecessary. We can precompute shift sizes and store them in a table.
The table will be indexed by all possible characters that can be encountered in a
text, including, for natural language texts, the space, punctuation symbols, and
other special characters. (Note that no other information about the text in which
eventual searching will be done is required.) The table's entries will indicate the
shift sizes computed by the formula
             the   pattern's length m,
           
             if c  is not among the first  m  -  1  characters  of  the    pattern;
t  (c)  =                                                                            (7.1)
             the distance from the rightmost c among the            first  m  -  1  characters
             of the pattern to its last character, otherwise.
For example, for the pattern BARBER, all the table's entries will be equal to 6, except
for the entries for E, B, R, and A, which will be 1, 2, 3, and 4, respectively.
   Here is a simple algorithm for computing the shift table entries. Initialize all
the entries to the pattern's length m and scan the pattern left to right repeating the
following step m - 1 times: for the j th character of the pattern (0  j  m - 2),
overwrite its entry in the table with m - 1 - j , which is the character's distance to
the last character of the pattern. Note that since the algorithm scans the pattern
from left to right, the last overwrite will happen for the character's rightmost
occurrence--exactly as we would like it to be.
ALGORITHM          ShiftTable(P [0..m - 1])
   //Fills the shift table used by Horspool's and Boyer-Moore algorithms
   //Input: Pattern P [0..m - 1] and an alphabet of possible characters
   //Output: Table[0..size - 1] indexed by the alphabet's characters and
   //        filled with shift sizes computed by formula (7.1)
   for i  0 to size - 1 do Table[i]  m
   for j  0 to m - 2 do Table[P [j ]]  m - 1 - j
   return Table
   Now, we can summarize the algorithm as follows:
Horspool's algorithm
   Step 1 For a given pattern of length m and the alphabet used in both the
             pattern and text, construct the shift table as described above.
   Step 2 Align the pattern against the beginning of the text.
   Step 3    Repeat the following until either a matching substring is found or the
             pattern reaches beyond the last character of the text. Starting with the
             last character in the pattern, compare the corresponding characters in
             the pattern and text until either all m characters are matched (then
                  stop) or a mismatching pair is encountered. In the latter case, retrieve
                  the entry t (c) from the c's column of the shift table where c is the text's
                  character currently aligned against the last character of the pattern,
                  and shift the pattern by t (c) characters to the right along the text.
     Here is pseudocode of Horspool's algorithm.
     ALGORITHM           HorspoolMatching(P [0..m - 1], T [0..n - 1])
     //Implements Horspool's algorithm for string matching
     //Input: Pattern P [0..m - 1] and text T [0..n - 1]
     //Output: The index of the left end of the first matching substring
     //              or -1 if there are no matches
     ShiftTable(P [0..m - 1])                 //generate Table of shifts
     i m-1                                    //position of the pattern's right end
     while i  n - 1 do
               k0                             //number of matched characters
               while k  m - 1 and P [m - 1 - k] = T [i - k] do
                  kk+1
               if k = m
                  return i - m + 1
               else i  i + Table[T [i]]
     return -1
     EXAMPLE         As an example of a complete application of Horspool's algorithm,
     consider searching for the pattern BARBER in a text that comprises English letters
     and spaces (denoted by underscores). The shift table, as we mentioned, is filled as
     follows:
               character c        A  B     C     D     E     F     ...     R     ...     Z     _
                  shift t (c)     4  2     6     6     1     6     6       3     6       6     6
     The actual search in a particular text proceeds as follows:
     J   I     M  _  S   A     W  _  M  E  _  I     N  _  A  _  B  A    R  B  E  R    S  H  O  P
     B   A     R  B  E   R                    B     A  R  B  E  R
                     B   A     R  B  E  R              B  A  R  B  E    R
                         B     A  R  B  E  R                    B  A    R  B  E  R
     A simple example can demonstrate that the worst-case efficiency of Hor-
     spool's algorithm is in O(nm) (Problem 4 in this section's exercises). But for
     random texts, it is in       (n), and, although in the same efficiency class, Horspool's
     algorithm is obviously faster on average than the brute-force algorithm. In fact,
     as mentioned, it is often at least as efficient as its more sophisticated predecessor
     discovered by R. Boyer and J. Moore.
Boyer-Moore Algorithm
Now we outline the Boyer-Moore algorithm itself. If the first comparison of the
rightmost character in the pattern with the corresponding character c in the text
fails, the algorithm does exactly the same thing as Horspool's algorithm. Namely,
it shifts the pattern to the right by the number of characters retrieved from the
table precomputed as explained earlier.
The two algorithms act differently, however, after some positive number k
(0 < k < m) of the pattern's characters are matched successfully before a mismatch
is encountered:
s0  ...                           c           si-k+1            ...               si  ...   sn-1  text
    p0               ...   pm-k-1                pm-k           ...         pm-1                  pattern
In this situation, the Boyer-Moore algorithm determines the shift size by consid-
ering two quantities. The first one is guided by the text's character c that caused
a mismatch with its counterpart in the pattern. Accordingly, it is called the bad-
symbol shift. The reasoning behind this shift is the reasoning we used in Hor-
spool's algorithm. If c is not in the pattern, we shift the pattern to just pass this
c in the text. Conveniently, the size of this shift can be computed by the formula
t1(c) - k where t1(c) is the entry in the precomputed table used by Horspool's
algorithm (see above) and k is the number of matched characters:
s0  ...                        c           si-k+1            ...               si     ...   sn-1  text
    p0           ...      pm-k-1              pm-k           ...            pm-1                  pattern
                                                 p0          ...                      pm-1
For example, if we search for the pattern BARBER in some text and match the last
two characters before failing on letter S in the text, we can shift the pattern by
t1(S) - 2 = 6 - 2 = 4 positions:
                 s0   ...                        S     E     R                        ...   sn-1
                               B     A     R     B     E     R
                                                       B     A     R     B     E   R
The same formula can also be used when the mismatching character c of the
text occurs in the pattern, provided t1(c) - k > 0. For example, if we search for the
pattern BARBER in some text and match the last two characters before failing on
letter A, we can shift the pattern by t1(A) - 2 = 4 - 2 = 2 positions:
                 s0       ...                       A     E     R                     ...   sn-1
                                  B     A     R     B     E     R
                                              B     A     R     B     E     R
     If t1(c) - k  0, we obviously do not want to shift the pattern by 0 or a negative
     number of positions. Rather, we can fall back on the brute-force thinking and
     simply shift the pattern by one position to the right.
     To summarize, the bad-symbol shift d1 is computed by the Boyer-Moore
     algorithm either as t1(c) - k if this quantity is positive and as 1 if it is negative
     or zero. This can be expressed by the following compact formula:
                                   d1 = max{t1(c) - k, 1}.                            (7.2)
     The second type of shift is guided by a successful match of the last k > 0
     characters of the pattern. We refer to the ending portion of the pattern as its suffix
     of size k and denote it suff (k). Accordingly, we call this type of shift the good-suffix
     shift. We now apply the reasoning that guided us in filling the bad-symbol shift
     table, which was based on a single alphabet character c, to the pattern's suffixes
     of sizes 1, . . . , m - 1 to fill in the good-suffix shift table.
     Let us first consider the case when there is another occurrence of suff (k) in
     the pattern or, to be more accurate, there is another occurrence of suff (k) not
     preceded by the same character as in its rightmost occurrence. (It would be useless
     to shift the pattern to match another occurrence of suff (k) preceded by the same
     character because this would simply repeat a failed trial.) In this case, we can shift
     the pattern by the distance d2 between such a second rightmost occurrence (not
     preceded by the same character as in the rightmost occurrence) of suff (k) and its
     rightmost occurrence. For example, for the pattern ABCBAB, these distances for
     k = 1 and 2 will be 2 and 4, respectively:
                                      k     pattern         d2
                                      1     ABCBAB          2
                                      2     ABCBAB          4
     What is to be done if there is no other occurrence of suff (k) not preceded by
     the same character as in its rightmost occurrence? In most cases, we can shift the
     pattern by its entire length m. For example, for the pattern DBCBAB and k = 3, we
     can shift the pattern by its entire length of 6 characters:
     s0  ...                       c     B  A  B                           ...  sn-1
              D                 B  C     B  A  B
                                                   D  B  C  B   A       B
     Unfortunately, shifting the pattern by its entire length when there is no other
     occurrence of suff (k) not preceded by the same character as in its rightmost
     occurrence is not always correct. For example, for the pattern ABCBAB and k = 3,
     shifting by 6 could miss a matching substring that starts with the text's AB aligned
     with the last two characters of the pattern:
        s0  ...              c     B  A  B  C  B  A  B         ...  sn-1
                       A  B  C     B  A  B
                                            A  B  C  B   A  B
Note that the shift by 6 is correct for the pattern DBCBAB but not for ABCBAB,
because the latter pattern has the same substring AB as its prefix (beginning part
of the pattern) and as its suffix (ending part of the pattern). To avoid such an
erroneous shift based on a suffix of size k, for which there is no other occurrence
in the pattern not preceded by the same character as in its rightmost occurrence,
we need to find the longest prefix of size l < k that matches the suffix of the same
size l. If such a prefix exists, the shift size d2 is computed as the distance between
this prefix and the corresponding suffix; otherwise, d2 is set to the pattern's length
m. As an example, here is the complete list of the d2 values--the good-suffix table
of the Boyer-Moore algorithm--for the pattern ABCBAB:
                                k        pattern     d2
                                1        ABCBAB      2
                                2        ABCBAB      4
                                3        ABCBAB      4
                                4        ABCBAB      4
                                5        ABCBAB      4
Now we are prepared to summarize the Boyer-Moore algorithm in its entirety.
The Boyer-Moore algorithm
Step 1 For a given pattern and the alphabet used in both the pattern and the
        text, construct the bad-symbol shift table as described earlier.
Step 2 Using the pattern, construct the good-suffix shift table as described
        earlier.
Step 3 Align the pattern against the beginning of the text.
Step 4  Repeat the following step until either a matching substring is found or
        the pattern reaches beyond the last character of the text. Starting with
        the last character in the pattern, compare the corresponding characters
        in the pattern and the text until either all m character pairs are matched
        (then stop) or a mismatching pair is encountered after k  0 character
        pairs are matched successfully. In the latter case, retrieve the entry
        t1(c) from the c's column of the bad-symbol table where c is the text's
        mismatched character. If k > 0, also retrieve the corresponding d2
        entry from the good-suffix table. Shift the pattern to the right by the
                  number of positions computed by the formula
                                            d=     d1           if k = 0,     (7.3)
                                                   max{d1, d2}  if k > 0,
                  where d1 = max{t1(c) - k, 1}.
          Shifting by the maximum of the two available shifts when k > 0 is quite log-
     ical. The two shifts are based on the observations--the first one about a text's
     mismatched character, and the second one about a matched group of the pattern's
     rightmost characters--that imply that shifting by less than d1 and d2 characters, re-
     spectively, cannot lead to aligning the pattern with a matching substring in the text.
     Since we are interested in shifting the pattern as far as possible without missing a
     possible matching substring, we take the maximum of these two numbers.
     EXAMPLE      As a complete example, let us consider searching for the pattern
     BAOBAB in a text made of English letters and spaces. The bad-symbol table looks
     as follows:
                  c      A      B           C   D      ...  O   ...  Z     _
                  t1(c)  1      2           6   6      6    3   6    6     6
     The good-suffix table is filled as follows:
                                         k      pattern     d2
                                         1      BAOBAB      2
                                         2      BAOBAB      5
                                         3      BAOBAB      5
                                         4      BAOBAB      5
                                         5      BAOBAB      5
          The actual search for this pattern in the text given in Figure 7.3 proceeds as
     follows. After the last B of the pattern fails to match its counterpart K in the text,
     the algorithm retrieves t1(K) = 6 from the bad-symbol table and shifts the pat-
     tern by d1 = max{t1(K) - 0, 1} = 6 positions to the right. The new try successfully
     matches two pairs of characters. After the failure of the third comparison on the
     space character in the text, the algorithm retrieves t1(      ) = 6 from the bad-symbol
     table and d2 = 5 from the good-suffix table to shift the pattern by max{d1, d2} =
     max{6 - 2, 5} = 5. Note that on this iteration it is the good-suffix rule that leads
     to a farther shift of the pattern.
          The next try successfully matches just one pair of B's. After the failure of
     the next comparison on the space character in the text, the algorithm retrieves
     t1(  ) = 6 from the bad-symbol table and d2 = 2 from the good-suffix table to shift
B  E  S  S  _       K   N   E   W     _  A    B  O      U  T      _  B  A  O        B  A  B     S
B  A  O  B  A       B
d1 = t1(K) - 0 = 6      B   A   O     B  A    B
                        d1 = t1( ) - 2 = 4    B  A      O  B      A  B
                        d2 = 5                d1 = t1(  )-1=5
                        d = max{4, 5} = 5     d2 = 2
                                              d = max{5, 2} = 5
                                                                     B  A  O        B  A  B
                    FIGURE 7.3 Example of string matching with the   Boyer-Moore    algorithm.
                    the pattern by max{d1,d2} = max{6 - 1, 2} = 5. Note that on this iteration it is the
                    bad-symbol rule that leads to a farther shift of the pattern. The next try finds a
                    matching substring in the text after successfully matching all six characters of the
                    pattern with their counterparts in the text.
                        When searching for the first occurrence of the pattern, the worst-case effi-
                    ciency of the Boyer-Moore algorithm is known to be linear. Though this algorithm
                    runs very fast, especially on large alphabets (relative to the length of the pattern),
                    many people prefer its simplified versions, such as Horspool's algorithm, when
                    dealing with natural-language­like strings.
                    Exercises 7.2
                    1.  Apply Horspool's algorithm to search for the pattern BAOBAB in the text
                                                 BESS KNEW ABOUT BAOBABS
                    2.  Consider the problem of searching for genes in DNA sequences using Hor-
                        spool's algorithm. A DNA sequence is represented by a text on the alphabet
                        {A, C, G, T}, and the gene or gene segment is the pattern.
                        a.  Construct the shift table for the following gene segment of your chromo-
                            some 10:
                                                           TCCTATTCTT
                        b. Apply Horspool's algorithm to locate the above pattern in the following
                            DNA sequence:
                                         TTATAGATCTCGTATTCTTTTATAGATCTCCTATTCTT
     3.   How many character comparisons will be made by Horspool's algorithm in
          searching for each of the following patterns in the binary text of 1000 zeros?
              a. 00001       b. 10000  c. 01010
     4.   For searching in a text of length n for a pattern of length m (n  m) with
          Horspool's algorithm, give an example of
              a. worst-case input.     b. best-case input.
     5.   Is it possible for Horspool's algorithm to make more character comparisons
          than the brute-force algorithm would make in searching for the same pattern
          in the same text?
     6.   If Horspool's algorithm discovers a matching substring, how large a shift
          should it make to search for a next possible match?
     7.   How many character comparisons will the Boyer-Moore algorithm make in
          searching for each of the following patterns in the binary text of 1000 zeros?
              a. 00001       b. 10000  c. 01010
     8.   a.  Would the Boyer-Moore algorithm work correctly with just the bad-symbol
              table to guide pattern shifts?
          b.  Would the Boyer-Moore algorithm work correctly with just the good-suffix
              table to guide pattern shifts?
     9.   a.  If the last characters of a pattern and its counterpart in the text do match,
              does Horspool's algorithm have to check other characters right to left, or
              can it check them left to right too?
          b.  Answer the same question for the Boyer-Moore algorithm.
     10.  Implement Horspool's algorithm, the Boyer-Moore algorithm, and the brute-
          force algorithm of Section 3.2 in the language of your choice and run an
          experiment to compare their efficiencies for matching
          a.  random binary patterns in random binary texts.
          b. random natural-language patterns in natural-language texts.
     11.  You are given two strings S and T , each n characters long. You have to
          establish whether one of them is a right cyclic shift of the other. For example,
          PLEA is a right cyclic shift of LEAP, and vice versa. (Formally, T is a right cyclic
          shift of S if T can be obtained by concatenating the (n - i)-character suffix of
          S and the i-character prefix of S for some 1  i  n.)
          a.  Design a space-efficient algorithm for the task. Indicate the space and time
              efficiencies of your algorithm.
          b. Design a time-efficient algorithm for the task. Indicate the time and space
              efficiencies of your algorithm.
7.3  Hashing
     In this section, we consider a very efficient way to implement dictionaries. Recall
     that a dictionary is an abstract data type, namely, a set with the operations of
     searching (lookup), insertion, and deletion defined on its elements. The elements
     of this set can be of an arbitrary nature: numbers, characters of some alphabet,
     character strings, and so on. In practice, the most important case is that of records
     (student records in a school, citizen records in a governmental office, book records
     in a library).
     Typically, records comprise several fields, each responsible for keeping a
     particular type of information about an entity the record represents. For example,
     a student record may contain fields for the student's ID, name, date of birth, sex,
     home address, major, and so on. Among record fields there is usually at least one
     called a key that is used for identifying entities represented by the records (e.g.,
     the student's ID). In the discussion below, we assume that we have to implement
     a dictionary of n records with keys K1, K2, . . . , Kn.
     Hashing is based on the idea of distributing keys among a one-dimensional
     array H [0..m - 1] called a hash table. The distribution is done by computing, for
     each of the keys, the value of some predefined function h called the hash function.
     This function assigns an integer between 0 and m - 1, called the hash address, to
     a key.
     For example, if keys are nonnegative integers, a hash function can be of
     the form h(K) = K mod m; obviously, the remainder of division by m is always
     between 0 and m - 1. If keys are letters of some alphabet, we can first assign a letter
     its position in the alphabet, denoted here ord(K), and then apply the same kind
     of a function used for integers. Finally, if K is a character string c0c1 . . . cs-1, we
                                                   s-1
     can use, as a very unsophisticated option, (       or    d    (ci  ))  mod  m.  A  better        option
                                                   i=0
     is to compute h(K) as follows:2
             h  0;   for i  0 to s - 1 do h  (h  C + ord(ci)) mod m,
     where C is a constant larger than every ord(ci).
     In general, a hash function needs to satisfy somewhat conflicting require-
     ments:
     A hash table's size should not be excessively large compared to the number of
     keys, but it should be sufficient to not jeopardize the implementation's time
     efficiency (see below).
     A hash function needs to distribute keys among the cells of the hash table as
     evenly as possible. (This requirement makes it desirable, for most applications,
     to have a hash function dependent on all bits of a key, not just some of them.)
     A hash function has to be easy to compute.
2.   This can be obtained by treating ord(ci) as digits of a number in the C-based system, computing its
     decimal value by Horner's rule, and finding the remainder of the number after dividing it by m.
                                          Ki             Kj
                                  .    .  .              .   .  .
                              0               b                           m ­1
     FIGURE   7.4  Collision  of  two  keys in hashing:  h(Ki) = h(Kj ).
     Obviously, if we choose a hash table's size m to be smaller than the number
     of keys n, we will get collisions--a phenomenon of two (or more) keys being
     hashed into the same cell of the hash table (Figure 7.4). But collisions should be
     expected even if m is considerably larger than n (see Problem 5 in this section's
     exercises). In fact, in the worst case, all the keys could be hashed to the same cell
     of the hash table. Fortunately, with an appropriately chosen hash table size and a
     good hash function, this situation happens very rarely. Still, every hashing scheme
     must have a collision resolution mechanism. This mechanism is different in the
     two principal versions of hashing: open hashing (also called separate chaining)
     and closed hashing (also called open addressing).
     Open Hashing (Separate Chaining)
     In open hashing, keys are stored in linked lists attached to cells of a hash table.
     Each list contains all the keys hashed to its cell. Consider, as an example, the
     following list of words:
                   A, FOOL, AND, HIS, MONEY, ARE, SOON, PARTED.
     As a hash function, we will use the simple function for strings mentioned above,
     i.e., we will add the positions of a word's letters in the alphabet and compute the
     sum's remainder after division by 13.
     We start with the empty table. The first key is the word A; its hash value is
     h(A) = 1 mod 13 = 1. The second key--the word FOOL--is installed in the ninth
     cell since (6 + 15 + 15 + 12) mod 13 = 9, and so on. The final result of this process
     is given in Figure 7.5; note a collision of the keys ARE and SOON because h(ARE) =
     (1 + 18 + 5) mod 13 = 11 and h(SOON) = (19 + 15 + 15 + 14) mod 13 = 11.
     How do we search in a dictionary implemented as such a table of linked lists?
     We do this by simply applying to a search key the same procedure that was used
     for creating the table. To illustrate, if we want to search for the key KID in the hash
     table of Figure 7.5, we first compute the value of the same hash function for the
     key: h(KID) = 11. Since the list attached to cell 11 is not empty, its linked list may
     contain the search key. But because of possible collisions, we cannot tell whether
     this is the case until we traverse this linked list. After comparing the string KID first
     with the string ARE and then with the string SOON, we end up with an unsuccessful
     search.
     In general, the efficiency of searching depends on the lengths of the linked
     lists, which, in turn, depend on the dictionary and table sizes, as well as the quality
keys                A     FOOL     AND      HIS    MONEY         ARE  SOON  PARTED
hash addresses      1        9     6        10     7             11    11   12
0     1  2    3  4     5        6        7      8  9             10   11    12
                                                                            
      A                      AND   MONEY           FOOL          HIS  ARE   PARTED
                                                                       
                                                                      SOON
FIGURE 7.5 Example  of a  hash table construction  with separate chaining.
of the hash function. If the hash function distributes n keys among m cells of the
hash table about evenly, each list will be about n/m keys long. The ratio  = n/m,
called the load factor of the hash table, plays a crucial role in the efficiency of
hashing. In particular, the average number of pointers (chain links) inspected in
successful searches, S, and unsuccessful searches, U, turns out to be
                             S 1+           and    U = ,                        (7.4)
                                         2
respectively, under the standard assumptions of searching for a randomly selected
element and a hash function distributing keys uniformly among the table's cells.
These results are quite natural. Indeed, they are almost identical to searching
sequentially in a linked list; what we have gained by hashing is a reduction in
average list size by a factor of m, the size of the hash table.
   Normally, we want the load factor to be not far from 1. Having it too small
would imply a lot of empty lists and hence inefficient use of space; having it too
large would mean longer linked lists and hence longer search times. But if we
do have the load factor around 1, we have an amazingly efficient scheme that
makes it possible to search for a given key for, on average, the price of one or
two comparisons! True, in addition to comparisons, we need to spend time on
computing the value of the hash function for a search key, but it is a constant-time
operation, independent from n and m. Note that we are getting this remarkable
efficiency not only as a result of the method's ingenuity but also at the expense of
extra space.
   The two other dictionary operations--insertion and deletion--are almost
identical to searching. Insertions are normally done at the end of a list (but see
Problem 6 in this section's exercises for a possible modification of this rule).
Deletion is performed by searching for a key to be deleted and then removing
it from its list. Hence, the efficiency of these operations is identical to that of
searching, and they are all        (1) in the average case if the number of keys n is
about equal to the hash table's size m.
     Closed Hashing (Open Addressing)
     In closed hashing, all keys are stored in the hash table itself without the use
     of linked lists. (Of course, this implies that the table size m must be at least as
     large as the number of keys n.) Different strategies can be employed for collision
     resolution. The simplest one--called linear probing--checks the cell following
     the one where the collision occurs. If that cell is empty, the new key is installed
     there; if the next cell is already occupied, the availability of that cell's immediate
     successor is checked, and so on. Note that if the end of the hash table is reached,
     the search is wrapped to the beginning of the table; i.e., it is treated as a circular
     array. This method is illustrated in Figure 7.6 with the same word list and hash
     function used above to illustrate separate chaining.
     To search for a given key K, we start by computing h(K) where h is the hash
     function used in the table construction. If the cell h(K) is empty, the search is
     unsuccessful. If the cell is not empty, we must compare K with the cell's occupant:
     if they are equal, we have found a matching key; if they are not, we compare K
     with a key in the next cell and continue in this manner until we encounter either
     a matching key (a successful search) or an empty cell (unsuccessful search). For
     example, if we search for the word LIT in the table of Figure 7.6, we will get h(LIT)
     = (12 + 9 + 20) mod 13 = 2 and, since cell 2 is empty, we can stop immediately.
     However, if we search for KID with h(KID) = (11 + 9 + 4) mod 13 = 11, we will
     have to compare KID with ARE, SOON, PARTED, and A before we can declare the
     search unsuccessful.
     Although the search and insertion operations are straightforward for this
     version of hashing, deletion is not. For example, if we simply delete the key ARE
     from the last state of the hash table in Figure 7.6, we will be unable to find the key
     SOON afterward. Indeed, after computing h(SOON) = 11, the algorithm would find
     this location empty and report the unsuccessful search result. A simple solution
     keys                  A    FOOL     AND  HIS        MONEY     ARE  SOON            PARTED
     hash addresses        1       9     6    10           7       11          11       12
     0           1   2     3    4     5  6    7          8      9       10         11   12
                 A
                 A                                              FOOL
                 A                       AND                    FOOL
                 A                       AND                    FOOL    HIS
                 A                       AND  MONEY             FOOL    HIS
                 A                       AND  MONEY             FOOL    HIS        ARE
                 A                       AND  MONEY             FOOL    HIS        ARE  SOON
     PARTED      A                       AND  MONEY             FOOL    HIS        ARE  SOON
     FIGURE 7.6  Example of   a hash table construction  with linear probing.
    is to use "lazy deletion," i.e., to mark previously occupied locations by a special
    symbol to distinguish them from locations that have not been occupied.
    The mathematical analysis of linear probing is a much more difficult problem
    than that of separate chaining.3 The simplified versions of these results state that
    the average number of times the algorithm must access the hash table with the
    load factor  in successful and unsuccessful searches is, respectively,
                 S  1(1 +  1        )       and        U     1(1 +           1     )   (7.5)
                 2         1-                                2           (1  - )2
    (and the accuracy of these approximations increases with larger sizes of the hash
    table). These numbers are surprisingly small even for densely populated tables,
    i.e., for large percentage values of :
                           1        (1  +        1  )     1  (1  +       1   )
                           2                1-            2         (1-)2
                 50%                        1.5                     2.5
                 75%                        2.5                     8.5
                 90%                        5.5                  50.5
    Still, as the hash table gets closer to being full, the performance of linear prob-
    ing deteriorates because of a phenomenon called clustering. A cluster in linear
    probing is a sequence of contiguously occupied cells (with a possible wrapping).
    For example, the final state of the hash table of Figure 7.6 has two clusters. Clus-
    ters are bad news in hashing because they make the dictionary operations less
    efficient. As clusters become larger, the probability that a new element will be
    attached to a cluster increases; in addition, large clusters increase the probabil-
    ity that two clusters will coalesce after a new key's insertion, causing even more
    clustering.
    Several other collision resolution strategies have been suggested to alleviate
    this problem. One of the most important is double hashing. Under this scheme, we
    use another hash function, s(K), to determine a fixed increment for the probing
    sequence to be used after a collision at location l = h(K):
                 (l + s(K)) mod m,               (l + 2s(K)) mod m,             ... .  (7.6)
    To guarantee that every location in the table is probed by sequence (7.6), the incre-
    ment s(k) and the table size m must be relatively prime, i.e., their only common
    divisor must be 1. (This condition is satisfied automatically if m itself is prime.)
    Some functions recommended in the literature are s(k) = m - 2 - k mod (m - 2)
    and s(k) = 8 - (k mod 8) for small tables and s(k) = k mod 97 + 1 for larger ones.
3.  This problem was solved in 1962 by a young graduate student in mathematics named Donald E.
    Knuth. Knuth went on to become one of the most important computer scientists of our time. His
    multivolume treatise The Art of Computer Programming [KnuI, KnuII, KnuIII, KnuIV] remains the
    most comprehensive and influential book on algorithmics ever published.
     Mathematical analysis of double hashing has proved to be quite difficult. Some
     partial results and considerable practical experience with the method suggest that
     with good hashing functions--both primary and secondary--double hashing is su-
     perior to linear probing. But its performance also deteriorates when the table gets
     close to being full. A natural solution in such a situation is rehashing: the current
     table is scanned, and all its keys are relocated into a larger table.
         It is worthwhile to compare the main properties of hashing with balanced
     search trees--its principal competitor for implementing dictionaries.
         Asymptotic time efficiency  With hashing, searching, insertion, and deletion
         can be implemented to take    (1) time on the average but          (n) time in the very
         unlikely worst case. For balanced search trees, the average time efficiencies
         are  (log n) for both the average and worst cases.
         Ordering preservation       Unlike    balanced  search  trees,     hashing  does   not
         assume existence of key ordering and usually does not preserve it. This makes
         hashing less suitable for applications that need to iterate over the keys in or-
         der or require range queries such as counting the number of keys between
         some lower and upper bounds.
         Since its discovery in the 1950s by IBM researchers, hashing has found many
     important applications. In particular, it has become a standard technique for stor-
     ing a symbol table--a table of a computer program's symbols generated during
     compilation. Hashing is quite handy for such AI applications as checking whether
     positions generated by a chess-playing computer program have already been con-
     sidered. With some modifications, it has also proved to be useful for storing very
     large dictionaries on disks; this variation of hashing is called extendible hashing.
     Since disk access is expensive compared with probes performed in the main mem-
     ory, it is preferable to make many more probes than disk accesses. Accordingly, a
     location computed by a hash function in extendible hashing indicates a disk ad-
     dress of a bucket that can hold up to b keys. When a key's bucket is identified,
     all its keys are read into main memory and then searched for the key in question.
     In the next section, we discuss B-trees, a principal alternative for storing large
     dictionaries.
     Exercises 7.3
     1.  For the input 30, 20, 56, 75, 31, 19 and hash function h(K) = K mod 11
         a.  construct the open hash table.
         b. find the largest number of key comparisons in a successful search in this
             table.
         c.  find the average number of key comparisons in a successful search in this
             table.
     2.  For the input 30, 20, 56, 75, 31, 19 and hash function h(K) = K mod 11
         a.  construct the closed hash table.
     b. find the largest number of key comparisons in a successful search in this
         table.
     c.  find the average number of key comparisons in a successful search in this
         table.
3.   Why is it not a good idea for a hash function to depend on just one letter (say,
     the first one) of a natural-language word?
4.   Find the probability of all n keys being hashed to the same cell of a hash table
     of size m if the hash function distributes keys evenly among all the cells of the
     table.
5.   Birthday paradox      The birthday paradox asks how many people should be
     in a room so that the chances are better than even that two of them will have
     the same birthday (month and day). Find the quite unexpected answer to this
     problem. What implication for hashing does this result have?
6.   Answer the following questions for the separate-chaining version of hashing.
     a.  Where would you insert keys if you knew that all the keys in the dictionary
         are distinct? Which dictionary operations, if any, would benefit from this
         modification?
     b.  We could keep keys of the same linked list sorted. Which of the dictio-
         nary operations would benefit from this modification? How could we take
         advantage of this if all the keys stored in the entire table need to be sorted?
7.   Explain how to use hashing to check whether all elements of a list are distinct.
     What is the time efficiency of this application? Compare its efficiency with
     that of the brute-force algorithm (Section 2.3) and of the presorting-based
     algorithm (Section 6.1).
8.   Fill in the following table with the average-case (as the first entry) and worst-
     case (as the second entry) efficiency classes for the five implementations of
     the ADT dictionary:
                        unordered  ordered       binary  balanced
                           array   array    search tree  search tree  hashing
             search
             insertion
             deletion
9.   We have discussed hashing in the context of techniques based on space­time
     trade-offs. But it also takes advantage of another general strategy. Which one?
10.  Write a computer program that uses hashing for the following problem. Given
     a natural-language text, generate a list of distinct words with the number of
     occurrences of each word in the text. Insert appropriate counters in the pro-
     gram to compare the empirical efficiency of hashing with the corresponding
     theoretical results.
              p0       K1  p1    ..        .  pi ­1   Ki  pi      ...       pn ­2           Kn ­1  pn ­1
          T0               T1                 Ti ­1           Ti       Tn ­2                       Tn ­1
          FIGURE 7.7   Parental  node  of  a B-tree.
     7.4  B-Trees
          The idea of using extra space to facilitate faster access to a given data set is partic-
          ularly important if the data set in question contains a very large number of records
          that need to be stored on a disk. A principal device in organizing such data sets
          is an index, which provides some information about the location of records with
          indicated key values. For data sets of structured records (as opposed to "unstruc-
          tured" data such as text, images, sound, and video), the most important index
          organization is the B-tree, introduced by R. Bayer and E. McGreight [Bay72]. It
          extends the idea of the 2-3 tree (see Section 6.3) by permitting more than a single
          key in the same node of a search tree.
          In the B-tree version we consider here, all data records (or record keys)
          are stored at the leaves, in increasing order of the keys. The parental nodes are
          used for indexing. Specifically, each parental node contains n - 1 ordered keys
          K1 < . . . < Kn-1 assumed, for the sake of simplicity, to be distinct. The keys are
          interposed with n pointers to the node's children so that all the keys in subtree T0
          are smaller than K1, all the keys in subtree T1 are greater than or equal to K1 and
          smaller than K2 with K1 being equal to the smallest key in T1, and so on, through
          the last subtree Tn-1 whose keys are greater than or equal to Kn-1 with Kn-1 being
          equal to the smallest key in Tn-1 (see Figure 7.7).4
          In addition, a B-tree of order m  2 must satisfy the following structural
          properties:
          The root is either a leaf or has between 2 and m children.
          Each node, except for the root and the leaves, has between                        m/2           and m
          children (and hence between         m/2     - 1 and m - 1 keys).
          The tree is (perfectly) balanced, i.e., all its leaves are at the same level.
     4.   The node depicted in Figure 7.7 is called the n-node. Thus, all the nodes in a classic binary search tree
          are 2-nodes; a 2-3 tree introduced in Section 6.3 comprises 2-nodes and 3-nodes.
                                         20      51
11        15                             25    34    40                   60
4, 7, 10  11, 14  15, 16, 19     20, 24  25, 28      34, 38   40, 43, 46  51, 55  60, 68, 80
FIGURE 7.8 Example of a B-tree of order 4.
An example of a B-tree of order 4 is given in Figure 7.8.
Searching in a B-tree is very similar to searching in the binary search tree, and
even more so in the 2-3 tree. Starting with the root, we follow a chain of pointers
to the leaf that may contain the search key. Then we search for the search key
among the keys of that leaf. Note that since keys are stored in sorted order, at
both parental nodes and leaves, we can use binary search if the number of keys at
a node is large enough to make it worthwhile.
It is not the number of key comparisons, however, that we should be con-
cerned about in a typical application of this data structure. When used for storing
a large data file on a disk, the nodes of a B-tree normally correspond to the disk
pages. Since the time needed to access a disk page is typically several orders of
magnitude larger than the time needed to compare keys in the fast computer mem-
ory, it is the number of disk accesses that becomes the principal indicator of the
efficiency of this and similar data structures.
How many nodes of a B-tree do we need to access during a search for a record
with a given key value? This number is, obviously, equal to the height of the tree
plus 1. To estimate the height, let us find the smallest number of keys a B-tree of
order m and positive height h can have. The root of the tree will contain at least
one key. Level 1 will have at least two nodes with at least   m/2         - 1 keys in each
of them, for the total minimum number of keys 2( m/2          - 1). Level 2 will have at
least 2 m/2   nodes (the children of the nodes on level 1) with at least          m/2     -1
in each of them, for the total minimum number of keys 2 m/2 ( m/2                    - 1). In
general, the nodes of level i, 1  i  h - 1, will contain at least 2       m/2  i-1(  m/2  -
1) keys. Finally, level h, the leaf level, will have at least 2 m/2 h-1 nodes with at
least one key in each. Thus, for any B-tree of order m with n nodes and height
h > 0, we have the following inequality:
                            h-1
                  n1+            2  m/2  i-1(  m/2   - 1) + 2 m/2 h-1.
                            i=1
After a series of standard simplifications (see Problem 2 in this section's exercises),
this inequality reduces to
                                n  4 m/2 h-1 - 1,
     which, in turn, yields the following upper bound on the height h of the B-tree of
     order m with n nodes:
                                h  log m/2   n+1         + 1.       (7.7)
                                                      4
        Inequality (7.7) immediately implies that searching in a B-tree is a O(log n)
     operation. But it is important to ascertain here not just the efficiency class but
     the actual number of disk accesses implied by this formula. The following table
     contains the values of the right-hand-side estimates for a file of 100 million records
     and a few typical values of the tree's order m:
                                order m      50          100   250
                            h's upper bound  6           5     4
     Keep in mind that the table's entries are upper estimates for the number of disk
     accesses. In actual applications, this number rarely exceeds 3, with the B-tree's
     root and sometimes first-level nodes stored in the fast memory to minimize the
     number of disk accesses.
        The operations of insertion and deletion are less straightforward than search-
     ing, but both can also be done in O(log n) time. Here we outline an insertion
     algorithm only; a deletion algorithm can be found in the references (e.g., [Aho83],
     [Cor09]).
        The most straightforward algorithm for inserting a new record into a B-
     tree is quite similar to the algorithm for insertion into a 2-3 tree outlined in
     Section 6.3. First, we apply the search procedure to the new record's key K to
     find the appropriate leaf for the new record. If there is room for the record in that
     leaf, we place it there (in an appropriate position so that the keys remain sorted)
     and we are done. If there is no room for the record, the leaf is split in half by
     sending the second half of the records to a new node. After that, the smallest key
     K  in the new node and the pointer to it are inserted into the old leaf's parent
     (immediately after the key and pointer to the old leaf). This recursive procedure
     may percolate up to the tree's root. If the root is already full too, a new root is
     created with the two halves of the old root's keys split between two children of
     the new root. As an example, Figure 7.9 shows the result of inserting 65 into the
     B-tree in Figure 7.8 under the restriction that the leaves cannot contain more than
     three items.
        You should be aware that there are other algorithms for implementing inser-
     tions into a B-tree. For example, to avoid the possibility of recursive node splits,
     we can split full nodes encountered in searching for an appropriate leaf for the
     new record. Another possibility is to avoid some node splits by moving a key to
     the node's sibling. For example, inserting 65 into the B-tree in Figure 7.8 can be
     done by moving 60, the smallest key of the full leaf, to its sibling with keys 51 and
     55, and replacing the key value of their parent by 65, the new smallest value in
                                        20   51
        11  15                          25   34  40                    60     68
4, 7, 10    11, 14  15, 16, 19  20, 24  25, 28   34, 38  40, 43, 46  51, 55   60, 65  68, 80
FIGURE 7.9 B-tree obtained after inserting 65 into the B-tree in Figure 7.8.
the second child. This modification tends to save some space at the expense of a
slightly more complicated algorithm.
    A B-tree does not have to be always associated with the indexing of a large
file, and it can be considered as one of several search tree varieties. As with other
types of search trees--such as binary search trees, AVL trees, and 2-3 trees--a B-
tree can be constructed by successive insertions of data records into the initially
empty tree. (The empty tree is considered to be a B-tree, too.) When all keys reside
in the leaves and the upper levels are organized as a B-tree comprising an index,
the entire structure is usually called, in fact, a B+-tree.
Exercises 7.4
1.  Give examples of using an index in real-life applications that do not involve
    computers.
2.  a.    Prove the equality
                    h-1
                1+       2  m/2  i-1(   m/2  - 1) + 2 m/2 h-1 = 4 m/2 h-1 - 1,
                    i=1
          which was used in the derivation of upper bound (7.7) for the height of a
          B-tree.
    b.    Complete the derivation of inequality (7.7).
3.  Find the minimum order of the B-tree that guarantees that the number of disk
    accesses in searching in a file of 100 million records does not exceed 3. Assume
    that the root's page is stored in main memory.
4.  Draw the B-tree obtained after inserting 30 and then 31 in the B-tree in
    Figure 7.8. Assume that a leaf cannot contain more than three items.
5.  Outline an algorithm for finding the largest key in a B-tree.
6.  a.    A top-down 2-3-4 tree is a B-tree of order 4 with the following modifica-
          tion of the insert operation: Whenever a search for a leaf for a new key
             encounters a full node (i.e., a node with three keys), the node is split into
             two nodes by sending its middle key to the node's parent, or, if the full
             node happens to be the root, the new root for the middle key is created.
             Construct a top-down 2-3-4 tree by inserting the following list of keys in
             the initially empty tree:
                                10,  6,  15,  31,  20,  27,  50,   44,  18.
         b.  What is the principal advantage of this insertion procedure compared with
             the one used for 2-3 trees in Section 6.3? What is its disadvantage?
     7.  a.  Write a program implementing a key insertion algorithm in a B-tree.
         b.  Write a program for visualization of a key insertion algorithm in a B-tree.
     SUMMARY
         Space and time trade-offs in algorithm design are a well-known issue for
         both theoreticians and practitioners of computing. As an algorithm design
         technique, trading space for time is much more prevalent than trading time
         for space.
         Input enhancement is one of the two principal varieties of trading space for
         time in algorithm design. Its idea is to preprocess the problem's input, in whole
         or in part, and store the additional information obtained in order to accelerate
         solving the problem afterward. Sorting by distribution counting and several
         important algorithms for string matching are examples of algorithms based
         on this technique.
         Distribution counting is a special method for sorting lists of elements from a
         small set of possible values.
         Horspool's algorithm for string matching can be considered a simplified
         version of the Boyer-Moore algorithm.Both algorithms are based on the ideas
         of input enhancement and right-to-left comparisons of a pattern's characters.
         Both algorithms use the same bad-symbol shift table; the Boyer-Moore also
         uses a second table, called the good-suffix shift table.
         Prestructuring--the second type of technique that exploits space-for-time
         trade-offs--uses extra space to facilitate a faster and/or more flexible access
         to the data. Hashing and B+-trees are important examples of prestructuring.
         Hashing is a very efficient approach to implementing dictionaries. It is based
         on the idea of mapping keys into a one-dimensional table. The size limitations
         of such a table make it necessary to employ a collision resolution mechanism.
         The two principal varieties of hashing are open hashing or separate chaining
         (with keys stored in linked lists outside of the hash table) and closed hashing
or open addressing (with keys stored inside the table). Both enable searching,
insertion, and deletion in  (1) time, on average.
The B-tree is a balanced search tree that generalizes the idea of the 2-3 tree
by allowing multiple keys at the same node. Its principal application, called
the B+-tree, is for keeping index-like information about data stored on a
disk. By choosing the order of the tree appropriately, one can implement the
operations of searching, insertion, and deletion with just a few disk accesses
even for extremely large files.

