Recommended Reading and Web Sites
36  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
             Core 0                      Core 1            Core 2  Core 3
             32 kB I&D       32 kB I&D             32 kB I&D       32 kB I&D
             L1 caches       L1 caches                 L1 caches   L1 caches
             256 kB                      256 kB            256 kB  256 kB
             L2 cache        L2 cache                  L2 cache    L2 cache
                                                 8 MB
                                                 L3 cache
             DDR3 memory                                   Quickpath
             controllers                                   interconnect
             3 × 8B @ 1.33 GT/s                            4 × 20b @ 6.4 GT/s
             Figure 1.20  Intel Core i7 Block Diagram
     At 16 bits per transfer, that adds up to 12.8 GB/s; and since QPI links involve dedi-
     cated bidirectional pairs, the total bandwidth is 25.6 GB/s.
1.9  RECOMMENDED READING AND WEB SITES
     [STAL10] covers the topics of this chapter in detail. In addition, there are many other
     texts on computer organization and architecture. Among the more worthwhile texts
     are the following. [PATT09] is a comprehensive survey; [HENN07], by the same
     authors, is a more advanced text that emphasizes quantitative aspects of design.
        [DENN05] looks at the history of the development and application of the
     locality principle, making for fascinating reading.
     DENN05  Denning, P. "The Locality Principle." Communications of the ACM, July 2005.
     HENN07  Hennessy, J., and Patterson, D. Computer Architecture: A Quantitative
        Approach. San Mateo, CA: Morgan Kaufmann, 2007.
     PATT09  Patterson, D., and Hennessy, J. Computer Organization and Design: The
        Hardware/Software Interface. San Mateo, CA: Morgan Kaufmann, 2009.
     STAL10  Stallings, W. Computer Organization and Architecture, 8th ed. Upper Saddle
        River, NJ: Prentice Hall, 2010.
     Recommended Web sites:
     ·  WWW Computer Architecture Home Page: A comprehensive index to information
        relevant to computer architecture researchers, including architecture groups and proj-
        ects, technical organizations, literature, employment, and commercial information
     ·  CPU Info Center: Information on specific processors, including technical papers, prod-
        uct information, and latest announcements

                                 1.10 / KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS                             37
1.10 KEY TERMS,                  REVIEW QUESTIONS, AND PROBLEMS
Key Terms
address register                 instruction register       program counter
cache memory                     interrupt                  programmed I/O
cache slot                       interrupt-driven I/O       reentrant procedure
central processing unit          I/O module                 register
data register                    locality                   secondary memory
direct memory access             main memory                spatial locality
hit ratio                        multicore                  stack
input/output                     multiprocessor             system bus
instruction                      processor                  temporal locality
instruction cycle
Review Questions
           1.1.   List and briefly define the four main elements of a computer.
           1.2.   Define the two main categories of processor registers.
           1.3.   In general terms, what are the four distinct actions that a machine instruction can
                  specify?
           1.4.   What is an interrupt?
           1.5.   How are multiple interrupts dealt with?
           1.6.   What characteristics distinguish the various elements of a memory hierarchy?
           1.7.   What is cache memory?
           1.8.   What is the difference between a multiprocessor and a multicore system?
           1.9.   What is the distinction between spatial locality and temporal locality?
           1.10.  In general, what are the strategies for exploiting spatial locality and temporal locality?
Problems
           1.1.   Suppose the hypothetical processor of Figure 1.3 also has two I/O instructions:
                                              0011  Load AC from I/O
                                              0111  Store AC to I/O
                  In these cases, the 12-bit address identifies a particular external device. Show the pro-
                  gram execution (using format of Figure 1.4) for the following program:
                  1.  Load AC from device 5.
                  2.  Add contents of memory location 940.
                  3.  Store AC to device 6.
                  Assume that the next value retrieved from device 5 is 3 and that location 940 contains
                  a value of 2.
           1.2.   The program execution of Figure 1.4 is described in the text using six steps. Expand
                  this description to show the use of the MAR and MBR.
           1.3.   Consider a hypothetical 32-bit microprocessor having 32-bit instructions composed of
                  two fields. The first byte contains the opcode and the remainder an immediate oper-
                  and or an operand address.

38  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
          a.  What is the maximum directly addressable memory capacity (in bytes)?
          b.  Discuss the impact on the system speed if the microprocessor bus has
              1.  a 32-bit local address bus and a 16-bit local data bus, or
              2.  a 16-bit local address bus and a 16-bit local data bus.
          c.  How many bits are needed for the program counter and the instruction register?
    1.4.  Consider a hypothetical microprocessor generating a 16-bit address (e.g., assume that
          the program counter and the address registers are 16 bits wide) and having a 16-bit
          data bus.
          a.  What is the maximum memory address space that the processor can access directly
              if it is connected to a "16-bit memory"?
          b.  What is the maximum memory address space that the processor can access directly
              if it is connected to an "8-bit memory"?
          c.  What architectural features will allow this microprocessor to access a separate
              "I/O space"?
          d.  If an input and an output instruction can specify an 8-bit I/O port number, how
              many 8-bit I/O ports can the microprocessor support? How many 16-bit I/O ports?
              Explain.
    1.5.  Consider a 32-bit microprocessor, with a 16-bit external data bus, driven by an 8-MHz
          input clock. Assume that this microprocessor has a bus cycle whose minimum dura-
          tion equals four input clock cycles. What is the maximum data transfer rate across
          the bus that this microprocessor can sustain in bytes/s? To increase its performance,
          would it be better to make its external data bus 32 bits or to double the external clock
          frequency supplied to the microprocessor? State any other assumptions you make and
          explain. Hint: Determine the number of bytes that can be transferred per bus cycle.
    1.6.  Consider     a  computer  system        that  contains  an  I/O  module  controlling  a  simple
          keyboard/printer Teletype. The following registers are contained in the CPU and con-
          nected directly to the system bus:
              INPR:       Input Register, 8 bits
              OUTR:       Output Register, 8 bits
              FGI:     Input Flag, 1 bit
              FGO:     Output Flag, 1 bit
              IEN:     Interrupt Enable, 1 bit
          Keystroke input from the Teletype and output to the printer are controlled by the I/O
          module. The Teletype is able to encode an alphanumeric symbol to an 8-bit word and
          decode an 8-bit word into an alphanumeric symbol. The Input flag is set when an 8-bit
          word enters the input register from the Teletype. The Output flag is set when a word
          is printed.
          a.  Describe how the CPU, using the first four registers listed in this problem, can
              achieve I/O with the Teletype.
          b.  Describe how the function can be performed more efficiently by also employing
              IEN.
    1.7.  In virtually all systems that include DMA modules, DMA access to main memory is
          given higher priority than processor access to main memory. Why?
    1.8.  A DMA module is transferring characters to main memory from an external device
          transmitting at 9600 bits per second (bps). The processor can fetch instructions at the
          rate of 1 million instructions per second. By how much will the processor be slowed
          down due to the DMA activity?
    1.9.  A computer consists of a CPU and an I/O device D connected to main memory M via
          a shared bus with a data bus width of one word. The CPU can execute a maximum
          of 106 instructions per second. An average instruction requires five processor cycles,
          three of which use the memory bus. A memory read or write operation uses one
          processor cycle. Suppose that the CPU is continuously executing "background" pro-
          grams that require 95% of its instruction execution rate but not any I/O instructions.

APPENDIX 1A / PERFORMANCE CHARACTERISTICS OF TWO-LEVEL MEMORIES                                   39
       Assume that one processor cycle equals one bus cycle. Now suppose that very large
       blocks of data are to be transferred between M and D.
       a.  If programmed I/O is used and each one-word I/O transfer requires the CPU to
           execute two instructions, estimate the maximum I/O data transfer rate, in words
           per second, possible through D.
       b.  Estimate the same rate if DMA transfer is used.
1.10.  Consider the following code:
           for (i  0; i  20; i++)
                 for (j  0; j  10; j++)
                 a[i]  a[i] * j
       a.  Give one example of the spatial locality in the code.
       b.  Give one example of the temporal locality in the code.
1.11.  Generalize Equations (1.1) and (1.2) in Appendix 1A to n-level memory hierarchies.
1.12.  Consider a memory system with the following parameters:
                                   Tc  100 ns  Cc  0.01 cents/bit
                         Tm  1,200 ns          Cm  0.001 cents/bit
       a.  What is the cost of 1 MByte of main memory?
       b.  What is the cost of 1 MByte of main memory using cache memory technology?
       c.  If the effective access time is 10% greater than the cache access time, what is the
           hit ratio H?
1.13.  A computer has a cache, main memory, and a disk used for virtual memory. If a refer-
       enced word is in the cache, 20 ns are required to access it. If it is in main memory but
       not in the cache, 60 ns are needed to load it into the cache (this includes the time to
       originally check the cache), and then the reference is started again. If the word is not
       in main memory, 12 ms are required to fetch the word from disk, followed by 60 ns to
       copy it to the cache, and then the reference is started again. The cache hit ratio is 0.9
       and the main-memory hit ratio is 0.6. What is the average time in ns required to access
       a referenced word on this system?
1.14.  Suppose a stack is to be used by the processor to manage procedure calls and returns.
       Can the program counter be eliminated by using the top of the stack as a program
       counter?
APPENDIX 1A PERFORMANCE CHARACTERISTICS
OF TWO-LEVEL MEMORIES
In this chapter, reference is made to a cache that acts as a buffer between main
memory and processor, creating a two-level internal memory. This two-level archi-
tecture exploits a property known as locality to provide improved performance over
a comparable one-level memory.
       The main memory cache mechanism is part of the computer architecture,
implemented in hardware and typically invisible to the OS. Accordingly, this
mechanism is not pursued in this book. However, there are two other instances
of a two-level memory approach that also exploit the property of locality and that
are, at least partially, implemented in the OS: virtual memory and the disk cache
(Table 1.2). These two topics are explored in Chapters 8 and 11, respectively. In this
appendix, we look at some of the performance characteristics of two-level memo-
ries that are common to all three approaches.

40  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
Table 1.2  Characteristics  of   Two-Level Memories
                                      Main Memory            Virtual Memory
                                      Cache                  (Paging)         Disk Cache
Typical access time ratios       5:1                 106: 1                   106: 1
Memory management                Implemented by      Combination of hardware  System software
system                           special hardware    and system software
Typical block size               4 to 128 bytes      64 to 4096 bytes         64 to 4096 bytes
Access of processor to           Direct access       Indirect access          Indirect access
second level
           Locality
           The basis for the performance advantage of a two-level memory is the principle of
           locality, referred to in Section 1.5. This principle states that memory references tend
           to cluster. Over a long period of time, the clusters in use change; but over a short
           period of time, the processor is primarily working with fixed clusters of memory
           references.
                  Intuitively, the principle of locality makes sense. Consider the following line
           of reasoning:
              1.  Except for branch and call instructions, which constitute only a small fraction
                  of all program instructions, program execution is sequential. Hence, in most
                  cases, the next instruction to be fetched immediately follows the last instruc-
                  tion fetched.
              2.  It is rare to have a long uninterrupted sequence of procedure calls followed
                  by the corresponding sequence of returns. Rather, a program remains con-
                  fined to a rather narrow window of procedure-invocation depth. Thus, over
                  a short period of time references to instructions tend to be localized to a few
                  procedures.
              3.  Most iterative constructs consist of a relatively small number of instructions
                  repeated many times. For the duration of the iteration, computation is there-
                  fore confined to a small contiguous portion of a program.
           4.     In many programs, much of the computation involves processing data struc-
                  tures, such as arrays or sequences of records. In many cases, successive
                  references to these data structures will be to closely located data items.
                  This line of reasoning has been confirmed in many studies. With reference to
           point (1), a variety of studies have analyzed the behavior of high-level language
           programs. Table 1.3 includes key results, measuring the appearance of various
           statement types during execution, from the following studies. The earliest study of
           programming language behavior, performed by Knuth [KNUT71], examined a col-
           lection of FORTRAN programs used as student exercises. Tanenbaum [TANE78]
           published measurements collected from over 300 procedures used in OS programs
           and written in a language that supports structured programming (SAL). Patterson
           and Sequin [PATT82] analyzed a set of measurements taken from compilers
           and programs for typesetting, computer-aided design (CAD), sorting, and file

    APPENDIX 1A / PERFORMANCE CHARACTERISTICS OF TWO-LEVEL                         MEMORIES          41
Table 1.3    Relative Dynamic Frequency of High-Level Language Operations
      Study           [HUCK83]    [KNUT71]                       [PATT82]          [TANE78]
    Language          Pascal      FORTRAN   Pascal                         C       SAL
Workload              Scientific  Student   System                         System  System
Assign                74          67                         45            38      42
Loop                  4           3                          5             3       4
Call                  1           3                          15            12      12
IF                    20          11                         29            43      36
GOTO                  2           9                          --            3       --
Other                 --          7                          6             1       6
           comparison. The programming languages C and Pascal were studied. Huck
           [HUCK83] analyzed four programs intended to represent a mix of general-purpose
           scientific computing, including fast Fourier transform and the integration of systems
           of differential equations. There is good agreement in the results of this mixture of
           languages and applications that branching and call instructions represent only a
           fraction of statements executed during the lifetime of a program. Thus, these
           studies confirm assertion (1), from the preceding list.
                  With respect to assertion (2), studies reported in [PATT85] provide confirma-
           tion. This is illustrated in Figure 1.21, which shows call-return behavior. Each call is
           represented by the line moving down and to the right, and each return by the line
           moving up and to the right. In the figure, a window with depth equal to 5 is defined.
           Only a sequence of calls and returns with a net movement of 6 in either direction
           causes the window to move. As can be seen, the executing program can remain
           within a stationary window for long periods of time. A study by the same analysts of
           C and Pascal programs showed that a window of depth 8 would only need to shift on
           less than 1% of the calls or returns [TAMI83].
                                            Time
                                      (in units of calls/returns)
                                            t  33
Return
Call              w5
        Nesting
           depth
Figure 1.21       Example Call-Return Behavior of a Program

42  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
    A distinction is made in the literature between spatial locality and temporal
    locality. Spatial locality refers to the tendency of execution to involve a number of
    memory locations that are clustered. This reflects the tendency of a processor to
    access instructions sequentially. Spatial location also reflects the tendency of a pro-
    gram to access data locations sequentially, such as when processing a table of data.
    Temporal locality refers to the tendency for a processor to access memory locations
    that have been used recently. For example, when an iteration loop is executed, the
    processor executes the same set of instructions repeatedly.
    Traditionally,       temporal  locality     is  exploited  by  keeping   recently  used
    instruction and data values in cache memory and by exploiting a cache hierarchy.
    Spatial locality is generally exploited by using larger cache blocks and by incor-
    porating prefetching mechanisms (fetching items whose use is expected) into the
    cache control logic. Recently, there has been considerable research on refining
    these techniques to achieve greater performance, but the basic strategies remain
    the same.
    Operation of Two-Level Memory
    The locality property can be exploited in the formation of a two-level memory. The
    upper-level memory (M1) is smaller, faster, and more expensive (per bit) than the
    lower-level memory (M2). M1 is used as a temporary store for part of the contents
    of the larger M2. When a memory reference is made, an attempt is made to access
    the item in M1. If this succeeds, then a quick access is made. If not, then a block of
    memory locations is copied from M2 to M1 and the access then takes place via M1.
    Because of locality, once a block is brought into M1, there should be a number of
    accesses to locations in that block, resulting in fast overall service.
    To express the average time to access an item, we must consider not only the
    speeds of the two levels of memory but also the probability that a given reference
    can be found in M1. We have
                         Ts  H  T1  (1  H)  (T1  T2)
                                 T1  (1  H)  T2                                        (1.1)
    where
    Ts  average (system) access time
    T1  access time of M1 (e.g., cache, disk cache)
    T2  access time of M2 (e.g., main memory, disk)
    H  hit ratio (fraction of time reference is found in M1)
    Figure 1.15 shows average access time as a function of hit ratio. As can be
    seen, for a high percentage of hits, the average total access time is much closer to
    that of M1 than M2.
    Performance
    Let us look at some of the parameters relevant to an assessment of a two-level
    memory mechanism. First consider cost. We have
                         Cs  =   C 1S 1  +  C 2S 2                                     (1.2)
                                   S1    +  S2

APPENDIX 1A / PERFORMANCE CHARACTERISTICS OF TWO-LEVEL MEMORIES                                                                              43
                                where
                                            Cs  average cost per bit for the combined two-level memory
                                            C1  average cost per bit of upper-level memory M1
                                            C2  average cost per bit of lower-level memory M2
                                            S1  size of M1
                                            S2  size of M2
                                We would like Cs  C2. Given that C1  C2, this requires S1  S2. Figure 1.22
                                shows the relationship.7
                                            Next, consider access time. For a two-level memory to provide a significant
                                performance improvement, we need to have Ts approximately equal to T1 Ts  T1.
                                Given that T1 is much less than T2 Ts  T1, a hit ratio of close to 1 is needed.
                                            So we would like M1 to be small to hold down cost, and large to improve the
                                hit ratio and therefore the performance. Is there a size of M1 that satisfies both
                                requirements to a reasonable extent? We can answer this question with a series of
                                subquestions:
                                         ·  What value of hit ratio is needed to satisfy the performance requirement?
                                         ·  What size of M1 will assure the needed hit ratio?
                                         ·  Does this size satisfy the cost requirement?
                                1000
                                      8
                                      7
                                      6
                                      5
                                      4
                                      3                    (C1/C2)  1000
                                      2
Relative combined cost (CS/C2)  100
                                      8
                                      7
                                      6
                                      5
                                      4
                                      3
                                      2                    (C1/C2)  100
                                10
                                      8
                                      7
                                      6
                                      5
                                      4                    (C1/C2)  10
                                      3
                                      2
                                1
                                         5  6  7  8  9 10   2             3  4  5  6  7  8  9100               2  3  4  5  6  7  8           1000
                                                                          Relative size of two levels (S2/S1)
Figure 1.22                                 Relationship of Average Memory Cost to Relative Memory Size for a Two-Level
                                            Memory
                                7Note that both axes use a log scale. A basic review of log scales is in the math refresher document at the
                                Computer Science Student Resource Site at ComputerScienceStudent.com.

44                        CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
                                 To get at this, consider the quantity T1/Ts, which is referred to as the access effi-
                                 ciency. It is a measure of how close average access time (Ts) is to M1 access time
                                 (T1). From Equation (1.1),
                                                                     T1   =             1                  (1.3)
                                                                     Ts        1  +  (1 -    H ) T2
                                                                                              T1
                                 In Figure 1.23, we plot T1/Ts as a function of the hit ratio H, with the quantity T2/T1
                                 as a parameter. A hit ratio in the range of 0.8 to 0.9 would seem to be needed to
                                 satisfy the performance requirement.
                                      We can now phrase the question about relative memory size more exactly. Is
                                 a hit ratio of 0.8 or higher reasonable for S1  S2? This will depend on a number
                                 of factors, including the nature of the software being executed and the details of the
                                 design of the two-level memory. The main determinant is, of course, the degree of
                                 locality. Figure 1.24 suggests the effect of locality on the hit ratio. Clearly, if M1 is
                                 the same size as M2, then the hit ratio will be 1.0: All of the items in M2 are always
                                 stored also in M1. Now suppose that there is no locality; that is, references are com-
                                 pletely random. In that case the hit ratio should be a strictly linear function of the
                                 relative memory size. For example, if M1 is half the size of M2, then at any time half
                                 of the items from M2 are also in M1 and the hit ratio will be 0.5. In practice, how-
                                 ever, there is some degree of locality in the references. The effects of moderate and
                                 strong locality are indicated in the figure.
                                      So, if there is strong locality, it is possible to achieve high values of hit ratio
                                 even with relatively small upper-level memory size. For example, numerous studies
                          1
                                                          r1
Access efficiency  T1/Ts  0.1                             r  10
                          0.01                            r     100
                          0.001                                                r  1000
                                 0.0          0.2                         0.4                 0.6     0.8                   1.0
                                                                               Hit ratio  H
Figure 1.23                           Access  Efficiency  as a  Function  of   Hit Ratio (r   T2/T1)

APPENDIX  1A  /          PERFORMANCE CHARACTERISTICS OF TWO-LEVEL                   MEMORIES  45
                         1.0
                         0.8       Strong
                                   locality
                         0.6                 Moderate
              Hit ratio                      locality
                         0.4                           No locality
                         0.2
                         0.0
                              0.0  0.2       0.4       0.6                 0.8      1.0
                                             Relative memory size (S1/S2)
          Figure 1.24              Hit Ratio as a Function of Relative Memory Size
have shown that rather small cache sizes will yield a hit ratio above 0.75 regardless
of the size of main memory (e.g., [AGAR89], [PRZY88], [STRE83], and [SMIT82]).
A cache in the range of 1K to 128K words is generally adequate, whereas main
memory is now typically in the gigabyte range. When we consider virtual mem-
ory and disk cache, we will cite other studies that confirm the same phenomenon,
namely that a relatively small M1 yields a high value of hit ratio because of locality.
This brings us to the last question listed earlier: Does the relative size of the
two memories satisfy the cost requirement? The answer is clearly yes. If we need
only a relatively small upper-level memory to achieve good performance, then the
average cost per bit of the two levels of memory will approach that of the cheaper
lower-level memory.

                                       CHAPTER
OPERATING SYSTEM OVERVIEW
    2.1   Operating System Objectives and Functions
                 The Operating System as a User/Computer Interface
                 The Operating System as Resource Manager
                 Ease of Evolution of an Operating System
    2.2   The Evolution of Operating Systems
                 Serial Processing
                 Simple Batch Systems
                 Multiprogrammed Batch Systems
                 Time-Sharing Systems
    2.3   Major Achievements
                 The Process
                 Memory Management
                 Information Protection and Security
                 Scheduling and Resource Management
    2.4   Developments Leading to Modern Operating Systems
    2.5   Virtual Machines
                 Virtual Machines and Virtualizing
                 Virtual Machine Architecture
    2.6   OS Design Considerations for Multiprocessor and Multicore
                 Symmetric Multiprocessor OS Considerations
                 Multicore OS Considerations
    2.7   Microsoft Windows Overview
                 History
                 The Modern OS
                 Architecture
                 Client/Server Model
                 Threads and SMP
                 Windows Objects
                 What Is New in Windows 7
    2.8   Traditional Unix Systems
                 History
                 Description
    2.9   Modern Unix Systems
                 System V Release 4 (SVR4)
                 BSD
                 Solaris 10
    2.10  Linux
                 History
                 Modular Structure
                 Kernel Components
    2.11  Linux Vserver Virtual Machine Architecture
    2.12  Recommended Reading and Web Sites
    2.13  Key Terms, Review Questions, and Problems
46
