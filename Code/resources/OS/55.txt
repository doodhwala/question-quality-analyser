Multiprocessor and Multicore Organization
                 1.8 / MULTIPROCESSOR AND MULTICORE ORGANIZATION                              33
1.8  MULTIPROCESSOR AND MULTICORE ORGANIZATION
     Traditionally, the computer has been viewed as a sequential machine. Most com-
     puter programming languages require the programmer to specify algorithms as
     sequences of instructions. A processor executes programs by executing machine
     instructions in sequence and one at a time. Each instruction is executed in a sequence
     of operations (fetch instruction, fetch operands, perform operation, store results).
         This view of the computer has never been entirely true. At the micro-operation
     level, multiple control signals are generated at the same time. Instruction pipelining,
     at least to the extent of overlapping fetch and execute operations, has been around
     for a long time. Both of these are examples of performing functions in parallel.
         As computer technology has evolved and as the cost of computer hardware
     has dropped, computer designers have sought more and more opportunities for par-
     allelism, usually to improve performance and, in some cases, to improve reliability.
     In this book, we examine the three most popular approaches to providing parallel-
     ism by replicating processors: symmetric multiprocessors (SMPs), multicore com-
     puters, and clusters. SMPs and multicore computers are discussed in this section;
     clusters are examined in Chapter 16.
     Symmetric Multiprocessors
     DEFINITION  An SMP can be defined as a stand-alone computer system with the
     following characteristics:
     1.  There are two or more similar processors of comparable capability.
     2.  These processors share the same main memory and I/O facilities and are inter-
         connected by a bus or other internal connection scheme, such that memory
         access time is approximately the same for each processor.
     3.  All processors share access to I/O devices, either through the same channels
         or through different channels that provide paths to the same device.
     4.  All processors can perform the same functions (hence the term symmetric).
     5.  The system is controlled by an integrated operating system that provides
         interaction between processors and their programs at the job, task, file, and
         data element levels.
         Points 1 to 4 should be self-explanatory. Point 5 illustrates one of the contrasts
     with a loosely coupled multiprocessing system, such as a cluster. In the latter, the
     physical unit of interaction is usually a message or complete file. In an SMP, indi-
     vidual data elements can constitute the level of interaction, and there can be a high
     degree of cooperation between processes.
         An SMP organization has a number of potential advantages over a uniproces-
     sor organization, including the following:
     ·   Performance: If the work to be done by a computer can be organized so that
         some portions of the work can be done in parallel, then a system with multiple
         processors will yield greater performance than one with a single processor of
         the same type.

34  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
    ·  Availability: In a symmetric multiprocessor, because all processors can per-
       form the same functions, the failure of a single processor does not halt the
       machine. Instead, the system can continue to function at reduced performance.
    ·  Incremental growth: A user can enhance the performance of a system by
       adding an additional processor.
    ·  Scaling: Vendors can offer a range of products with different price and
       performance characteristics based on the number of processors configured in
       the system.
    It is important to note that these are potential, rather than guaranteed, benefits.
    The operating system must provide tools and functions to exploit the parallelism in
    an SMP system.
       An attractive feature of an SMP is that the existence of multiple processors is
    transparent to the user. The operating system takes care of scheduling of tasks on
    individual processors and of synchronization among processors.
    ORGANIZATION    Figure 1.19 illustrates the general organization of an SMP. There
    are multiple processors, each of which contains its own control unit, arithmetic-
    logic unit, and registers. Each processor has access to a shared main memory and
    the I/O devices through some form of interconnection mechanism; a shared bus
    is a common facility. The processors can communicate with each other through
    memory (messages and status information left in shared address spaces). It may
       Processor            Processor                               Processor
       L1 cache             L1 cache                                     L1 cache
       L2 cache             L2 cache                                L2 cache
                                                        System bus
                    Main                                            I/O
                    memory                              I/O         adapter
                                          subsystem
                                                                    I/O
                                                                    adapter
                                                                    I/O
                                                                    adapter
    Figure 1.19  Symmetric Multiprocessor Organization

           1.8 / MULTIPROCESSOR AND MULTICORE ORGANIZATION                               35
also be possible for processors to exchange signals directly. The memory is often
organized so that multiple simultaneous accesses to separate blocks of memory are
possible.
In modern computers, processors generally have at least one level of cache
memory that is private to the processor. This use of cache introduces some new
design considerations. Because each local cache contains an image of a portion of
main memory, if a word is altered in one cache, it could conceivably invalidate a
word in another cache. To prevent this, the other processors must be alerted that an
update has taken place. This problem is known as the cache coherence problem and
is typically addressed in hardware rather than by the OS.6
Multicore Computers
A multicore computer, also known as a chip multiprocessor, combines two or more
processors (called cores) on a single piece of silicon (called a die). Typically, each
core consists of all of the components of an independent processor, such as registers,
ALU, pipeline hardware, and control unit, plus L1 instruction and data caches. In
addition to the multiple cores, contemporary multicore chips also include L2 cache
and, in some cases, L3 cache.
The motivation for the development of multicore computers can be summed
up as follows. For decades, microprocessor systems have experienced a steady, usu-
ally exponential, increase in performance. This is partly due to hardware trends,
such as an increase in clock frequency and the ability to put cache memory closer
to the processor because of the increasing miniaturization of microcomputer
components. Performance has also been improved by the increased complexity of
processor design to exploit parallelism in instruction execution and memory access.
In brief, designers have come up against practical limits in the ability to achieve
greater performance by means of more complex processors. Designers have found
that the best way to improve performance to take advantage of advances in hard-
ware is to put multiple processors and a substantial amount of cache memory on a
single chip. A detailed discussion of the rationale for this trend is beyond our scope,
but is summarized in Appendix C.
An example of a multicore system is the Intel Core i7, which includes four x86
processors, each with a dedicated L2 cache, and with a shared L3 cache (Figure 1.20).
One mechanism Intel uses to make its caches more effective is prefetching, in which
the hardware examines memory access patterns and attempts to fill the caches spec-
ulatively with data that's likely to be requested soon.
The Core i7 chip supports two forms of external communications to other
chips. The DDR3 memory controller brings the memory controller for the DDR
(double data rate) main memory onto the chip. The interface supports three chan-
nels that are 8 bytes wide for a total bus width of 192 bits, for an aggregate data
rate of up to 32 GB/s. With the memory controller on the chip, the Front Side Bus
is eliminated. The QuickPath Interconnect (QPI) is a point-to-point link electri-
cal interconnect specification. It enables high-speed communications among con-
nected processor chips. The QPI link operates at 6.4 GT/s (transfers per second).
6A description of hardware-based cache coherency schemes is provided in [STAL10].

36  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
             Core 0                      Core 1            Core 2  Core 3
             32 kB I&D       32 kB I&D             32 kB I&D       32 kB I&D
             L1 caches       L1 caches                 L1 caches   L1 caches
             256 kB                      256 kB            256 kB  256 kB
             L2 cache        L2 cache                  L2 cache    L2 cache
                                                 8 MB
                                                 L3 cache
             DDR3 memory                                   Quickpath
             controllers                                   interconnect
             3 × 8B @ 1.33 GT/s                            4 × 20b @ 6.4 GT/s
             Figure 1.20  Intel Core i7 Block Diagram
     At 16 bits per transfer, that adds up to 12.8 GB/s; and since QPI links involve dedi-
     cated bidirectional pairs, the total bandwidth is 25.6 GB/s.
1.9  RECOMMENDED READING AND WEB SITES
     [STAL10] covers the topics of this chapter in detail. In addition, there are many other
     texts on computer organization and architecture. Among the more worthwhile texts
     are the following. [PATT09] is a comprehensive survey; [HENN07], by the same
     authors, is a more advanced text that emphasizes quantitative aspects of design.
        [DENN05] looks at the history of the development and application of the
     locality principle, making for fascinating reading.
     DENN05  Denning, P. "The Locality Principle." Communications of the ACM, July 2005.
     HENN07  Hennessy, J., and Patterson, D. Computer Architecture: A Quantitative
        Approach. San Mateo, CA: Morgan Kaufmann, 2007.
     PATT09  Patterson, D., and Hennessy, J. Computer Organization and Design: The
        Hardware/Software Interface. San Mateo, CA: Morgan Kaufmann, 2009.
     STAL10  Stallings, W. Computer Organization and Architecture, 8th ed. Upper Saddle
        River, NJ: Prentice Hall, 2010.
     Recommended Web sites:
     ·  WWW Computer Architecture Home Page: A comprehensive index to information
        relevant to computer architecture researchers, including architecture groups and proj-
        ects, technical organizations, literature, employment, and commercial information
     ·  CPU Info Center: Information on specific processors, including technical papers, prod-
        uct information, and latest announcements
