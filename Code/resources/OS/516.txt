RAID
494  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     likely to be affected by this characteristic than lower-density disks and/or disks with
     only one or two surfaces. To avoid this "arm stickiness," the disk request queue
     can be segmented, with one segment at a time being processed completely. Two
     examples of this approach are N-step-SCAN and FSCAN.
         The N-step-SCAN policy segments the disk request queue into subqueues of
     length N. Subqueues are processed one at a time, using SCAN. While a queue is
     being processed, new requests must be added to some other queue. If fewer than N
     requests are available at the end of a scan, then all of them are processed with the
     next scan. With large values of N, the performance of N-step-SCAN approaches
     that of SCAN; with a value of N = 1, the FIFO policy is adopted.
         FSCAN is a policy that uses two subqueues. When a scan begins, all of the
     requests are in one of the queues, with the other empty. During the scan, all new
     requests are put into the other queue. Thus, service of new requests is deferred until
     all of the old requests have been processed.
11.6 RAID
     As discussed earlier, the rate in improvement in secondary storage performance
     has been considerably less than the rate for processors and main memory. This
     mismatch has made the disk storage system perhaps the main focus of concern in
     improving overall computer system performance.
         As in other areas of computer performance, disk storage designers recognize
     that if one component can only be pushed so far, additional gains in performance
     are to be had by using multiple parallel components. In the case of disk storage,
     this leads to the development of arrays of disks that operate independently and
     in parallel. With multiple disks, separate I/O requests can be handled in parallel,
     as long as the data required reside on separate disks. Further, a single I/O request
     can be executed in parallel if the block of data to be accessed is distributed across
     multiple disks.
         With the use of multiple disks, there is a wide variety of ways in which the data
     can be organized and in which redundancy can be added to improve reliability. This
     could make it difficult to develop database schemes that are usable on a number of
     platforms and operating systems. Fortunately, industry has agreed on a standard-
     ized scheme for multiple-disk database design, known as RAID (redundant array of
     independent disks). The RAID scheme consists of seven levels,2 zero through six.
     These levels do not imply a hierarchical relationship but designate different design
     architectures that share three common characteristics:
     1.  RAID is a set of physical disk drives viewed by the OS as a single logical drive.
     2.  Data are distributed across the physical drives of an array in a scheme known
         as striping, described subsequently.
     3.  Redundant disk capacity is used to store parity information, which guarantees
         data recoverability in case of a disk failure.
     2Additional levels have been defined by some researchers and some companies, but the seven levels
     described in this section are the ones universally agreed on.

                                                                    11.6 / RAID                           495
The details of the second and third characteristics differ for the different RAID
levels. RAID 0 and RAID 1 do not support the third characteristic.
The term RAID was originally coined in a paper by a group of researchers
at the University of California at Berkeley [PATT88].3 The paper outlined vari-
ous RAID configurations and applications and introduced the definitions of the
RAID levels that are still used. The RAID strategy employs multiple disk drives
and distributes data in such a way as to enable simultaneous access to data from
multiple drives, thereby improving I/O performance and allowing easier incremen-
tal increases in capacity.
The unique contribution of the RAID proposal is to address effectively the
need for redundancy. Although allowing multiple heads and actuators to operate
simultaneously achieves higher I/O and transfer rates, the use of multiple devices
increases the probability of failure. To compensate for this decreased reliability,
RAID makes use of stored parity information that enables the recovery of data lost
due to a disk failure.
We now examine each of the RAID levels. Table 11.4 provides a rough
guide to the seven levels. In the table, I/O performance is shown both in terms of
data transfer capacity, or ability to move data, and I/O request rate, or ability to
satisfy I/O requests, since these RAID levels inherently perform differently rela-
tive to these two metrics. Each RAID level's strong point is highlighted in color.
Figure 11.8 is an example that illustrates the use of the seven RAID schemes
to support a data capacity requiring four disks with no redundancy. The figure
highlights the layout of user data and redundant data and indicates the relative
storage requirements of the various levels. We refer to this figure throughout the
following discussion.
Of the seven RAID levels described, only four are commonly used: RAID
levels 0, 1, 5, and 6.
RAID Level 0
RAID level 0 is not a true member of the RAID family, because it does not include
redundancy to improve performance or provide data protection. However, there
are a few applications, such as some on supercomputers in which performance
and capacity are primary concerns and low cost is more important than improved
reliability.
For RAID 0, the user and system data are distributed across all of the disks
in the array. This has a notable advantage over the use of a single large disk: If two
different I/O requests are pending for two different blocks of data, then there is a
good chance that the requested blocks are on different disks. Thus, the two requests
can be issued in parallel, reducing the I/O queueing time.
But RAID 0, as with all of the RAID levels, goes further than simply distrib-
uting the data across a disk array: The data are striped across the available disks.
3In that paper, the acronym RAID stood for Redundant Array of Inexpensive Disks. The term inexpen-
sive was used to contrast the small relatively inexpensive disks in the RAID array to the alternative, a
single large expensive disk (SLED). The SLED is essentially a thing of the past, with similar disk technol-
ogy being used for both RAID and non-RAID configurations. Accordingly, the industry has adopted the
term independent to emphasize that the RAID array creates significant performance and reliability gains.

496
     Table 11.4   RAID Levels
                                                               Disks                              Large I/O Data
     Category         Level    Description                     Required  Data Availability        Transfer Capacity                Small I/O Request Rate
     Striping         0        Nonredundant                    N         Lower than single disk   Very high                        Very high for both read
                                                                                                                                   and write
     Mirroring        1        Mirrored                        2N        Higher than RAID 2,      Higher than single disk for      Up to twice that of a single
                                                                         3, 4, or 5; lower than   read; similar to single disk     disk for read; similar to
                                                                         RAID 6                   for write                        single disk for write
     Parallel access  2        Redundant via                   Nm        Much higher than single  Highest of all listed            Approximately twice that
                               Hamming code                              disk; comparable to      alternatives                     of a single disk
                                                                         RAID 3, 4, or 5
                      3        Bit-interleaved                 N1        Much higher than single  Highest of all listed            Approximately twice that
                               parity                                    disk; comparable to      alternatives                     of a single disk
                                                                         RAID 2, 4, or 5
     Independent      4        Block-interleaved               N1        Much higher than single  Similar to RAID 0 for read;      Similar to RAID 0 for read;
     access                    parity                                    disk; comparable to      significantly lower than single  significantly lower than
                                                                         RAID 2, 3, or 5          disk for write                   single disk for write
                      5        Block-interleaved               N1        Much higher than single  Similar to RAID 0 for read;      Similar to RAID 0 for read;
                               distributed parity                        disk; comparable to      lower than single disk for       generally lower than single
                                                                         RAID 2, 3, or 4          write                            disk for write
                      6        Block-interleaved               N2        Highest of all listed    Similar to RAID 0 for read;      Similar to RAID 0 for read;
                               dual distributed                          alternatives             lower than RAID 5 for write      significantly lower than
                               parity                                                                                              RAID 5 for write
     Note: N, number of data disks; m, proportional to log N.

strip 0      strip 1             strip 2           strip 3
strip 4      strip 5             strip 6           strip 7
strip 8      strip 9             strip 10          strip 11
strip 12     strip 13            strip 14          strip 15
(a) RAID 0 (nonredundant)
strip 0      strip 1             strip 2           strip 3       strip 0     strip 1   strip 2   strip 3
strip 4      strip 5             strip 6           strip 7       strip 4     strip 5   strip 6   strip 7
strip 8      strip 9             strip 10          strip 11      strip 8     strip 9   strip 10  strip 11
strip 12     strip 13            strip 14          strip 15      strip 12    strip 13  strip 14  strip 15
(b) RAID 1 (mirrored)
b0           b1                      b2            b3                 f0(b)  f1(b)     f2(b)
(c) RAID 2 (redundancy through Hamming code)
b0                     b1                  b2                b3              P(b)
(d) RAID 3 (bit-interleaved parity)
block 0      block 1                     block 2             block 3         P(0-3)
block 4      block 5                     block 6             block 7         P(4-7)
block 8      block 9                     block 10            block 11        P(8-11)
block 12     block 13                    block 14            block 15        P(12-15)
(e) RAID 4 (block-level parity)
Figure 11.8  RAID Levels                                                                         497

498  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     block 0             block 1       block 2    block 3   P(0-3)
     block 4             block 5       block 6    P(4-7)    block 7
     block 8             block 9       P(8-11)    block 10  block 11
     block 12            P(12-15)  block 13       block 14  block 15
     P(16-19)            block 16  block 17       block 18  block 19
     (f) RAID 5 (block-level distributed parity)
     block 0             block 1       block 2    block 3   P(0-3)    Q(0-3)
     block 4             block 5       block 6    P(4-7)    Q(4-7)    block 7
     block 8             block 9       P(8-11)    Q(8-11)   block 10  block 11
     block 12            P(12-15)  Q(12-15)       block 13  block 14  block 15
     (g) RAID 6 (dual redundancy)
     Figure 11.8  RAID Levels (Continued)
     This is best understood by considering Figure 11.8. All user and system data are
     viewed as being stored on a logical disk. The logical disk is divided into strips; these
     strips may be physical blocks, sectors, or some other unit. The strips are mapped
     round robin to consecutive physical disks in the RAID array. A set of logically
     consecutive strips that maps exactly one strip to each array member is referred
     to as a stripe. In an n-disk array, the first n logical strips are physically stored as
     the first strip on each of the n disks, forming the first stripe; the second n strips
     are distributed as the second strips on each disk; and so on. The advantage of this
     layout is that if a single I/O request consists of multiple logically contiguous strips,
     then up to n strips for that request can be handled in parallel, greatly reducing the
     I/O transfer time.
     RAID 0 FOR HIGH DATA TRANSFER CAPACITY                 The performance of any of the
     RAID levels depends critically on the request patterns of the host system and
     on the layout of the data. These issues can be most clearly addressed in RAID 0,
     where the impact of redundancy does not interfere with the analysis. First, let us
     consider the use of RAID 0 to achieve a high data transfer rate. For applications to
     experience a high transfer rate, two requirements must be met. First, a high transfer
     capacity must exist along the entire path between host memory and the individual
     disk drives. This includes internal controller buses, host system I/O buses, I/O
     adapters, and host memory buses.
     The second requirement is that the application must make I/O requests that
     drive the disk array efficiently. This requirement is met if the typical request is for
     large amounts of logically contiguous data, compared to the size of a strip. In this

                                                                   11.6 / RAID               499
case, a single I/O request involves the parallel transfer of data from multiple disks,
increasing the effective transfer rate compared to a single-disk transfer.
RAID 0 FOR HIGH I/O REQUEST RATE     In a transaction-oriented environment,
the user is typically more concerned with response time than with transfer rate. For
an individual I/O request for a small amount of data, the I/O time is dominated by
the motion of the disk heads (seek time) and the movement of the disk (rotational
latency).
    In a transaction environment, there may be hundreds of I/O requests per sec-
ond. A disk array can provide high I/O execution rates by balancing the I/O load
across multiple disks. Effective load balancing is achieved only if there are typi-
cally multiple I/O requests outstanding. This, in turn, implies that there are multiple
independent applications or a single transaction-oriented application that is capable
of multiple asynchronous I/O requests. The performance will also be influenced
by the strip size. If the strip size is relatively large, so that a single I/O request only
involves a single disk access, then multiple waiting I/O requests can be handled in
parallel, reducing the queueing time for each request.
RAID Level 1
RAID 1 differs from RAID levels 2 through 6 in the way in which redundancy is
achieved. In these other RAID schemes, some form of parity calculation is used
to introduce redundancy, whereas in RAID 1, redundancy is achieved by the sim-
ple expedient of duplicating all the data. Figure 11.8b shows data striping being
used, as in RAID 0. But in this case, each logical strip is mapped to two separate
physical disks so that every disk in the array has a mirror disk that contains the
same data. RAID 1 can also be implemented without data striping, though this is
less common.
    There are a number of positive aspects to the RAID 1 organization:
1.  A read request can be serviced by either of the two disks that contains the
    requested data, whichever one involves the minimum seek time plus rotational
    latency.
2.  A write request requires that both corresponding strips be updated, but this
    can be done in parallel. Thus, the write performance is dictated by the slower
    of the two writes (i.e., the one that involves the larger seek time plus rotational
    latency). However, there is no "write penalty" with RAID 1. RAID levels
    2 through 6 involve the use of parity bits. Therefore, when a single strip is
    updated, the array management software must first compute and update the
    parity bits as well as updating the actual strip in question.
3.  Recovery from a failure is simple. When a drive fails, the data may still be
    accessed from the second drive.
    The principal disadvantage of RAID 1 is the cost; it requires twice the disk
space of the logical disk that it supports. Because of that, a RAID 1 configuration
is likely to be limited to drives that store system software and data and other highly
critical files. In these cases, RAID 1 provides real-time backup of all data so that in
the event of a disk failure, all of the critical data is still immediately available.

500  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     In a transaction-oriented environment, RAID 1 can achieve high I/O request
     rates if the bulk of the requests are reads. In this situation, the performance of RAID
     1 can approach double of that of RAID 0. However, if a substantial fraction of the I/O
     requests are write requests, then there may be no significant performance gain over
     RAID 0. RAID 1 may also provide improved performance over RAID 0 for data
     transfer-intensive applications with a high percentage of reads. Improvement occurs
     if the application can split each read request so that both disk members participate.
     RAID Level 2
     RAID levels 2 and 3 make use of a parallel access technique. In a parallel access
     array, all member disks participate in the execution of every I/O request. Typically,
     the spindles of the individual drives are synchronized so that each disk head is in the
     same position on each disk at any given time.
     As in the other RAID schemes, data striping is used. In the case of RAID 2
     and 3, the strips are very small, often as small as a single byte or word. With RAID 2,
     an error-correcting code is calculated across corresponding bits on each data disk,
     and the bits of the code are stored in the corresponding bit positions on multiple
     parity disks. Typically, a Hamming code is used, which is able to correct single-bit
     errors and detect double-bit errors.
     Although RAID 2 requires fewer disks than RAID 1, it is still rather costly.
     The number of redundant disks is proportional to the log of the number of data
     disks. On a single read, all disks are simultaneously accessed. The requested data
     and the associated error-correcting code are delivered to the array controller. If
     there is a single-bit error, the controller can recognize and correct the error instantly,
     so that the read access time is not slowed. On a single write, all data disks and parity
     disks must be accessed for the write operation.
     RAID 2 would only be an effective choice in an environment in which many
     disk errors occur. Given the high reliability of individual disks and disk drives,
     RAID 2 is overkill and is not implemented.
     RAID Level 3
     RAID 3 is organized in a similar fashion to RAID 2. The difference is that RAID 3
     requires only a single redundant disk, no matter how large the disk array. RAID 3
     employs parallel access, with data distributed in small strips. Instead of an error-
     correcting code, a simple parity bit is computed for the set of individual bits in the
     same position on all of the data disks.
     REDUNDANCY  In the event of a drive failure, the parity drive is accessed and data
     is reconstructed from the remaining devices. Once the failed drive is replaced, the
     missing data can be restored on the new drive and operation resumed.
     Data reconstruction is simple. Consider an array of five drives in which X0
     through X3 contain data and X4 is the parity disk. The parity for the ith bit is
     calculated as follows:
                             X4(i) = X3(i)  X2(i)  X1(i)  X0(i)
     where  is exclusive-OR function.

                                                       11.6 / RAID                     501
Suppose that drive X1 has failed. If we add X4(i)  X1(i) to both sides of the
preceding equation, we get
              X1(i) = X4(i)  X3(i)  X2(i)  X0(i)
Thus, the contents of each strip of data on X1 can be regenerated from the contents
of the corresponding strips on the remaining disks in the array. This principle is true
for RAID levels 3 through 6.
In the event of a disk failure, all of the data are still available in what is referred
to as reduced mode. In this mode, for reads, the missing data are regenerated on the
fly using the exclusive-OR calculation. When data are written to a reduced RAID 3
array, consistency of the parity must be maintained for later regeneration. Return to
full operation requires that the failed disk be replaced and the entire contents of the
failed disk be regenerated on the new disk.
PERFORMANCE   Because data are striped in very small strips, RAID 3 can achieve
very high data transfer rates. Any I/O request will involve the parallel transfer of
data from all of the data disks. For large transfers, the performance improvement is
especially noticeable. On the other hand, only one I/O request can be executed at a
time. Thus, in a transaction-oriented environment, performance suffers.
RAID Level 4
RAID levels 4 through 6 make use of an independent access technique. In an inde-
pendent access array, each member disk operates independently, so that separate
I/O requests can be satisfied in parallel. Because of this, independent access arrays
are more suitable for applications that require high I/O request rates and are rela-
tively less suited for applications that require high data transfer rates.
As in the other RAID schemes, data striping is used. In the case of RAID
4 through 6, the strips are relatively large. With RAID 4, a bit-by-bit parity strip
is calculated across corresponding strips on each data disk, and the parity bits are
stored in the corresponding strip on the parity disk.
RAID 4 involves a write penalty when an I/O write request of small size is per-
formed. Each time that a write occurs, the array management software must update
not only the user data but also the corresponding parity bits. Consider an array of
five drives in which X0 through X3 contain data and X4 is the parity disk. Suppose
that a write is performed that only involves a strip on disk X1. Initially, for each bit i,
we have the following relationship:
              X4(i) = X3(i)  X2(i)  X1(i)  X0(i)                              (11.1)
After the update, with potentially altered bits indicated by a prime symbol:
X4=(i) = X3(i)  X2(i)  X1=(i)  X0(i)
             = X3(i)  X2(i)  X1=(i)  X0(i)  X1(i)  X1(i)
             = X3(i)  X2(i)  X1 (i)  X0(i)  X1(i)  X1=(i)
             = X4(i)  X1(i)  X1=(i)
The preceding set of equations is derived as follows. The first line shows that
a change in X1 will also affect the parity disk X4. In the second line, we add the

502  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     terms [ X1(i)  X1(i)]. Because the exclusive-OR of any quantity with itself is 0,
     this does not affect the equation. However, it is a convenience that is used to create
     the third line, by reordering. Finally, Equation (11.1) is used to replace the first
     four terms by X4(i).
     To calculate the new parity, the array management software must read the old
     user strip and the old parity strip. Then it can update these two strips with the new
     data and the newly calculated parity. Thus, each strip write involves two reads and
     two writes.
     In the case of a larger size I/O write that involves strips on all disk drives, parity
     is easily computed by calculation using only the new data bits. Thus, the parity drive
     can be updated in parallel with the data drives and there are no extra reads or writes.
     In any case, every write operation must involve the parity disk, which therefore
     can become a bottleneck.
     RAID Level 5
     RAID 5 is organized in a similar fashion to RAID 4. The difference is that RAID
     5 distributes the parity strips across all disks. A typical allocation is a round-robin
     scheme, as illustrated in Figure 11.8f. For an n-disk array, the parity strip is on a
     different disk for the first n stripes, and the pattern then repeats.
     The distribution of parity strips across all drives avoids the potential I/O
     bottleneck of the single parity disk found in RAID 4. Further, RAID 5 has the
     characteristic that the loss of any one disk does not result in data loss.
     RAID Level 6
     RAID 6 was introduced in a subsequent paper by the Berkeley researchers
     [KATZ89]. In the RAID 6 scheme, two different parity calculations are carried out
     and stored in separate blocks on different disks. Thus, a RAID 6 array whose user
     data require N disks consists of N + 2 disks.
     Figure 11.8g illustrates the scheme. P and Q are two different data check algo-
     rithms. One of the two is the exclusive-OR calculation used in RAID 4 and 5. But
     the other is an independent data check algorithm. This makes it possible to regener-
     ate data even if two disks containing user data fail.
     The advantage of RAID 6 is that it provides extremely high data availability.
     Three disks would have to fail within the MTTR (mean time to repair) interval to
     cause data to be lost. On the other hand, RAID 6 incurs a substantial write penalty,
     because each write affects two parity blocks. Performance benchmarks [EISC07]
     show that a RAID 6 controller can suffer more than a 30% drop in overall write
     performance compared with a RAID 5 implementation. RAID 5 and RAID 6 read
     performance is comparable.
11.7 DISK CACHE
     In Section 1.6 and Appendix 1A, we summarized the principles of cache memory.
     The term cache memory is usually used to apply to a memory that is smaller and
     faster than main memory and that is interposed between main memory and the
