I/O Buffering
                                                                 11.4 / I/O BUFFERING       483
         are handled at this layer and I/O status is collected and reported. This is the
         layer of software that actually interacts with the I/O module and hence the
         device hardware.
         For a communications device, the I/O structure (Figure 11.4b) looks much the
      same as that just described. The principal difference is that the logical I/O module is
      replaced by a communications architecture, which may itself consist of a number of
      layers. An example is TCP/IP, which is discussed in Chapter 17.
         Figure 11.4c shows a representative structure for managing I/O on a secondary
      storage device that supports a file system. The three layers not previously discussed
      are as follows:
      ·  Directory management: At this layer, symbolic file names are converted to
         identifiers that either reference the file directly or indirectly through a file
         descriptor or index table. This layer is also concerned with user operations
         that affect the directory of files, such as add, delete, and reorganize.
      ·  File system: This layer deals with the logical structure of files and with the
         operations that can be specified by users, such as open, close, read, and write.
         Access rights are also managed at this layer.
      ·  Physical organization: Just as virtual memory addresses must be converted
         into physical main memory addresses, taking into account the segmentation
         and paging structure, logical references to files and records must be con-
         verted to physical secondary storage addresses, taking into account the physi-
         cal track and sector structure of the secondary storage device. Allocation of
         secondary storage space and main storage buffers is generally treated at this
         layer as well.
         Because of the importance of the file system, we will spend some time, in this
      chapter and the next, looking at its various components. The discussion in this chap-
      ter focuses on the lower three layers, while the upper two layers are examined in
      Chapter 12.
11.4  I/O BUFFERING
      Suppose that a user process wishes to read blocks of data from a disk one at a time,
      with each block having a length of 512 bytes. The data are to be read into a data
      area within the address space of the user process at virtual location 1000 to 1511.
      The simplest way would be to execute an I/O command (something like Read_
      Block[1000,      disk]) to the disk unit and then wait for the data to become avail-
      able. The waiting could either be busy waiting (continuously test the device status)
      or, more practically, process suspension on an interrupt.
         There are two problems with this approach. First, the program is hung up
      waiting for the relatively slow I/O to complete. The second problem is that this
      approach to I/O interferes with swapping decisions by the OS. Virtual locations
      1000 to 1511 must remain in main memory during the course of the block transfer.
      Otherwise, some of the data may be lost. If paging is being used, at least the page
      containing the target locations must be locked into main memory. Thus, although

484  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     portions of the process may be paged out to disk, it is impossible to swap the proc-
     ess out completely, even if this is desired by the operating system. Notice also that
     there is a risk of single-process deadlock. If a process issues an I/O command, is
     suspended awaiting the result, and then is swapped out prior to the beginning of the
     operation, the process is blocked waiting on the I/O event, and the I/O operation is
     blocked waiting for the process to be swapped in. To avoid this deadlock, the user
     memory involved in the I/O operation must be locked in main memory immediately
     before the I/O request is issued, even though the I/O operation is queued and may
     not be executed for some time.
     The same considerations apply to an output operation. If a block is being
     transferred from a user process area directly to an I/O module, then the process is
     blocked during the transfer and the process may not be swapped out.
     To avoid these overheads and inefficiencies, it is sometimes convenient to
     perform input transfers in advance of requests being made and to perform output
     transfers some time after the request is made. This technique is known as buffer-
     ing. In this section, we look at some of the buffering schemes that are supported by
     operating systems to improve the performance of the system.
     In discussing the various approaches to buffering, it is sometimes important
     to make a distinction between two types of I/O devices: block oriented and stream
     oriented. A block-oriented device stores information in blocks that are usually of
     fixed size, and transfers are made one block at a time. Generally, it is possible to
     reference data by its block number. Disks and USB keys are examples of block-
     oriented devices. A stream-oriented device transfers data in and out as a stream of
     bytes, with no block structure. Terminals, printers, communications ports, mouse
     and other pointing devices, and most other devices that are not secondary storage
     are stream oriented.
     Single Buffer
     The simplest type of support that the OS can provide is single buffering (Figure 11.5b).
     When a user process issues an I/O request, the OS assigns a buffer in the system
     portion of main memory to the operation.
     For block-oriented devices, the single buffering scheme can be described
     as follows: Input transfers are made to the system buffer. When the transfer is
     complete, the process moves the block into user space and immediately requests
     another block. This is called reading ahead, or anticipated input; it is done in the
     expectation that the block will eventually be needed. For many types of compu-
     tation, this is a reasonable assumption most of the time because data are usually
     accessed sequentially. Only at the end of a sequence of processing will a block be
     read in unnecessarily.
     This approach will generally provide a speedup compared to the lack of system
     buffering. The user process can be processing one block of data while the next block
     is being read in. The OS is able to swap the process out because the input operation
     is taking place in system memory rather than user process memory. This technique
     does, however, complicate the logic in the operating system. The OS must keep
     track of the assignment of system buffers to user processes. The swapping logic is
     also affected: If the I/O operation involves the same disk that is used for swapping,

                                                                11.4  / I/O BUFFERING  485
                                    Operating system                  User process
I/O device   In
                                    (a) No buffering
                                    Operating system                  User process
I/O device   In                                           Move
                                    (b) Single buffering
                                    Operating system                  User process
I/O device   In                                           Move
                                    (c) Double buffering
                                    Operating system                  User process
I/O device   In                                           Move
                 (d) Circular buffering
Figure 11.5  I/O Buffering Schemes (Input)
it hardly makes sense to queue disk writes to the same device for swapping the proc-
ess out. This attempt to swap the process and release main memory will itself not
begin until after the I/O operation finishes, at which time swapping the process to
disk may no longer be appropriate.
Similar considerations apply to block-oriented output. When data are being
transmitted to a device, they are first copied from the user space into the system
buffer, from which they will ultimately be written. The requesting process is now
free to continue or to be swapped as necessary.
[KNUT97] suggests a crude but informative performance comparison between
single buffering and no buffering. Suppose that T is the time required to input one
block and that C is the computation time that intervenes between input requests.
Without buffering, the execution time per block is essentially T + C. With a single
buffer, the time is max [C, T] + M, where M is the time required to move the data
from the system buffer to user memory. In most cases, execution time per block is
substantially less with a single buffer compared to no buffer.
For stream-oriented I/O, the single buffering scheme can be used in a line-
at-a-time fashion or a byte-at-a-time fashion. Line-at-a-time operation is appropri-
ate for scroll-mode terminals (sometimes called dumb terminals). With this form

486  CHAPTER 11 / I/O MANAGEMENT AND DISK SCHEDULING
     of terminal, user input is one line at a time, with a carriage return signaling the end
     of a line, and output to the terminal is similarly one line at a time. A line printer is
     another example of such a device. Byte-at-a-time operation is used on forms-mode
     terminals, when each keystroke is significant, and for many other peripherals, such
     as sensors and controllers.
     In the case of line-at-a-time I/O, the buffer can be used to hold a single line.
     The user process is suspended during input, awaiting the arrival of the entire line.
     For output, the user process can place a line of output in the buffer and continue
     processing. It need not be suspended unless it has a second line of output to send
     before the buffer is emptied from the first output operation. In the case of byte-at-a-
     time I/O, the interaction between the OS and the user process follows the producer/
     consumer model discussed in Chapter 5.
     Double Buffer
     An improvement over single buffering can be had by assigning two system buffers to
     the operation (Figure 11.5c). A process now transfers data to (or from) one buffer
     while the operating system empties (or fills) the other. This technique is known as
     double buffering or buffer swapping.
     For block-oriented transfer, we can roughly estimate the execution time as
     max [C, T]. It is therefore possible to keep the block-oriented device going at full
     speed if C ... T. On the other hand, if C 7 T, double buffering ensures that the
     process will not have to wait on I/O. In either case, an improvement over single
     buffering is achieved. Again, this improvement comes at the cost of increased
     complexity.
     For stream-oriented input, we again are faced with the two alternative modes
     of operation. For line-at-a-time I/O, the user process need not be suspended for
     input or output, unless the process runs ahead of the double buffers. For byte-at-a-
     time operation, the double buffer offers no particular advantage over a single buffer
     of twice the length. In both cases, the producer/consumer model is followed.
     Circular Buffer
     A double-buffer scheme should smooth out the flow of data between an I/O device
     and a process. If the performance of a particular process is the focus of our concern,
     then we would like for the I/O operation to be able to keep up with the process.
     Double buffering may be inadequate if the process performs rapid bursts of I/O. In
     this case, the problem can often be alleviated by using more than two buffers.
     When more than two buffers are used, the collection of buffers is itself referred
     to as a circular buffer (Figure 11.5d), with each individual buffer being one unit in
     the circular buffer. This is simply the bounded-buffer producer/consumer model
     studied in Chapter 5.
     The Utility of Buffering
     Buffering is a technique that smoothes out peaks in I/O demand. However, no
     amount of buffering will allow an I/O device to keep pace with a process indefi-
     nitely when the average demand of the process is greater than the I/O device can

                                                           11.5 / DISK SCHEDULING                  487
      service. Even with multiple buffers, all of the buffers will eventually fill up and the
      process will have to wait after processing each chunk of data. However, in a multi-
      programming environment, when there is a variety of I/O activity and a variety of
      process activity to service, buffering is one tool that can increase the efficiency of
      the OS and the performance of individual processes.
11.5  DISK SCHEDULING
      Over the last 40 years, the increase in the speed of processors and main memory has
      far outstripped that for disk access, with processor and main memory speeds increas-
      ing by about two orders of magnitude compared to one order of magnitude for disk.
      The result is that disks are currently at least four orders of magnitude slower than
      main memory. This gap is expected to continue into the foreseeable future. Thus,
      the performance of disk storage subsystem is of vital concern, and much research
      has gone into schemes for improving that performance. In this section, we highlight
      some of the key issues and look at the most important approaches. Because the
      performance of the disk system is tied closely to file system design issues, the discus-
      sion continues in Chapter 12.
      Disk Performance Parameters
      The actual details of disk I/O operation depend on the computer system, the oper-
      ating system, and the nature of the I/O channel and disk controller hardware. A
      general timing diagram of disk I/O transfer is shown in Figure 11.6.
      When the disk drive is operating, the disk is rotating at constant speed. To
      read or write, the head must be positioned at the desired track and at the beginning
      of the desired sector on that track.1 Track selection involves moving the head in a
      movable-head system or electronically selecting one head on a fixed-head system.
      On a movable-head system, the time it takes to position the head at the track is
      known as seek time. In either case, once the track is selected, the disk controller
      waits until the appropriate sector rotates to line up with the head. The time it takes
      for the beginning of the sector to reach the head is known as rotational delay, or
      rotational latency. The sum of the seek time, if any, and the rotational delay equals
      the access time, which is the time it takes to get into position to read or write. Once
      the head is in position, the read or write operation is then performed as the sector
      moves under the head; this is the data transfer portion of the operation; the time
      required for the transfer is the transfer time.
      Wait for         Wait for      Seek                                    Rotational  Data
      device           channel                                               delay       transfer
                                                       Device busy
      Figure 11.6  Timing of a Disk I/O Transfer
      1See Appendix J for a discussion of disk organization and formatting.
